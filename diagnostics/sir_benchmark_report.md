# SIR/LHS Resampling Benchmark Report

Generated: 2025-12-13

## Overview

This report documents the performance of Sampling Importance Resampling (SIR) and Latin Hypercube Sampling (LHS) methods for accelerating the M-step optimization in MCEM.

## Test Configuration

- **Model**: 3-state progressive model (1 → 2 → 3)
- **Subjects**: 50
- **Hazards**: Weibull
- **Pool size**: ~995 paths/subject average
- **ESS target**: 200 paths/subject

## Key Results

### Log-Likelihood Computation Speedup (M-step bottleneck)

The primary benefit of SIR/LHS is reducing the number of paths over which log-likelihoods must be computed during M-step optimization.

| Method | Total Paths | Time (ms) | Speedup |
|--------|-------------|-----------|---------|
| Full pool IS | 49,760 | 105.9 | 1.0x |
| SIR subset | 10,000 | 21.31 | **4.97x** |
| LHS subset | 10,000 | 19.77 | **5.36x** |

**Expected speedup (paths ratio)**: 4.98x ✓

The observed speedup matches the theoretical reduction in paths almost exactly.

### MLL Estimation Accuracy

All three methods produce consistent marginal log-likelihood estimates:

| Method | MLL Estimate |
|--------|--------------|
| IS-weighted (full pool) | -34.7158 |
| SIR (ESS=200) | -34.4844 |
| LHS (ESS=200) | -34.6346 |

**Max absolute difference**: 0.2314

### Subject-Level Agreement

| Comparison | Correlation | Relative MAE |
|------------|-------------|--------------|
| SIR vs IS | 0.9992 | 2.08% |
| LHS vs IS | 0.9995 | 1.76% |
| SIR vs LHS | 0.9986 | - |

### Variance Comparison (50 replications, ESS=200)

| Method | Variance | SD |
|--------|----------|-----|
| Full Pool IS | 0.0 (deterministic) | 0.0 |
| SIR | 0.0331 | 0.182 |
| LHS | 0.0267 | 0.164 |

**Variance ratio (SIR/LHS)**: 1.24

LHS provides ~20% lower resampling variance than standard SIR.

### Unbiasedness (100 bootstrap samples)

| Method | Mean | Bias | SD |
|--------|------|------|-----|
| IS (gold standard) | -34.7158 | - | - |
| SIR | -34.7193 | -0.0036 | 0.2494 |
| LHS | -34.7687 | -0.0529 | 0.2668 |

Both SIR and LHS are unbiased (bias well within 3 SD).

### MLL Aggregation Time

The MLL aggregation step (summing pre-computed log-likelihoods) is trivially fast and shows no meaningful speedup:

| Method | Time (ms) |
|--------|-----------|
| IS-weighted | 0.010 |
| SIR resampled | 0.010 |
| LHS resampled | 0.009 |

### Resampling Overhead

| Method | Time (ms) |
|--------|-----------|
| SIR resampling | 1.39 |
| LHS resampling | 0.55 |

LHS resampling is ~2.5x faster than SIR.

## Recommendations

1. **Use LHS (`:lhs`) over SIR (`:sir`)**: LHS provides both lower variance and faster resampling.

2. **Expected speedup**: M-step optimization will be approximately (pool_size / ess_target)x faster.

3. **Default settings** (as of v0.2.3): 
   - `sir=:adaptive_lhs` (starts with IS, switches to LHS when cost-effective)
   - `sir_adaptive_threshold=2.0` (switch when path ratio exceeds 2.0)
   - `sir_adaptive_min_iters=3` (minimum iterations before considering switch)
   - `sir_pool_constant=2.0` (pool = 2 × ESS × log(ESS))
   - `sir_max_pool=8192`

4. **SIR mode options**:
   - `:none` - Standard IS without resampling
   - `:sir` - Multinomial SIR from iteration 1
   - `:lhs` - Latin Hypercube SIR from iteration 1 (lower variance)
   - `:adaptive_sir` - Start with IS, switch to SIR when cost-effective
   - `:adaptive_lhs` - Start with IS, switch to LHS when cost-effective (recommended)

5. **When to use SIR/LHS**:
   - Large pool sizes (>500 paths/subject)
   - Many MCEM iterations expected
   - Computational cost is dominated by M-step optimization
   - The adaptive modes automatically detect when switching is beneficial

## Test Source

These benchmarks are generated by `MultistateModelsTests/unit/test_mll_consistency.jl`.

Run tests with:
```julia
include("MultistateModelsTests/src/MultistateModelsTests.jl")
using .MultistateModelsTests
MultistateModelsTests.runtests()
```
