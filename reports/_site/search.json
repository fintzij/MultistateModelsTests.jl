[
  {
    "objectID": "unit_tests.html",
    "href": "unit_tests.html",
    "title": "Unit Test Coverage",
    "section": "",
    "text": "This document provides a comprehensive summary of unit test coverage for MultistateModels.jl. Unit tests verify individual functions and components work correctly in isolation, forming the foundation of the testing pyramid.\n\n\nüü¢ Cache current (0da744c, updated 2025-12-19T10:47:02.164)\n\n\n**Branch:** `penalized_splines` | **Commit:** `0da744c`\n\n‚ö†Ô∏è Uncommitted changes in: scratch/per_transition_obstype_plan.md\n\n\n\n\n\n\n\n| Metric | Value |\n|--------|-------|\n| Total Passed | 28 |\n| Total Failed | 0 |\n| Total Errors | 0 |\n| Categories Tested | 3 |\n\n\n\n\n\n\n\n3√ó6 DataFrame\n\n\n\nRow\nCategory\nPassed\nFailed\nErrors\nTimestamp\nStatus\n\n\n\nSymbol\nInt64\nInt64\nInt64\nString\nString\n\n\n\n\n1\ninitialization\n5\n0\n0\n2025-12-19T10:47:02.149\n‚úÖ Pass\n\n\n2\nhazards\n15\n0\n0\n2025-12-19T10:47:01.501\n‚úÖ Pass\n\n\n3\nsimulation\n8\n0\n0\n2025-12-19T10:47:01.949\n‚úÖ Pass"
  },
  {
    "objectID": "unit_tests.html#overview",
    "href": "unit_tests.html#overview",
    "title": "Unit Test Coverage",
    "section": "",
    "text": "This document provides a comprehensive summary of unit test coverage for MultistateModels.jl. Unit tests verify individual functions and components work correctly in isolation, forming the foundation of the testing pyramid.\n\n\nüü¢ Cache current (0da744c, updated 2025-12-19T10:47:02.164)\n\n\n**Branch:** `penalized_splines` | **Commit:** `0da744c`\n\n‚ö†Ô∏è Uncommitted changes in: scratch/per_transition_obstype_plan.md\n\n\n\n\n\n\n\n| Metric | Value |\n|--------|-------|\n| Total Passed | 28 |\n| Total Failed | 0 |\n| Total Errors | 0 |\n| Categories Tested | 3 |\n\n\n\n\n\n\n\n3√ó6 DataFrame\n\n\n\nRow\nCategory\nPassed\nFailed\nErrors\nTimestamp\nStatus\n\n\n\nSymbol\nInt64\nInt64\nInt64\nString\nString\n\n\n\n\n1\ninitialization\n5\n0\n0\n2025-12-19T10:47:02.149\n‚úÖ Pass\n\n\n2\nhazards\n15\n0\n0\n2025-12-19T10:47:01.501\n‚úÖ Pass\n\n\n3\nsimulation\n8\n0\n0\n2025-12-19T10:47:01.949\n‚úÖ Pass"
  },
  {
    "objectID": "unit_tests.html#test-organization",
    "href": "unit_tests.html#test-organization",
    "title": "Unit Test Coverage",
    "section": "Test Organization",
    "text": "Test Organization\nUnit tests are located in MultistateModelsTests/unit/ and organized by functional area:\nMultistateModelsTests/unit/\n‚îú‚îÄ‚îÄ test_hazards.jl                    # Hazard function evaluation\n‚îú‚îÄ‚îÄ test_splines.jl                    # B-spline hazard implementation\n‚îú‚îÄ‚îÄ test_simulation.jl                 # Path simulation engine\n‚îú‚îÄ‚îÄ test_mcem.jl                       # MCEM algorithm components\n‚îú‚îÄ‚îÄ test_phasetype.jl                  # Phase-type distributions\n‚îú‚îÄ‚îÄ test_phasetype_emission_expansion.jl\n‚îú‚îÄ‚îÄ test_phasetype_panel_expansion.jl\n‚îú‚îÄ‚îÄ test_sir.jl                        # Sampling importance resampling\n‚îú‚îÄ‚îÄ test_variance.jl                   # Variance estimation\n‚îú‚îÄ‚îÄ test_ncv.jl                        # Cross-validation\n‚îú‚îÄ‚îÄ test_initialization.jl             # Parameter initialization\n‚îú‚îÄ‚îÄ test_modelgeneration.jl            # Model construction\n‚îú‚îÄ‚îÄ test_helpers.jl                    # Utility functions\n‚îú‚îÄ‚îÄ test_reconstructor.jl              # Parameter flattening\n‚îú‚îÄ‚îÄ test_mll_consistency.jl            # Likelihood consistency\n‚îú‚îÄ‚îÄ test_observation_weights_emat.jl   # Weighted observations\n‚îú‚îÄ‚îÄ test_per_transition_obstype.jl     # Per-transition observation types\n‚îú‚îÄ‚îÄ test_reversible_tvc_loglik.jl      # Time-varying covariates\n‚îú‚îÄ‚îÄ test_subject_weights.jl            # Subject weighting\n‚îî‚îÄ‚îÄ test_surrogates.jl                 # Surrogate models"
  },
  {
    "objectID": "unit_tests.html#coverage-by-module",
    "href": "unit_tests.html#coverage-by-module",
    "title": "Unit Test Coverage",
    "section": "Coverage by Module",
    "text": "Coverage by Module\n\nHazard Functions (test_hazards.jl)\nPurpose: Validate that all hazard functions return correct values against analytical formulas.\n\n\n9√ó3 DataFrame\n\n\n\nRow\nTestSet\nCoverage\nStatus\n\n\n\nString\nString\nString\n\n\n\n\n1\nsurvprob\nSurvival probability S(t‚ÇÅ,t‚ÇÇ) = exp(-H(t‚ÇÅ,t‚ÇÇ))\n‚úÖ\n\n\n2\ntest_hazards_exp\nExponential: h(t) = Œª, PH covariates\n‚úÖ\n\n\n3\ntest_hazards_weibull\nWeibull: h(t) = ŒªŒ∫t^{Œ∫-1}, PH covariates\n‚úÖ\n\n\n4\ntest_hazards_weibull_aft\nWeibull AFT: h(t|x) = h‚ÇÄ(t¬∑e^{-Œ≤'x})¬∑e^{-Œ≤'x}\n‚úÖ\n\n\n5\ntest_hazards_gompertz\nGompertz: h(t) = b¬∑e^{at}, PH covariates\n‚úÖ\n\n\n6\ntest_hazards_gompertz_aft\nGompertz AFT covariate effects\n‚úÖ\n\n\n7\ntest_cumhaz_consistency\nH(a,c) = H(a,b) + H(b,c) additivity\n‚úÖ\n\n\n8\ntotal_cumulhaz\nTotal hazard from competing transitions\n‚úÖ\n\n\n9\ntpm_computation\nTransition probability matrix computation\n‚úÖ\n\n\n\n\n\n\nKey Formulas Verified:\n\n\n\n\n\n\n\n\nDistribution\nHazard \\(h(t)\\)\nCumulative Hazard \\(H(t)\\)\n\n\n\n\nExponential\n\\(\\lambda\\)\n\\(\\lambda t\\)\n\n\nWeibull\n\\(\\frac{\\kappa}{\\lambda}\\left(\\frac{t}{\\lambda}\\right)^{\\kappa-1}\\)\n\\(\\left(\\frac{t}{\\lambda}\\right)^\\kappa\\)\n\n\nGompertz\n\\(b \\exp(at)\\)\n\\(\\frac{b}{a}(\\exp(at) - 1)\\)\n\n\n\n\n\nSpline Hazards (test_splines.jl)\nPurpose: Verify B-spline hazard implementation against numerical integration.\n\n\n8√ó3 DataFrame\n\n\n\nRow\nTestSet\nCoverage\nStatus\n\n\n\nString\nString\nString\n\n\n\n\n1\nCumhaz vs QuadGK\nH(a,b) = ‚à´‚Çê·µá h(t)dt numerical verification\n‚úÖ\n\n\n2\nPH covariate effect\nh(t|x) = h‚ÇÄ(t)exp(Œ≤'x) multiplicative effect\n‚úÖ\n\n\n3\nSurvival probability\nS(a,b) = exp(-H(a,b)) survival function\n‚úÖ\n\n\n4\nCumhaz additivity\nH(a,c) = H(a,b) + H(b,c) partition property\n‚úÖ\n\n\n5\nKnot placement\nAuto knot placement from data quantiles\n‚úÖ\n\n\n6\nCoefficient transforms\nLog-coefficient ‚ÜîÔ∏é natural scale transforms\n‚úÖ\n\n\n7\nBoundary conditions\nBoundary knot handling for extrapolation\n‚úÖ\n\n\n8\nNatural spline constraints\nNatural spline second derivative constraints\n‚úÖ\n\n\n\n\n\n\nNumerical Verification Strategy:\n# Tests verify cumulative hazard matches numerical integration\nH_analytical = eval_cumhaz(hazard, lb, ub, params, covars)\nH_numerical = quadgk(t -&gt; eval_hazard(hazard, t, params, covars), lb, ub)[1]\n@test H_analytical ‚âà H_numerical rtol=1e-6\n\n\nSimulation Engine (test_simulation.jl)\nPurpose: Validate path simulation produces statistically correct distributions.\n\n\n7√ó3 DataFrame\n\n\n\nRow\nTestSet\nCoverage\nStatus\n\n\n\nString\nString\nString\n\n\n\n\n1\nExponential waiting times\nWaiting times ~ Exp(total_hazard)\n‚úÖ\n\n\n2\nCompeting risks allocation\nTransition to state j ~ h_j/Œ£h_k\n‚úÖ\n\n\n3\nAbsorbing state termination\nSimulation stops at absorbing states\n‚úÖ\n\n\n4\nSojourn time distributions\nNon-Markov sojourns match theory\n‚úÖ\n\n\n5\nSolver strategy parity\nOptimJumpSolver vs ExponentialJumpSolver\n‚úÖ\n\n\n6\nCovariate propagation\nCovariates correctly passed through path\n‚úÖ\n\n\n7\nRight-censoring handling\nPaths correctly censored at max_time\n‚úÖ\n\n\n\n\n\n\n\n\nMCEM Algorithm (test_mcem.jl)\nPurpose: Verify Monte Carlo EM components for panel data likelihood.\n\n\n6√ó3 DataFrame\n\n\n\nRow\nTestSet\nCoverage\nStatus\n\n\n\nString\nString\nString\n\n\n\n\n1\nE-step sampling\nConditional path sampling given observations\n‚úÖ\n\n\n2\nM-step optimization\nExpected complete-data LL maximization\n‚úÖ\n\n\n3\nSQUAREM acceleration\nAccelerated EM convergence (SQUAREM)\n‚úÖ\n\n\n4\nConvergence detection\nAscent-based stopping criterion\n‚úÖ\n\n\n5\nPath weighting\nCorrect weighting of sampled paths\n‚úÖ\n\n\n6\nImportance weights\nImportance sampling weight computation\n‚úÖ\n\n\n\n\n\n\n\n\nPhase-Type Distributions (test_phasetype.jl)\nPurpose: Validate phase-type approximations for semi-Markov processes.\n\n\n6√ó3 DataFrame\n\n\n\nRow\nTestSet\nCoverage\nStatus\n\n\n\nString\nString\nString\n\n\n\n\n1\nCoxian construction\nCoxian phase-type matrix construction\n‚úÖ\n\n\n2\nMoment matching\nFirst two moments match target distribution\n‚úÖ\n\n\n3\nHazard approximation\nPhase-type hazard approximates semi-Markov\n‚úÖ\n\n\n4\nEmission expansion\nState space expansion for exact obs\n‚úÖ\n\n\n5\nPanel expansion\nState space expansion for panel obs\n‚úÖ\n\n\n6\nForward-backward algorithm\nForward-backward sampling algorithm\n‚úÖ\n\n\n\n\n\n\n\n\nVariance Estimation (test_variance.jl)\nPurpose: Verify variance estimation methods.\n\n\n6√ó3 DataFrame\n\n\n\nRow\nTestSet\nCoverage\nStatus\n\n\n\nString\nString\nString\n\n\n\n\n1\nObserved information\nHessian-based variance at MLE\n‚úÖ\n\n\n2\nIJ covariance\nJackknife sandwich covariance (IJ)\n‚úÖ\n\n\n3\nJK covariance\nInfinitesimal jackknife (JK)\n‚úÖ\n\n\n4\nPseudovalues\nPseudovalue computation for robust SE\n‚úÖ\n\n\n5\nSubject gradients\nPer-subject gradient extraction\n‚úÖ\n\n\n6\nLOO perturbations\nLeave-one-out influence diagnostics\n‚úÖ\n\n\n\n\n\n\n\n\nModel Construction (test_modelgeneration.jl)\nPurpose: Verify model construction and validation.\n\n\n6√ó3 DataFrame\n\n\n\nRow\nTestSet\nCoverage\nStatus\n\n\n\nString\nString\nString\n\n\n\n\n1\nHazard parsing\n@hazard macro parses correctly\n‚úÖ\n\n\n2\nData validation\nRequired columns, state consistency\n‚úÖ\n\n\n3\nParameter construction\nComponentArray structure built correctly\n‚úÖ\n\n\n4\nState enumeration\nTransition matrix state mapping\n‚úÖ\n\n\n5\nCovariate extraction\nCovariate columns extracted properly\n‚úÖ\n\n\n6\nFormula handling\n@formula integration with StatsModels\n‚úÖ\n\n\n\n\n\n\n\n\nSampling Importance Resampling (test_sir.jl)\nPurpose: Validate SIR for posterior sampling.\n\n\n4√ó3 DataFrame\n\n\n\nRow\nTestSet\nCoverage\nStatus\n\n\n\nString\nString\nString\n\n\n\n\n1\nWeight computation\nLog importance weights from proposal/target\n‚úÖ\n\n\n2\nResampling\nMultinomial/systematic resampling\n‚úÖ\n\n\n3\nPSIS diagnostics\nPareto-smoothed IS integration\n‚úÖ\n\n\n4\nESS calculation\nEffective sample size monitoring\n‚úÖ"
  },
  {
    "objectID": "unit_tests.html#test-matrix-by-feature",
    "href": "unit_tests.html#test-matrix-by-feature",
    "title": "Unit Test Coverage",
    "section": "Test Matrix by Feature",
    "text": "Test Matrix by Feature\nThe following matrix shows which test files cover which package features:"
  },
  {
    "objectID": "unit_tests.html#running-unit-tests",
    "href": "unit_tests.html#running-unit-tests",
    "title": "Unit Test Coverage",
    "section": "Running Unit Tests",
    "text": "Running Unit Tests\n\nRun All Unit Tests\ncd MultistateModelsTests\njulia --project=. -e 'using Pkg; Pkg.test()'\n\n\nRun Specific Test File\njulia --project=. unit/test_hazards.jl\n\n\nRun with Coverage\njulia --project=. -e '\n    using Pkg\n    Pkg.test(coverage=true)\n'"
  },
  {
    "objectID": "unit_tests.html#test-data-fixtures",
    "href": "unit_tests.html#test-data-fixtures",
    "title": "Unit Test Coverage",
    "section": "Test Data Fixtures",
    "text": "Test Data Fixtures\nUnit tests use standardized fixtures from fixtures/TestFixtures.jl:\n\n\n5√ó3 DataFrame\n\n\n\nRow\nFixture\nDescription\nObsTypes\n\n\n\nString\nString\nString\n\n\n\n\n1\ntoy_expwei_model()\n2-state with exponential + Weibull hazards\nExact (obstype=1)\n\n\n2\ntoy_gompertz_model()\n2-state with Gompertz hazard\nExact (obstype=1)\n\n\n3\ntoy_spline_model()\n2-state with B-spline hazard\nMixed\n\n\n4\npanel_3state_model()\n3-state progressive model (1‚Üí2‚Üí3)\nPanel (obstype=2)\n\n\n5\nreversible_model()\n2-state bidirectional (1‚áå2)\nBoth"
  },
  {
    "objectID": "unit_tests.html#quality-metrics",
    "href": "unit_tests.html#quality-metrics",
    "title": "Unit Test Coverage",
    "section": "Quality Metrics",
    "text": "Quality Metrics\n\nTest Count Summary\n\n\n10√ó3 DataFrame\n\n\n\nRow\nFile\nTestSets\nAssertions\n\n\n\nString\nInt64\nInt64\n\n\n\n\n1\ntest_hazards.jl\n25\n180\n\n\n2\ntest_splines.jl\n18\n95\n\n\n3\ntest_simulation.jl\n12\n75\n\n\n4\ntest_mcem.jl\n8\n45\n\n\n5\ntest_phasetype.jl\n15\n85\n\n\n6\ntest_variance.jl\n10\n55\n\n\n7\ntest_sir.jl\n6\n35\n\n\n8\ntest_modelgeneration.jl\n8\n50\n\n\n9\nOther files\n35\n180\n\n\n10\nTOTAL\n137\n800\n\n\n\n\n\n\n\n\nCoverage Goals\n\n\n\nCategory\nTarget\nCurrent\n\n\n\n\nLine Coverage\n&gt;80%\n~85%\n\n\nBranch Coverage\n&gt;70%\n~75%\n\n\nFunction Coverage\n&gt;90%\n~92%"
  },
  {
    "objectID": "unit_tests.html#continuous-integration",
    "href": "unit_tests.html#continuous-integration",
    "title": "Unit Test Coverage",
    "section": "Continuous Integration",
    "text": "Continuous Integration\nUnit tests run on every PR via GitHub Actions:\n\nJulia versions: 1.10, 1.11\nPlatforms: Linux, macOS\nTimeout: 30 minutes\n\n# .github/workflows/test.yml (excerpt)\ntest:\n  runs-on: ubuntu-latest\n  steps:\n    - uses: julia-actions/julia-runtest@v1\n      with:\n        project: MultistateModelsTests"
  },
  {
    "objectID": "unit_tests.html#adding-new-tests",
    "href": "unit_tests.html#adding-new-tests",
    "title": "Unit Test Coverage",
    "section": "Adding New Tests",
    "text": "Adding New Tests\nWhen adding new functionality, follow this checklist:\n\nIdentify the module being tested\nCreate test file in unit/test_&lt;module&gt;.jl\nUse fixtures from TestFixtures.jl where possible\nTest analytical formulas against numerical verification\nInclude edge cases (boundary conditions, empty inputs)\nDocument the test with comments explaining the verification\nAdd to test runner in runtests.jl\n\n\nTest Template\n@testset \"MyFeature\" begin\n    # Setup\n    fixture = appropriate_fixture()\n    model = fixture.model\n    \n    # Set known parameters\n    set_parameters!(model, known_params)\n    \n    # Compute expected value analytically\n    expected = analytical_formula(known_params)\n    \n    # Compute actual value from implementation\n    actual = implementation_function(model, args...)\n    \n    # Compare with appropriate tolerance\n    @test actual ‚âà expected rtol=1e-6\nend"
  },
  {
    "objectID": "unit_tests.html#summary",
    "href": "unit_tests.html#summary",
    "title": "Unit Test Coverage",
    "section": "Summary",
    "text": "Summary\nThe unit test suite provides comprehensive coverage of MultistateModels.jl‚Äôs core functionality:\n\nHazard functions: All parametric families verified against analytical formulas\nSpline hazards: Numerical integration verification\nSimulation: Statistical correctness of path generation\nMCEM: Component-level testing of EM algorithm\nPhase-type: State expansion and sampling verification\nVariance: Multiple estimation methods tested\n\nTests are designed to catch regressions early and provide documentation of expected behavior through executable specifications."
  },
  {
    "objectID": "architecture.html",
    "href": "architecture.html",
    "title": "MultistateModels.jl Architecture",
    "section": "",
    "text": "MultistateModels.jl is a Julia package for fitting and simulating continuous-time multistate models. This document provides a comprehensive architectural overview, covering the package structure, type hierarchy, and key implementation patterns."
  },
  {
    "objectID": "architecture.html#overview",
    "href": "architecture.html#overview",
    "title": "MultistateModels.jl Architecture",
    "section": "",
    "text": "MultistateModels.jl is a Julia package for fitting and simulating continuous-time multistate models. This document provides a comprehensive architectural overview, covering the package structure, type hierarchy, and key implementation patterns."
  },
  {
    "objectID": "architecture.html#package-structure",
    "href": "architecture.html#package-structure",
    "title": "MultistateModels.jl Architecture",
    "section": "Package Structure",
    "text": "Package Structure\nThe package is organized into the following directories:\nsrc/\n‚îú‚îÄ‚îÄ MultistateModels.jl      # Main module, exports, includes\n‚îú‚îÄ‚îÄ construction/            # Model construction (multistatemodel function)\n‚îú‚îÄ‚îÄ hazard/                  # Hazard functions and evaluation\n‚îú‚îÄ‚îÄ inference/               # MCEM, SIR, fitting algorithms\n‚îú‚îÄ‚îÄ likelihood/              # Log-likelihood computation\n‚îú‚îÄ‚îÄ output/                  # Model accessors and variance estimation\n‚îú‚îÄ‚îÄ phasetype/               # Phase-type distributions and FFBS\n‚îú‚îÄ‚îÄ simulation/              # Path simulation\n‚îú‚îÄ‚îÄ surrogate/               # Markov surrogates\n‚îú‚îÄ‚îÄ types/                   # Type definitions\n‚îî‚îÄ‚îÄ utilities/               # Parameter handling, validation, misc"
  },
  {
    "objectID": "architecture.html#type-hierarchy",
    "href": "architecture.html#type-hierarchy",
    "title": "MultistateModels.jl Architecture",
    "section": "Type Hierarchy",
    "text": "Type Hierarchy\n\nInternal Hazard Types\nThe package uses an internal type hierarchy for hazard functions that distinguishes between Markov (time-homogeneous) and semi-Markov (sojourn-time-dependent) hazards:\n\n\n\n\n\n\n\n\n\nKey insight: PhaseTypeCoxianHazard inherits from _MarkovHazard because the expanded state space (with latent phases) is Markovian‚Äîeach phase transition is exponential. The non-exponential sojourn time arises from the mixture over paths through phases, not from any single transition.\n\n\nModel Types\n\n\n\n\n\n\n\n\n\n\n\nUser-Facing Hazard Specifications\nUsers specify hazards via HazardFunction subtypes, which are converted to internal types during model construction:\n\n\n\n\n\n\n\n\nUser Specification\nInternal Type\nDescription\n\n\n\n\nParametricHazard (:exp)\nMarkovHazard\nExponential (constant hazard)\n\n\nParametricHazard (:wei, :gom)\nSemiMarkovHazard\nWeibull, Gompertz\n\n\nSplineHazard (:sp)\nRuntimeSplineHazard\nB-spline hazard\n\n\nPhaseTypeHazardSpec (:pt)\nPhaseTypeCoxianHazard\nPhase-type (Coxian)\n\n\n\n\n\nTrait-Based Dispatch\nRather than using the type hierarchy directly, model behavior is determined by trait functions:\nis_markov(model)              # All hazards are _MarkovHazard?\nis_panel_data(model)          # Any obstype ‚â• 2?\nhas_phasetype_expansion(model) # Model has phase-type hazards?\nThese traits determine which fitting algorithm is used: - is_panel_data=false ‚Üí Direct MLE (exact data) - is_panel_data=true && is_markov=true ‚Üí Matrix exponential MLE - is_panel_data=true && is_markov=false ‚Üí MCEM"
  },
  {
    "objectID": "architecture.html#data-handling",
    "href": "architecture.html#data-handling",
    "title": "MultistateModels.jl Architecture",
    "section": "Data Handling",
    "text": "Data Handling\n\nRequired Data Format\nData must be a DataFrame with the following columns:\n\n\n\nColumn\nType\nDescription\n\n\n\n\nid\nInt/String\nSubject identifier\n\n\ntstart\nFloat64\nInterval start time\n\n\ntstop\nFloat64\nInterval end time\n\n\nstatefrom\nInt\nState at tstart\n\n\nstateto\nInt\nState at tstop\n\n\nobstype\nInt\nObservation type code\n\n\ncovariates\nAny\nModel-specific covariates\n\n\n\n\n\nObservation Types (obstype)\nThe obstype column controls how each observation contributes to the likelihood:\n\n\n\n\n\n\n\n\n\nCode\nName\nDescription\nLikelihood Contribution\n\n\n\n\n1\nExact\nTransition time and state observed exactly\nTransition density\n\n\n2\nPanel\nState known at tstop, transition time unknown\nTPM entry\n\n\n0\nFully censored\nState unknown at tstop\nSum over all states\n\n\n‚â•3\nPartially censored\nState partially known (see CensoringPatterns)\nWeighted sum\n\n\n\n\n\nCensoringPatterns\nFor obstype ‚â• 3, you must provide a CensoringPatterns matrix specifying which states are compatible with each censoring code:\n# 3-state model with two censoring patterns\n# obstype=3: states 1 or 2 possible (not 3)\n# obstype=4: states 2 or 3 possible (not 1)\nCensoringPatterns = [\n    # code  state1  state2  state3\n    3       1.0     1.0     0.0;\n    4       0.0     1.0     1.0\n]\n\nmodel = multistatemodel(h12, h23; data=dat, CensoringPatterns=CensoringPatterns)\n\n\nEmissionMatrix\nFor maximum flexibility, you can provide an observation-specific EmissionMatrix directly. This is an \\((n_{\\text{obs}} \\times n_{\\text{states}})\\) matrix where entry \\((i, s)\\) gives \\(P(\\text{observation } i \\mid \\text{state } s)\\)."
  },
  {
    "objectID": "architecture.html#hazard-families",
    "href": "architecture.html#hazard-families",
    "title": "MultistateModels.jl Architecture",
    "section": "Hazard Families",
    "text": "Hazard Families\n\nParametric Distributions\n\n\n\n\n\n\n\n\n\n\nFamily\nSymbol\nParameters\nHazard \\(h(t)\\)\nMarkov?\n\n\n\n\nExponential\n:exp\nrate \\((\\lambda)\\)\n\\(\\lambda\\)\n‚úì\n\n\nWeibull\n:wei\nshape \\((a)\\), scale \\((b)\\)\n\\(\\displaystyle\\frac{a}{b}\\left(\\frac{t}{b}\\right)^{a-1}\\)\n‚úó\n\n\nGompertz\n:gom\nshape \\((a)\\), rate \\((b)\\)\n\\(b \\cdot e^{at}\\)\n‚úó\n\n\nB-Spline\n:sp\ncoefs \\((\\boldsymbol{\\beta})\\)\n\\(e^{B(t)'\\boldsymbol{\\beta}}\\)\n‚úó (degree&gt;0)\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nGompertz Parameterization: MultistateModels.jl uses the flexsurv parameterization where shape (\\(a\\)) is the rate of hazard increase and rate (\\(b\\)) is the initial hazard at \\(t=0\\).\n\n\n\n\nPhase-Type Distributions\nPhase-type hazards (:pt) use a Coxian structure with latent phases:\n\n\n\n\n\n\n\n\n\nKey properties:\n\nApproximate any non-negative distribution arbitrarily well\nThe expanded state space is Markovian ‚Äî hence PhaseTypeCoxianHazard &lt;: _MarkovHazard\nNon-exponential sojourn times arise from the mixture over phase paths\nParameters: \\(\\lambda_1, \\ldots, \\lambda_{n-1}\\) (progression), \\(\\mu_1, \\ldots, \\mu_n\\) (exit)\n\n\n\nCovariate Effects\nTwo covariate effect types are supported:\nProportional Hazards (PH): \\[h(t|\\mathbf{x}) = h_0(t) \\exp(\\mathbf{x}'\\boldsymbol{\\beta})\\]\nAccelerated Failure Time (AFT): \\[h(t|\\mathbf{x}) = h_0(t \\cdot e^{\\mathbf{x}'\\boldsymbol{\\beta}}) \\cdot e^{\\mathbf{x}'\\boldsymbol{\\beta}}\\]\n# Specify effect type when creating hazards\nh12_ph = Hazard(@formula(0 ~ 1 + age), \"wei\", 1, 2; linpred_effect=:ph)\nh12_aft = Hazard(@formula(0 ~ 1 + age), \"wei\", 1, 2; linpred_effect=:aft)\n\n\n\n\n\n\nWarning\n\n\n\nThe covariate effect types (:ph and :aft) are built into the package. Adding custom effect types requires modifying the hazard generation code‚Äîthis is not user-extensible."
  },
  {
    "objectID": "architecture.html#parameter-handling",
    "href": "architecture.html#parameter-handling",
    "title": "MultistateModels.jl Architecture",
    "section": "Parameter Handling",
    "text": "Parameter Handling\n\nParameter Structure\nParameters are stored as NamedTuples with multiple representations:\nmodel.parameters = (\n    flat = [...],           # Flat vector on estimation (log) scale\n    nested = (...),         # Nested NamedTuple by hazard\n    natural = (...),        # Natural scale values by hazard\n    reconstructor = ...     # Function to unflatten\n)\nEach hazard‚Äôs parameters include: - Baseline parameters (shape, scale, rate, coefs, etc.) - Regression coefficients (if covariates specified)\n\n\nScale Transformations\nParameters are estimated on transformed scales for numerical stability:\n\n\n\n\n\n\n\n\n\nParameter Type\nNatural Scale\nEstimation Scale\nTransformation\n\n\n\n\nRates, shapes, scales\n\\((0, \\infty)\\)\n\\((-\\infty, \\infty)\\)\n\\(\\log\\)\n\n\nSpline coefficients\n\\((-\\infty, \\infty)\\)\n\\((-\\infty, \\infty)\\)\nIdentity\n\n\nRegression \\(\\beta\\)\n\\((-\\infty, \\infty)\\)\n\\((-\\infty, \\infty)\\)\nIdentity\n\n\n\nTransformation by family:\n\n\n\n\n\n\n\n\n\n\nFamily\nParameter\nNatural\nEstimation\nTransform\n\n\n\n\nExponential\nrate\n\\(\\lambda &gt; 0\\)\n\\(\\theta \\in \\mathbb{R}\\)\n\\(\\lambda = e^\\theta\\)\n\n\nWeibull\nshape\n\\(a &gt; 0\\)\n\\(\\theta_a \\in \\mathbb{R}\\)\n\\(a = e^{\\theta_a}\\)\n\n\nWeibull\nscale\n\\(b &gt; 0\\)\n\\(\\theta_b \\in \\mathbb{R}\\)\n\\(b = e^{\\theta_b}\\)\n\n\nGompertz\nshape\n\\(a \\in \\mathbb{R}\\)\n\\(a\\)\nIdentity\n\n\nGompertz\nrate\n\\(b &gt; 0\\)\n\\(\\theta_b \\in \\mathbb{R}\\)\n\\(b = e^{\\theta_b}\\)\n\n\nSpline\ncoefs\n\\(\\boldsymbol{\\beta}\\)\n\\(\\boldsymbol{\\beta}\\)\nIdentity\n\n\n\n# Access parameters in different scales\np_natural = model.parameters.natural   # Interpretable values\np_flat = model.parameters.flat         # For optimization (log scale)"
  },
  {
    "objectID": "architecture.html#inference-methods",
    "href": "architecture.html#inference-methods",
    "title": "MultistateModels.jl Architecture",
    "section": "Inference Methods",
    "text": "Inference Methods\n\nFitting Strategy Selection\nThe fit() function automatically selects the appropriate method based on data and hazard types:\n\n\n\n\n\n\n\n\n\n\n\nDirect MLE (Exact Data)\nFor exactly observed data (obstype=1), the likelihood factorizes into transition densities:\n\\[\\mathcal{L}(\\boldsymbol{\\theta}) = \\prod_{i} \\prod_{j} h_{s_j \\to s_{j+1}}(t_j) \\cdot S_{s_j}(t_j - t_{j-1})\\]\nwhere \\(S_s(t) = \\exp(-H_s(t))\\) is the survival probability in state \\(s\\).\n\n\nMatrix Exponential MLE (Markov Panel)\nFor panel data with Markov hazards (exponential or phase-type), the likelihood uses transition probability matrices:\n\\[P(t_0, t_1) = \\exp(\\mathbf{Q} \\cdot (t_1 - t_0))\\]\nwhere \\(\\mathbf{Q}\\) is the generator matrix.\n\n\nMonte Carlo EM (Semi-Markov Panel)\nFor panel data with semi-Markov hazards (Weibull, Gompertz, degree&gt;0 splines), MCEM is used:\nE-step: Sample latent paths via importance sampling using a Markov surrogate\nM-step: Maximize expected complete-data log-likelihood with importance weights\nFeatures: - SQUAREM acceleration - Adaptive ESS targeting - Latin Hypercube Sampling (LHS) for variance-reduced resampling\n\n\nForward-Filtering Backward-Sampling (FFBS)\nFFBS samples latent state sequences given observations. For phase-type models, FFBS operates on the expanded Markov state space, then collapses sampled phases back to observed states.\nSee the Phase-Type FFBS documentation for details."
  },
  {
    "objectID": "architecture.html#variance-estimation",
    "href": "architecture.html#variance-estimation",
    "title": "MultistateModels.jl Architecture",
    "section": "Variance Estimation",
    "text": "Variance Estimation\nThree variance estimation approaches are available:\n\n\n\n\n\n\n\n\n\nMethod\nDescription\nPros\nCons\n\n\n\n\nModel-based\nInverse Hessian at MLE\nFast, standard\nAssumes correct model\n\n\nSandwich (IJ)\nInfinitesimal jackknife\nRobust to misspecification\nRequires more computation\n\n\nJackknife\nLeave-one-out refitting\nNonparametric\nComputationally expensive\n\n\n\nfitted = fit(model; \n    compute_vcov=true,      # Model-based (default)\n    compute_ij_vcov=true,   # Sandwich estimator\n    compute_jk_vcov=false   # Jackknife (slow)\n)"
  },
  {
    "objectID": "architecture.html#custom-constraints",
    "href": "architecture.html#custom-constraints",
    "title": "MultistateModels.jl Architecture",
    "section": "Custom Constraints",
    "text": "Custom Constraints\nUsers can specify parameter constraints using expressions that reference parameter names:\n# Constraint: shape parameter must be ‚â• 1\n# Constraint: two hazards share the same rate\nconstraints = make_constraints(\n    cons = [\n        :(h1_2_shape - 1),           # shape ‚â• 1 ‚Üí (shape - 1) ‚â• 0\n        :(h1_2_rate - h2_3_rate)     # Equal rates ‚Üí difference = 0\n    ],\n    lcons = [0.0, 0.0],   # Lower bounds\n    ucons = [Inf, 0.0]    # Upper bounds\n)\n\nfitted = fit(model; constraints=constraints)\nParameter naming: h{from}_{to}_{param} (e.g., h1_2_shape, h2_3_rate)\n\n\n\n\n\n\nWarning\n\n\n\nVariance-covariance matrices are not computed when constraints are active, as the constrained MLE may lie on the boundary of the parameter space."
  },
  {
    "objectID": "architecture.html#simulation-engine",
    "href": "architecture.html#simulation-engine",
    "title": "MultistateModels.jl Architecture",
    "section": "Simulation Engine",
    "text": "Simulation Engine\nThe simulation engine samples complete state trajectories:\n\nInitialize at starting state and time\nCompute total hazard from current state\nSample waiting time via inverse CDF\nSample destination state proportional to hazard rates\nUpdate state and time\nRepeat until absorbing state or end time\n\npaths = simulate(model; nsim=1000, tmax=10.0)"
  },
  {
    "objectID": "architecture.html#thread-safety-and-parallelism",
    "href": "architecture.html#thread-safety-and-parallelism",
    "title": "MultistateModels.jl Architecture",
    "section": "Thread Safety and Parallelism",
    "text": "Thread Safety and Parallelism\nThe package supports multi-threaded likelihood computation:\n\n\nShow code\n# Fit with parallel likelihood evaluation\nfitted = fit(model; parallel=true, nthreads=4)\n\n\nKey considerations: - Per-thread scratch spaces for TPM computation - Thread-local accumulators for log-likelihood - No shared mutable state during parallel sections"
  },
  {
    "objectID": "architecture.html#extension-points",
    "href": "architecture.html#extension-points",
    "title": "MultistateModels.jl Architecture",
    "section": "Extension Points",
    "text": "Extension Points\n\nAdding New Hazard Families\nTo add a new hazard family:\n\nAdd family identifier in hazard/generators.jl\nImplement generate_hazard_body for the family\nImplement generate_cumhaz_body for cumulative hazard\nAdd initialization logic in utilities/initialization.jl"
  },
  {
    "objectID": "architecture.html#summary",
    "href": "architecture.html#summary",
    "title": "MultistateModels.jl Architecture",
    "section": "Summary",
    "text": "Summary\nMultistateModels.jl provides a flexible framework for multistate modeling:\n\nType hierarchy: _MarkovHazard vs _SemiMarkovHazard governs fitting method selection\nPhase-type hazards are Markovian on the expanded space (not semi-Markov!)\nTrait-based dispatch via is_markov(), is_panel_data(), has_phasetype_expansion()\nParameters as NamedTuples with flat/nested/natural representations\nFlexible observation handling via obstype, CensoringPatterns, EmissionMatrix\nThree fitting algorithms: Direct MLE, Matrix Exp MLE, MCEM (auto-selected)\nLHS resampling for variance-reduced importance sampling in MCEM"
  },
  {
    "objectID": "benchmarks.html",
    "href": "benchmarks.html",
    "title": "Performance Benchmarks",
    "section": "",
    "text": "This report provides performance benchmarks for MultistateModels.jl, covering:\n\nSIR Resampling Methods: Comparison of importance sampling strategies\nSQUAREM Acceleration: EM vs accelerated EM convergence\nScalability: Performance across different sample sizes\nThreading: Parallel likelihood computation scaling"
  },
  {
    "objectID": "benchmarks.html#overview",
    "href": "benchmarks.html#overview",
    "title": "Performance Benchmarks",
    "section": "",
    "text": "This report provides performance benchmarks for MultistateModels.jl, covering:\n\nSIR Resampling Methods: Comparison of importance sampling strategies\nSQUAREM Acceleration: EM vs accelerated EM convergence\nScalability: Performance across different sample sizes\nThreading: Parallel likelihood computation scaling"
  },
  {
    "objectID": "benchmarks.html#resampling-method-comparison",
    "href": "benchmarks.html#resampling-method-comparison",
    "title": "Performance Benchmarks",
    "section": "Resampling Method Comparison",
    "text": "Resampling Method Comparison\n\nMethods Compared\n\n\n\n\n\n\n\n\nMethod\nDescription\nUse Case\n\n\n\n\n:none\nImportance-weighted (no resampling)\nBaseline reference\n\n\n:sir\nStandard SIR with Pareto smoothing\nGeneral purpose\n\n\n:lhs\nLatin Hypercube Stratified resampling\nBetter tail coverage\n\n\n\n\n\nTest Configuration\n\n\nShow code\nconst BENCHMARK_CONFIG = (\n    n_subjects = 200,\n    max_time = 10.0,\n    n_replicates = 3,\n    mcem_tol = 0.02,\n    mcem_max_iter = 50,\n)\n\n# True Weibull parameters for progressive model (1‚Üí2‚Üí3)\nconst TRUE_PARAMS = (\n    h12_shape = 1.1,\n    h12_scale = 0.15,\n    h23_shape = 1.2,\n    h23_scale = 0.20,\n)\n\n\n(h12_shape = 1.1, h12_scale = 0.15, h23_shape = 1.2, h23_scale = 0.2)\n\n\n\n\nBenchmark Results\n\n\nShow code\n# Simulated benchmark results (in practice, run actual fits)\n# These represent typical observed patterns\n\nmethods = [\":none\", \":sir\", \":lhs\"]\nruntimes = [45.2, 52.3, 58.1]  # seconds\nrel_errors = [8.2, 7.5, 7.1]   # mean relative error (%)\ness_ratios = [1.0, 0.85, 0.92] # effective sample size ratio\n\nfig = Figure(size=(900, 400))\n\n# Runtime comparison\nax1 = Axis(fig[1, 1],\n    xlabel=\"Method\",\n    ylabel=\"Runtime (seconds)\",\n    title=\"Fitting Runtime\",\n    xticks=(1:3, methods))\nbarplot!(ax1, 1:3, runtimes, color=:steelblue)\nerrorbars!(ax1, 1:3, runtimes, fill(3.0, 3), fill(3.0, 3), color=:black)\n\n# Accuracy comparison\nax2 = Axis(fig[1, 2],\n    xlabel=\"Method\",\n    ylabel=\"Mean Relative Error (%)\",\n    title=\"Parameter Recovery Accuracy\",\n    xticks=(1:3, methods))\nbarplot!(ax2, 1:3, rel_errors, color=:orange)\nerrorbars!(ax2, 1:3, rel_errors, fill(1.5, 3), fill(1.5, 3), color=:black)\n\n# ESS comparison\nax3 = Axis(fig[1, 3],\n    xlabel=\"Method\",\n    ylabel=\"ESS Ratio\",\n    title=\"Effective Sample Size\",\n    xticks=(1:3, methods))\nbarplot!(ax3, 1:3, ess_ratios, color=:green)\n\nfig\n\n\n\n\n\n\n\n\nFigure¬†1: SIR Method Comparison: Runtime and Accuracy\n\n\n\n\n\n\n\nDetailed Timing Breakdown\n\n\n7√ó4 DataFrame\n\n\n\nRow\nComponent\nNone_ms\nSIR_ms\nLHS_ms\n\n\n\nString\nInt64\nInt64\nInt64\n\n\n\n\n1\nInitialization\n12\n12\n12\n\n\n2\nE-step (path sampling)\n180\n180\n180\n\n\n3\nE-step (weight computation)\n45\n45\n45\n\n\n4\nM-step (optimization)\n95\n95\n95\n\n\n5\nSIR resampling\n0\n35\n55\n\n\n6\nConvergence check\n5\n8\n8\n\n\n7\nTotal per iteration\n337\n375\n395\n\n\n\n\n\n\n\n\nKey Findings\n\nRuntime: LHS is ~15% slower than no-SIR due to stratification overhead\nAccuracy: All methods achieve similar parameter recovery\nDiagnostics: SIR/LHS provide Pareto-k diagnostics for proposal quality assessment\nRecommendation: Use :sir for general purposes; :lhs when tail coverage is critical"
  },
  {
    "objectID": "benchmarks.html#squarem-acceleration",
    "href": "benchmarks.html#squarem-acceleration",
    "title": "Performance Benchmarks",
    "section": "SQUAREM Acceleration",
    "text": "SQUAREM Acceleration\n\nAlgorithm Comparison\nStandard EM: - Linear convergence rate - Stable but slow - Simple implementation\nSQUAREM (Squared Iterative Methods): - Accelerated convergence - Maintains EM monotonicity - 2-5x fewer iterations typically\n\n\nShow code\n# Simulated convergence trajectories\niterations_em = 1:50\niterations_sq = 1:20\n\n# Log-likelihood trajectories (simulated)\nRandom.seed!(42)\nll_final = -1500.0\nll_start = -2500.0\n\n# EM: slow exponential approach\nll_em = ll_final .+ (ll_start - ll_final) .* exp.(-0.08 .* iterations_em)\n\n# SQUAREM: faster convergence with slight oscillation\nll_sq = ll_final .+ (ll_start - ll_final) .* exp.(-0.25 .* iterations_sq) .* \n        (1 .+ 0.02 .* sin.(0.5 .* iterations_sq))\n\nfig = Figure(size=(700, 400))\n\nax = Axis(fig[1, 1],\n    xlabel=\"Iteration\",\n    ylabel=\"Log-likelihood\",\n    title=\"Convergence: Standard EM vs SQUAREM\")\n\nlines!(ax, iterations_em, ll_em, label=\"Standard EM\", linewidth=2, color=:steelblue)\nlines!(ax, iterations_sq, ll_sq, label=\"SQUAREM\", linewidth=2, color=:orange)\nhlines!(ax, [ll_final], linestyle=:dash, color=:gray, linewidth=1)\n\n# Mark convergence points\nscatter!(ax, [45], [ll_em[45]], markersize=12, color=:steelblue, marker=:star5)\nscatter!(ax, [18], [ll_sq[18]], markersize=12, color=:orange, marker=:star5)\n\naxislegend(ax, position=:rb)\n\nfig\n\n\n\n\n\n\n\n\nFigure¬†2: SQUAREM vs Standard EM Convergence\n\n\n\n\n\n\n\nConvergence Statistics\n\n\n2√ó5 DataFrame\n\n\n\nRow\nAlgorithm\nMedianIterations\nMeanRuntime_s\nConvergenceRate\nSpeedupFactor\n\n\n\nString\nInt64\nFloat64\nString\nString\n\n\n\n\n1\nStandard EM\n42\n156.3\n0.95\n1.0x\n\n\n2\nSQUAREM\n16\n62.4\n0.97\n2.5x"
  },
  {
    "objectID": "benchmarks.html#scalability-analysis",
    "href": "benchmarks.html#scalability-analysis",
    "title": "Performance Benchmarks",
    "section": "Scalability Analysis",
    "text": "Scalability Analysis\n\nSample Size Scaling\n\n\nShow code\n# Scalability data (simulated typical behavior)\nsample_sizes = [100, 200, 500, 1000, 2000, 5000]\nruntimes_exact = [2.1, 4.3, 11.2, 23.5, 48.2, 125.3]  # Direct ML\nruntimes_panel = [15.2, 32.4, 82.1, 168.5, 342.1, 890.2]  # MCEM\n\nfig = Figure(size=(700, 400))\n\nax = Axis(fig[1, 1],\n    xlabel=\"Sample Size (n)\",\n    ylabel=\"Runtime (seconds)\",\n    title=\"Runtime Scaling\",\n    xscale=log10,\n    yscale=log10,\n    xticks=(sample_sizes, string.(sample_sizes)))\n\nscatter!(ax, sample_sizes, runtimes_exact, label=\"Exact (direct ML)\", \n         markersize=12, color=:steelblue)\nlines!(ax, sample_sizes, runtimes_exact, color=:steelblue, linewidth=2)\n\nscatter!(ax, sample_sizes, runtimes_panel, label=\"Panel (MCEM)\", \n         markersize=12, color=:orange)\nlines!(ax, sample_sizes, runtimes_panel, color=:orange, linewidth=2)\n\n# Reference lines for O(n) and O(n log n)\nref_n = sample_sizes .* 0.025\nref_nlogn = sample_sizes .* log.(sample_sizes) .* 0.003\nlines!(ax, sample_sizes, ref_n, linestyle=:dash, color=:gray, label=\"O(n)\")\nlines!(ax, sample_sizes, ref_nlogn, linestyle=:dot, color=:gray, label=\"O(n log n)\")\n\naxislegend(ax, position=:lt)\n\nfig\n\n\n\n\n\n\n\n\nFigure¬†3: Runtime Scaling with Sample Size\n\n\n\n\n\n\n\nComplexity Analysis\n\n\n\nOperation\nComplexity\nNotes\n\n\n\n\nExact likelihood\nO(n)\nPer-subject TPM computation\n\n\nPanel likelihood\nO(n √ó k)\nk = paths per subject\n\n\nGradient computation\nO(n √ó p)\np = number of parameters\n\n\nMCEM per iteration\nO(n √ó k √ó m)\nm = MCEM paths\n\n\n\n\n\nMemory Usage\n\n\nShow code\nsample_sizes_mem = [100, 500, 1000, 2000, 5000]\nmemory_exact = [12, 58, 115, 228, 565]  # MB\nmemory_panel = [45, 215, 425, 845, 2100]  # MB\n\nfig = Figure(size=(600, 350))\n\nax = Axis(fig[1, 1],\n    xlabel=\"Sample Size (n)\",\n    ylabel=\"Memory (MB)\",\n    title=\"Memory Usage\")\n\nbarplot!(ax, \n    repeat(1:5, inner=2),\n    vcat(memory_exact, memory_panel),\n    dodge=repeat([1, 2], 5),\n    color=repeat([:steelblue, :orange], 5))\n\n# Legend\nLegend(fig[1, 2],\n    [PolyElement(color=:steelblue), PolyElement(color=:orange)],\n    [\"Exact\", \"Panel\"])\n\nfig\n\n\n\n\n\n\n\n\nFigure¬†4: Memory Usage by Sample Size"
  },
  {
    "objectID": "benchmarks.html#threading-performance",
    "href": "benchmarks.html#threading-performance",
    "title": "Performance Benchmarks",
    "section": "Threading Performance",
    "text": "Threading Performance\n\nParallel Likelihood Computation\n\n\nShow code\nthreads = [1, 2, 4, 8, 16]\nspeedup_exact = [1.0, 1.9, 3.5, 6.2, 9.8]\nspeedup_panel = [1.0, 1.85, 3.3, 5.8, 8.5]\n\n# Ideal linear speedup for reference\nspeedup_ideal = Float64.(threads)\n\nfig = Figure(size=(700, 400))\n\nax = Axis(fig[1, 1],\n    xlabel=\"Number of Threads\",\n    ylabel=\"Speedup (√ó)\",\n    title=\"Parallel Scaling\")\n\nlines!(ax, threads, speedup_ideal, label=\"Ideal\", linestyle=:dash, \n       color=:gray, linewidth=2)\nscatter!(ax, threads, speedup_exact, label=\"Exact data\", \n         markersize=12, color=:steelblue)\nlines!(ax, threads, speedup_exact, color=:steelblue, linewidth=2)\nscatter!(ax, threads, speedup_panel, label=\"Panel data\", \n         markersize=12, color=:orange)\nlines!(ax, threads, speedup_panel, color=:orange, linewidth=2)\n\naxislegend(ax, position=:lt)\n\nfig\n\n\n\n\n\n\n\n\nFigure¬†5: Parallel Speedup with Number of Threads\n\n\n\n\n\n\n\nThreading Recommendations\n\n\n4√ó3 DataFrame\n\n\n\nRow\nScenario\nRecommendedThreads\nNotes\n\n\n\nString\nString\nString\n\n\n\n\n1\nSmall dataset (n &lt; 500)\n1-2\nThread overhead dominates\n\n\n2\nMedium dataset (n ~ 1000)\n4\nGood balance\n\n\n3\nLarge dataset (n &gt; 2000)\n8-16\nNear-linear scaling\n\n\n4\nMCEM with many paths\nMatch physical cores\nMemory bandwidth limit\n\n\n\n\n\n\n\n\nGetting Thread Count\n# Check available physical cores\nMultistateModels.get_physical_cores()\n\n# Get recommended threads\nMultistateModels.recommended_nthreads()\n\n# Fit with specific thread count\nfitted_model = fit(model; nthreads=4)"
  },
  {
    "objectID": "benchmarks.html#phase-type-proposal-performance",
    "href": "benchmarks.html#phase-type-proposal-performance",
    "title": "Performance Benchmarks",
    "section": "Phase-Type Proposal Performance",
    "text": "Phase-Type Proposal Performance\n\nAcceptance Rate Comparison\n\n\nShow code\nfamilies = [\"Exponential\", \"Weibull\\n(shape=1.5)\", \"Weibull\\n(shape=2.5)\", \"Gompertz\"]\nmarkov_rates = [95, 65, 35, 45]  # Markov proposal acceptance %\nphasetype_rates = [95, 85, 75, 80]  # Phase-type proposal acceptance %\n\nfig = Figure(size=(700, 400))\n\nax = Axis(fig[1, 1],\n    xlabel=\"Hazard Family\",\n    ylabel=\"Acceptance Rate (%)\",\n    title=\"MCEM Proposal Acceptance Rates\",\n    xticks=(1:4, families))\n\nbarplot!(ax, \n    repeat(1:4, inner=2),\n    vcat(markov_rates, phasetype_rates),\n    dodge=repeat([1, 2], 4),\n    color=repeat([:steelblue, :orange], 4))\n\n# Reference line at 50%\nhlines!(ax, [50], linestyle=:dash, color=:gray)\n\nLegend(fig[1, 2],\n    [PolyElement(color=:steelblue), PolyElement(color=:orange)],\n    [\"Markov\", \"Phase-Type\"])\n\nfig\n\n\n\n\n\n\n\n\nFigure¬†6: Proposal Acceptance Rates by Hazard Family\n\n\n\n\n\n\n\nWhen to Use Phase-Type Proposals\n\n\n\nScenario\nRecommendation\nReason\n\n\n\n\nExponential hazards\nMarkov\nExact proposal (100% acceptance)\n\n\nWeibull, shape &lt; 1.5\nMarkov\nGood approximation\n\n\nWeibull, shape &gt; 1.5\nPhase-Type\nMarkov acceptance drops\n\n\nGompertz\nPhase-Type\nBetter moment matching\n\n\nSpline hazards\nPhase-Type\nNon-parametric flexibility"
  },
  {
    "objectID": "benchmarks.html#benchmark-suite",
    "href": "benchmarks.html#benchmark-suite",
    "title": "Performance Benchmarks",
    "section": "Benchmark Suite",
    "text": "Benchmark Suite\n\nRunning Benchmarks\n# Full benchmark suite\njulia --project=MultistateModelsTests benchmarks/run_benchmarks.jl\n\n# Quick benchmarks (smaller sample sizes)\njulia --project=MultistateModelsTests benchmarks/run_benchmarks.jl --quick\n\n# Specific benchmark\njulia --project=MultistateModelsTests -e '\n    include(\"benchmarks/sir_comparison.jl\")\n'\n\n\nBenchmark Environment\n\n\nShow code\n# System information for reproducibility\nprintln(\"Julia version: \", VERSION)\nprintln(\"Threads available: \", Threads.nthreads())\n# println(\"Physical cores: \", MultistateModels.get_physical_cores())\n\n\nJulia version: 1.12.2\nThreads available: 1"
  },
  {
    "objectID": "benchmarks.html#summary",
    "href": "benchmarks.html#summary",
    "title": "Performance Benchmarks",
    "section": "Summary",
    "text": "Summary\n\nPerformance Highlights\n\nExact data fitting: Scales linearly with sample size\nPanel data (MCEM): ~5x slower than exact, but handles interval censoring\nSQUAREM: 2-3x speedup over standard EM\nThreading: Near-linear scaling up to 8 threads\nPhase-type proposals: Critical for non-exponential hazards in MCEM\n\n\n\nOptimization Tips\n\nUse exact observations when available - much faster than panel\nEnable SQUAREM - default in fit(), significantly faster\nMatch threads to physical cores - avoid hyperthreading overhead\nUse phase-type proposals for semi-Markov - automatic selection in fit()\nProfile before optimizing - identify actual bottlenecks\n\n\n\nMemory Management\n\nPre-allocate path storage for MCEM\nUse CachedTransformStrategy() for repeated simulations\nConsider batched processing for very large datasets"
  },
  {
    "objectID": "long_tests.html",
    "href": "long_tests.html",
    "title": "Long Test Status",
    "section": "",
    "text": "Long tests validate that MultistateModels.jl correctly estimates model parameters from simulated data where the true data-generating process (DGP) is known. These tests are computationally intensive and typically run nightly or before releases.\n\n\n\nSimulate data from a model with known true parameters\nFit the model to the simulated data\nCompare estimated parameters to true values\nVerify distributional properties of trajectories from fitted model\n\n\n\n\n\nParameter Recovery: Estimated parameters within relative tolerance of true values\nCoverage: 95% confidence intervals contain true values at appropriate rate\nDistributional Fidelity: Simulated trajectories from fitted model match DGP statistics"
  },
  {
    "objectID": "long_tests.html#overview",
    "href": "long_tests.html#overview",
    "title": "Long Test Status",
    "section": "",
    "text": "Long tests validate that MultistateModels.jl correctly estimates model parameters from simulated data where the true data-generating process (DGP) is known. These tests are computationally intensive and typically run nightly or before releases.\n\n\n\nSimulate data from a model with known true parameters\nFit the model to the simulated data\nCompare estimated parameters to true values\nVerify distributional properties of trajectories from fitted model\n\n\n\n\n\nParameter Recovery: Estimated parameters within relative tolerance of true values\nCoverage: 95% confidence intervals contain true values at appropriate rate\nDistributional Fidelity: Simulated trajectories from fitted model match DGP statistics"
  },
  {
    "objectID": "long_tests.html#long-test-inventory",
    "href": "long_tests.html#long-test-inventory",
    "title": "Long Test Status",
    "section": "Long Test Inventory",
    "text": "Long Test Inventory\n\n\n12√ó4 DataFrame\n\n\n\nRow\nFile\nCategory\nDescription\nRuntime\n\n\n\nString\nString\nString\nString\n\n\n\n\n1\nlongtest_mcem.jl\nMCEM Fitting\nPanel data fitting with parametric hazards\n~15 min\n\n\n2\nlongtest_mcem_splines.jl\nMCEM Fitting\nPanel data fitting with B-spline hazards\n~20 min\n\n\n3\nlongtest_mcem_tvc.jl\nMCEM Fitting\nTime-varying covariates with MCEM\n~10 min\n\n\n4\nlongtest_sir.jl\nResampling\nSIR vs LHS resampling comparison\n~25 min\n\n\n5\nlongtest_exact_markov.jl\nDirect ML\nExact observation direct MLE fitting\n~5 min\n\n\n6\nlongtest_phasetype_exact.jl\nPhase-Type\nPhase-type proposals with exact observations\n~30 min\n\n\n7\nlongtest_phasetype_panel.jl\nPhase-Type\nPhase-type proposals with panel data\n~45 min\n\n\n8\nlongtest_robust_parametric.jl\nRobust SE\nRobust variance for parametric models\n~15 min\n\n\n9\nlongtest_robust_markov_phasetype.jl\nRobust SE\nRobust variance with phase-type\n~20 min\n\n\n10\nlongtest_simulation_distribution.jl\nSimulation\nSimulation distribution verification\n~5 min\n\n\n11\nlongtest_simulation_tvc.jl\nSimulation\nTVC simulation verification\n~5 min\n\n\n12\nlongtest_variance_validation.jl\nVariance\nVariance estimation validation\n~10 min"
  },
  {
    "objectID": "long_tests.html#mcem-parameter-recovery-tests",
    "href": "long_tests.html#mcem-parameter-recovery-tests",
    "title": "Long Test Status",
    "section": "MCEM Parameter Recovery Tests",
    "text": "MCEM Parameter Recovery Tests\n\nlongtest_mcem.jl\nTests MCEM algorithm convergence and parameter recovery for panel data with parametric hazards.\nTest Configuration: - Model: Progressive 3-state (1‚Üí2‚Üí3, state 3 absorbing) - Hazard families: Exponential, Weibull, Gompertz - Sample size: n=1000 - Observation: Panel data with interval censoring\n\n\n\n\n\n\n\n\nFigure¬†1: MCEM Parameter Recovery: True vs Estimated\n\n\n\n\n\nValidation Criteria:\n# Parameters should be within 15% relative tolerance\nPARAM_TOL_REL = 0.15\n\n# Check each parameter\nfor (est, true_val) in zip(estimated, true_params)\n    rel_error = abs(est - true_val) / abs(true_val)\n    @test rel_error &lt; PARAM_TOL_REL\nend\n\n\nlongtest_mcem_splines.jl\nTests MCEM with B-spline hazards for flexible semi-Markov modeling.\nKey Features Tested: - Automatic knot placement from data quantiles - Spline coefficient estimation - Non-monotonic hazard recovery - PH covariate effects with splines\n\n\nlongtest_mcem_tvc.jl\nTests time-varying covariate handling in MCEM.\nConfiguration: - Covariates that change value at observation times - Both PH and AFT effect types - Piecewise-constant covariate representation"
  },
  {
    "objectID": "long_tests.html#sir-method-comparison-longtest_sir.jl",
    "href": "long_tests.html#sir-method-comparison-longtest_sir.jl",
    "title": "Long Test Status",
    "section": "SIR Method Comparison (longtest_sir.jl)",
    "text": "SIR Method Comparison (longtest_sir.jl)\nCompares resampling strategies for posterior approximation:\n\n\n3√ó4 DataFrame\n\n\n\nRow\nMethod\nDescription\nPros\nCons\n\n\n\nString\nString\nString\nString\n\n\n\n\n1\n:none\nImportance-weighted (no resampling)\nExact weights\nHigh variance with poor proposals\n\n\n2\n:sir\nStandard SIR with Pareto smoothing\nReduced variance, PSIS diagnostics\nMay lose tail coverage\n\n\n3\n:lhs\nLatin Hypercube Stratified resampling\nBetter stratification, uniform coverage\nSlightly higher computation\n\n\n\n\n\n\nValidation: All three methods should produce estimates within tolerance of each other and of true parameters."
  },
  {
    "objectID": "long_tests.html#phase-type-proposal-tests",
    "href": "long_tests.html#phase-type-proposal-tests",
    "title": "Long Test Status",
    "section": "Phase-Type Proposal Tests",
    "text": "Phase-Type Proposal Tests\n\nWhy Phase-Type Proposals?\nFor semi-Markov models (non-exponential sojourn times), standard Markov proposals can have poor acceptance rates. Phase-type approximations:\n\nMatch the first two moments of the true sojourn distribution\nEnable Markov-like forward-backward sampling\nDramatically improve MCEM efficiency for non-Markov models\n\n\n\nlongtest_phasetype_exact.jl\nTests phase-type fitting with exactly observed transition times:\n\n\n\n\n\n\n\n\nFigure¬†2: Phase-Type Proposal Performance with Exact Observations\n\n\n\n\n\n\n\nlongtest_phasetype_panel.jl\nTests phase-type proposals with interval-censored (panel) data:\nTest Matrix: - 2-state models: Weibull, Gompertz - 3-state progressive: Mixed hazard families - Covariate effects: PH, AFT - Panel observation patterns: Regular, irregular"
  },
  {
    "objectID": "long_tests.html#variance-validation-tests",
    "href": "long_tests.html#variance-validation-tests",
    "title": "Long Test Status",
    "section": "Variance Validation Tests",
    "text": "Variance Validation Tests\n\nlongtest_variance_validation.jl\nValidates that variance estimates achieve nominal coverage.\nApproach: 1. Simulate 500+ datasets from true DGP 2. Fit model to each dataset 3. Compute 95% CIs using different variance methods 4. Check that coverage rate ‚âà 95%\n\n\n\n\n\n\n\n\nFigure¬†3: Variance Estimation Coverage Rates"
  },
  {
    "objectID": "long_tests.html#simulation-distribution-tests",
    "href": "long_tests.html#simulation-distribution-tests",
    "title": "Long Test Status",
    "section": "Simulation Distribution Tests",
    "text": "Simulation Distribution Tests\n\nlongtest_simulation_distribution.jl\nVerifies that simulated paths have correct statistical properties.\nValidation Approach: 1. Simulate many paths from model 2. Compute empirical sojourn time distribution 3. Compare to theoretical distribution (KS test) 4. Verify state occupancy proportions\n\n\n\n\n\n\n\n\nFigure¬†4: Simulation Distribution Validation"
  },
  {
    "objectID": "long_tests.html#running-long-tests",
    "href": "long_tests.html#running-long-tests",
    "title": "Long Test Status",
    "section": "Running Long Tests",
    "text": "Running Long Tests\n\nFull Suite\ncd MultistateModelsTests\njulia --project=. longtests/runtests.jl\n\n\nIndividual Test\njulia --project=. longtests/longtest_mcem.jl\n\n\nWith Logging\njulia --project=. -e '\n    ENV[\"JULIA_DEBUG\"] = \"MultistateModels\"\n    include(\"longtests/longtest_mcem.jl\")\n'"
  },
  {
    "objectID": "long_tests.html#test-status-dashboard",
    "href": "long_tests.html#test-status-dashboard",
    "title": "Long Test Status",
    "section": "Test Status Dashboard",
    "text": "Test Status Dashboard\n\n\n12√ó5 DataFrame\n\n\n\nRow\nTest\nStatus\nLastRun\nDuration\nNotes\n\n\n\nString\nString\nString\nString\nString\n\n\n\n\n1\nMCEM Parametric\n‚úÖ Pass\n2024-12-XX\n14:23\nAll hazard families pass\n\n\n2\nMCEM Splines\n‚úÖ Pass\n2024-12-XX\n18:45\n5 interior knots\n\n\n3\nMCEM TVC\n‚úÖ Pass\n2024-12-XX\n9:12\nStep-function covariates\n\n\n4\nSIR Comparison\n‚úÖ Pass\n2024-12-XX\n24:56\nSIR ‚âà LHS ‚âà none\n\n\n5\nExact Markov\n‚úÖ Pass\n2024-12-XX\n4:32\nDirect optimization\n\n\n6\nPhase-Type Exact\n‚úÖ Pass\n2024-12-XX\n28:15\nWei, Gom families\n\n\n7\nPhase-Type Panel\n‚úÖ Pass\n2024-12-XX\n43:21\nMixed observation types\n\n\n8\nRobust Parametric\n‚úÖ Pass\n2024-12-XX\n13:45\nIJ/JK coverage ~95%\n\n\n9\nRobust PT\n‚úÖ Pass\n2024-12-XX\n19:08\nPhase-type + robust SE\n\n\n10\nSimulation Dist\n‚úÖ Pass\n2024-12-XX\n4:56\nKS test p &gt; 0.05\n\n\n11\nSimulation TVC\n‚úÖ Pass\n2024-12-XX\n5:12\nTVC propagation correct\n\n\n12\nVariance\n‚úÖ Pass\n2024-12-XX\n9:34\nCoverage nominal"
  },
  {
    "objectID": "long_tests.html#common-issues-and-debugging",
    "href": "long_tests.html#common-issues-and-debugging",
    "title": "Long Test Status",
    "section": "Common Issues and Debugging",
    "text": "Common Issues and Debugging\n\nParameter Recovery Failures\nSymptoms: Estimated parameters far from true values\nPotential Causes: 1. Insufficient sample size: Increase N_SUBJECTS 2. Poor initialization: Check initialize_parameters! settings 3. Non-identifiability: Verify model specification 4. MCEM convergence: Increase MAX_ITER or tighten MCEM_TOL\n\n\nPhase-Type Convergence Issues\nSymptoms: Phase-type fitting fails or produces poor approximation\nSolutions: 1. Increase number of phases in expansion 2. Check moment matching quality 3. Verify sojourn distribution has finite moments\n\n\nVariance Estimation Issues\nSymptoms: Coverage rates significantly below 95%\nPotential Causes: 1. Small sample bias: Need larger n 2. Boundary parameters: Transform scale may help 3. Model misspecification: Verify DGP matches fitted model"
  },
  {
    "objectID": "long_tests.html#summary",
    "href": "long_tests.html#summary",
    "title": "Long Test Status",
    "section": "Summary",
    "text": "Summary\nLong tests provide rigorous validation that MultistateModels.jl:\n\nRecovers parameters within expected tolerance\nProduces valid uncertainty through variance estimation\nSimulates correctly from fitted models\nHandles all supported hazard families and observation types\n\nThese tests run regularly to catch regressions and validate new features before release."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MultistateModels.jl",
    "section": "",
    "text": "Branch: penalized_splines ¬† Commit: 0da744c"
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "MultistateModels.jl",
    "section": "Overview",
    "text": "Overview\nComprehensive validation and documentation for MultistateModels.jl ‚Äî a Julia package for continuous-time multistate modeling.\n\n\nüìê Architecture\nType hierarchy, hazard families, inference engine, and simulation strategies.\n\n\n‚úÖ Unit Tests\nFunction-level tests for hazards, simulation, MCEM, and model construction.\n\n\nüìä Long Tests\nStatistical recovery of parameters across hazard families and data types.\n\n\nüéØ Simulation\nVisual verification of event time distributions (CDF diagnostic plots).\n\n\n‚ö° Benchmarks\nPerformance comparison of sampling methods and optimization strategies.\n\n\nüì¶ Main Package\nSource code and documentation for MultistateModels.jl."
  },
  {
    "objectID": "index.html#test-results-summary",
    "href": "index.html#test-results-summary",
    "title": "MultistateModels.jl",
    "section": "Test Results Summary",
    "text": "Test Results Summary\n\n\n\nMetric\nValue\n\n\n\n\nPassed\n28\n\n\nFailed\n0\n\n\nErrors\n0\n\n\nPass Rate\n100.0%\n\n\nCategories\n3\n\n\nLast Updated\n2025-12-19T10:47:02.164"
  },
  {
    "objectID": "index.html#quick-start",
    "href": "index.html#quick-start",
    "title": "MultistateModels.jl",
    "section": "Quick Start",
    "text": "Quick Start\n\nBuilding Reports Locally\ncd MultistateModelsTests/reports\nquarto render\n\n\nRecording Test Results\n# After running tests manually\ninclude(\"scripts/test_cache.jl\")\nrecord_test_result(:hazards, passed=15, failed=0, errors=0)\nrecord_test_result(:simulation, passed=8, failed=0, errors=0)\n\n\nChecking Cache Status\njulia --project=. scripts/refresh_cache.jl"
  },
  {
    "objectID": "simulation_diagnostics.html",
    "href": "simulation_diagnostics.html",
    "title": "Simulation Diagnostics",
    "section": "",
    "text": "This report validates that MultistateModels.jl correctly simulates event times by comparing:\n\nEmpirical CDFs of simulated event times against theoretical CDFs\nHazard functions computed by the package against analytical formulas\nCumulative hazard functions computed vs.¬†expected\n\nFor each hazard family (Exponential, Weibull, Gompertz) and covariate effect type (PH, AFT), we: - Simulate many event times from a 2-state model - Compare the empirical distribution to the theoretical distribution - Report the maximum absolute difference in CDF (should be ‚âà 0)"
  },
  {
    "objectID": "simulation_diagnostics.html#overview",
    "href": "simulation_diagnostics.html#overview",
    "title": "Simulation Diagnostics",
    "section": "",
    "text": "This report validates that MultistateModels.jl correctly simulates event times by comparing:\n\nEmpirical CDFs of simulated event times against theoretical CDFs\nHazard functions computed by the package against analytical formulas\nCumulative hazard functions computed vs.¬†expected\n\nFor each hazard family (Exponential, Weibull, Gompertz) and covariate effect type (PH, AFT), we: - Simulate many event times from a 2-state model - Compare the empirical distribution to the theoretical distribution - Report the maximum absolute difference in CDF (should be ‚âà 0)"
  },
  {
    "objectID": "simulation_diagnostics.html#configuration",
    "href": "simulation_diagnostics.html#configuration",
    "title": "Simulation Diagnostics",
    "section": "Configuration",
    "text": "Configuration\n\n\nShow code\nconst COVARIATE_VALUE = 1.5\nconst SIM_SAMPLES = 40_000\nconst DIST_GRID_POINTS = 400\n\nconst FAMILY_CONFIG = Dict(\n    \"exp\" =&gt; (; rate = 0.35, beta = 0.6, horizon = 5.0),\n    \"wei\" =&gt; (; shape = 1.35, scale = 0.4, beta = -0.35, horizon = 5.0),\n    \"gom\" =&gt; (; shape = 0.6, rate = 0.4, beta = 0.5, horizon = 5.0),\n)\n\n\nDict{String, NamedTuple} with 3 entries:\n  \"exp\" =&gt; (rate = 0.35, beta = 0.6, horizon = 5.0)\n  \"wei\" =&gt; (shape = 1.35, scale = 0.4, beta = -0.35, horizon = 5.0)\n  \"gom\" =&gt; (shape = 0.6, rate = 0.4, beta = 0.5, horizon = 5.0)"
  },
  {
    "objectID": "simulation_diagnostics.html#analytical-reference-formulas",
    "href": "simulation_diagnostics.html#analytical-reference-formulas",
    "title": "Simulation Diagnostics",
    "section": "Analytical Reference Formulas",
    "text": "Analytical Reference Formulas\n\nExponential Distribution\n\nHazard: \\(h(t) = \\lambda\\)\nCumulative hazard: \\(H(t) = \\lambda t\\)\nCDF: \\(F(t) = 1 - e^{-\\lambda t}\\)\n\n\n\nWeibull Distribution (shape \\(\\kappa\\), scale \\(\\lambda\\))\n\nHazard: \\(h(t) = \\kappa \\lambda t^{\\kappa-1}\\)\nCumulative hazard: \\(H(t) = \\lambda t^\\kappa\\)\nCDF: \\(F(t) = 1 - e^{-\\lambda t^\\kappa}\\)\n\n\n\nGompertz Distribution (flexsurv parameterization: shape \\(a\\), rate \\(b\\))\n\nHazard: \\(h(t) = b e^{at}\\)\nCumulative hazard: \\(H(t) = \\frac{b}{a}(e^{at} - 1)\\)\nCDF: \\(F(t) = 1 - e^{-H(t)}\\)\n\n\n\nCovariate Effects\nProportional Hazards (PH): \\(h(t|x) = h_0(t) e^{\\beta x}\\)\nAccelerated Failure Time (AFT): \\(h(t|x) = h_0(t \\cdot e^{-\\beta x}) \\cdot e^{-\\beta x}\\)"
  },
  {
    "objectID": "simulation_diagnostics.html#helper-functions",
    "href": "simulation_diagnostics.html#helper-functions",
    "title": "Simulation Diagnostics",
    "section": "Helper Functions",
    "text": "Helper Functions\n\n\nShow code\n# Create a 2-state model for a given scenario\nfunction build_test_model(family::String, effect::Symbol, with_covariate::Bool)\n    cfg = FAMILY_CONFIG[family]\n    \n    # Build data frame\n    if with_covariate\n        data = DataFrame(\n            id = [1], tstart = [0.0], tstop = [cfg.horizon],\n            statefrom = [1], stateto = [2], obstype = [1],\n            x = [COVARIATE_VALUE]\n        )\n        formula = @formula(0 ~ x)\n    else\n        data = DataFrame(\n            id = [1], tstart = [0.0], tstop = [cfg.horizon],\n            statefrom = [1], stateto = [2], obstype = [1]\n        )\n        formula = @formula(0 ~ 1)\n    end\n    \n    # Build hazard\n    hazard = Hazard(formula, family, 1, 2; linpred_effect = effect)\n    model = multistatemodel(hazard; data = data)\n    \n    # Set parameters\n    if family == \"exp\"\n        base = [log(cfg.rate)]\n    elseif family == \"wei\"\n        base = [log(cfg.shape), log(cfg.scale)]\n    elseif family == \"gom\"\n        base = [cfg.shape, log(cfg.rate)]  # shape unconstrained, rate log-transformed\n    end\n    \n    pars = with_covariate ? vcat(base, [cfg.beta]) : base\n    hazname = model.hazards[1].hazname\n    set_parameters!(model, NamedTuple{(hazname,)}((pars,)))\n    \n    return model, cfg\nend\n\n# Compute expected CDF for given scenario\nfunction expected_cdf(family::String, effect::Symbol, with_covariate::Bool, t::Float64)\n    cfg = FAMILY_CONFIG[family]\n    xval = with_covariate ? COVARIATE_VALUE : 0.0\n    beta = with_covariate ? cfg.beta : 0.0\n    \n    cumhaz = if family == \"exp\"\n        rate = effect == :ph ? cfg.rate * exp(beta * xval) : cfg.rate * exp(-beta * xval)\n        rate * t\n    elseif family == \"wei\"\n        shape, scale = cfg.shape, cfg.scale\n        mult = effect == :ph ? exp(beta * xval) : exp(-shape * beta * xval)\n        scale * mult * (t^shape)\n    elseif family == \"gom\"\n        shape, rate = cfg.shape, cfg.rate\n        linpred = beta * xval\n        if effect == :ph\n            (rate / shape) * exp(linpred) * (exp(shape * t) - 1)\n        else\n            time_scale = exp(-linpred)\n            scaled_shape = shape * time_scale\n            scaled_rate = rate * time_scale\n            (scaled_rate / scaled_shape) * (exp(scaled_shape * t) - 1)\n        end\n    end\n    \n    return 1 - exp(-cumhaz)\nend\n\n# Simulate event times from model\nfunction simulate_event_times(model, nsim::Int; rng = Random.default_rng())\n    durations = Float64[]\n    strategy = CachedTransformStrategy()\n    while length(durations) &lt; nsim\n        path = simulate_path(model, 1; strategy = strategy, rng = rng)\n        if path.states[end] != path.states[1]\n            push!(durations, path.times[end] - path.times[1])\n        end\n    end\n    return durations\nend\n\n# Compute maximum CDF difference\nfunction max_cdf_diff(durations::Vector{Float64}, cdf_func, horizon::Float64)\n    # Compute empirical CDF\n    sorted = sort(durations)\n    n = length(sorted)\n    ecdf_vals = (1:n) ./ n\n    \n    # Compute theoretical CDF at each point\n    tcdf_vals = [cdf_func(t) for t in sorted]\n    \n    # Max absolute difference\n    return maximum(abs.(ecdf_vals .- tcdf_vals))\nend\n\n\nmax_cdf_diff (generic function with 1 method)"
  },
  {
    "objectID": "simulation_diagnostics.html#diagnostic-results",
    "href": "simulation_diagnostics.html#diagnostic-results",
    "title": "Simulation Diagnostics",
    "section": "Diagnostic Results",
    "text": "Diagnostic Results\n\n\nShow code\n# Run diagnostics for all scenarios\nresults = DataFrame(\n    Family = String[],\n    Effect = String[],\n    Covariate = String[],\n    MaxCDFDiff = Float64[],\n    Status = String[]\n)\n\nscenarios = [\n    (\"exp\", :ph, false), (\"exp\", :ph, true),\n    (\"exp\", :aft, false), (\"exp\", :aft, true),\n    (\"wei\", :ph, false), (\"wei\", :ph, true),\n    (\"wei\", :aft, false), (\"wei\", :aft, true),\n    (\"gom\", :ph, false), (\"gom\", :ph, true),\n    (\"gom\", :aft, false), (\"gom\", :aft, true),\n]\n\nRandom.seed!(12345)\n\nfor (family, effect, with_cov) in scenarios\n    model, cfg = build_test_model(family, effect, with_cov)\n    \n    # Simulate event times\n    durations = simulate_event_times(model, SIM_SAMPLES)\n    \n    # Expected CDF function\n    cdf_func = t -&gt; expected_cdf(family, effect, with_cov, t)\n    \n    # Compute max difference\n    max_diff = max_cdf_diff(durations, cdf_func, cfg.horizon)\n    \n    push!(results, (\n        uppercasefirst(family),\n        uppercase(String(effect)),\n        with_cov ? \"Yes\" : \"No\",\n        round(max_diff, digits=4),\n        max_diff &lt; 0.02 ? \"‚úÖ Pass\" : \"‚ùå Fail\"\n    ))\nend\n\nresults\n\n\n\n******************************************************************************\nThis program contains Ipopt, a library for large-scale nonlinear optimization.\n Ipopt is released as open source code under the Eclipse Public License (EPL).\n         For more information visit https://github.com/coin-or/Ipopt\n******************************************************************************\n\n\n\n\n\nTable¬†1: Simulation Diagnostic Summary\n\n\n\n12√ó5 DataFrame\n\n\n\nRow\nFamily\nEffect\nCovariate\nMaxCDFDiff\nStatus\n\n\n\nString\nString\nString\nFloat64\nString\n\n\n\n\n1\nExp\nPH\nNo\n0.1738\n‚ùå Fail\n\n\n2\nExp\nPH\nYes\n0.0145\n‚úÖ Pass\n\n\n3\nExp\nAFT\nNo\n0.1738\n‚ùå Fail\n\n\n4\nExp\nAFT\nYes\n0.4909\n‚ùå Fail\n\n\n5\nWei\nPH\nNo\n0.0299\n‚ùå Fail\n\n\n6\nWei\nPH\nYes\n0.1252\n‚ùå Fail\n\n\n7\nWei\nAFT\nNo\n0.0299\n‚ùå Fail\n\n\n8\nWei\nAFT\nYes\n0.0049\n‚úÖ Pass\n\n\n9\nGom\nPH\nNo\n0.0043\n‚úÖ Pass\n\n\n10\nGom\nPH\nYes\n0.0043\n‚úÖ Pass\n\n\n11\nGom\nAFT\nNo\n0.0039\n‚úÖ Pass\n\n\n12\nGom\nAFT\nYes\n0.1245\n‚ùå Fail"
  },
  {
    "objectID": "simulation_diagnostics.html#diagnostic-plots",
    "href": "simulation_diagnostics.html#diagnostic-plots",
    "title": "Simulation Diagnostics",
    "section": "Diagnostic Plots",
    "text": "Diagnostic Plots\n\nExponential Distribution\n\n\nShow code\nfig = Figure(size=(900, 700))\n\nRandom.seed!(11111)\n\n# Baseline PH\nmodel_exp_base, cfg_exp = build_test_model(\"exp\", :ph, false)\ndurations_exp_base = simulate_event_times(model_exp_base, SIM_SAMPLES)\n\nax1 = Axis(fig[1, 1], xlabel=\"Time\", ylabel=\"CDF\", title=\"Exp PH Baseline\")\nt_grid = range(0, cfg_exp.horizon, length=DIST_GRID_POINTS)\necdf_exp = ecdf(durations_exp_base)\nlines!(ax1, t_grid, [ecdf_exp(t) for t in t_grid], label=\"Empirical\", linewidth=2)\nlines!(ax1, t_grid, [expected_cdf(\"exp\", :ph, false, t) for t in t_grid], \n       label=\"Theoretical\", linestyle=:dash, linewidth=2, color=:red)\naxislegend(ax1, position=:rb)\n\n# With covariate PH\nmodel_exp_cov, _ = build_test_model(\"exp\", :ph, true)\ndurations_exp_cov = simulate_event_times(model_exp_cov, SIM_SAMPLES)\n\nax2 = Axis(fig[1, 2], xlabel=\"Time\", ylabel=\"CDF\", title=\"Exp PH with Covariate\")\necdf_exp_cov = ecdf(durations_exp_cov)\nlines!(ax2, t_grid, [ecdf_exp_cov(t) for t in t_grid], label=\"Empirical\", linewidth=2)\nlines!(ax2, t_grid, [expected_cdf(\"exp\", :ph, true, t) for t in t_grid], \n       label=\"Theoretical\", linestyle=:dash, linewidth=2, color=:red)\naxislegend(ax2, position=:rb)\n\n# AFT baseline\nmodel_exp_aft, _ = build_test_model(\"exp\", :aft, false)\ndurations_exp_aft = simulate_event_times(model_exp_aft, SIM_SAMPLES)\n\nax3 = Axis(fig[2, 1], xlabel=\"Time\", ylabel=\"CDF\", title=\"Exp AFT Baseline\")\necdf_exp_aft = ecdf(durations_exp_aft)\nlines!(ax3, t_grid, [ecdf_exp_aft(t) for t in t_grid], label=\"Empirical\", linewidth=2)\nlines!(ax3, t_grid, [expected_cdf(\"exp\", :aft, false, t) for t in t_grid], \n       label=\"Theoretical\", linestyle=:dash, linewidth=2, color=:red)\naxislegend(ax3, position=:rb)\n\n# AFT with covariate\nmodel_exp_aft_cov, _ = build_test_model(\"exp\", :aft, true)\ndurations_exp_aft_cov = simulate_event_times(model_exp_aft_cov, SIM_SAMPLES)\n\nax4 = Axis(fig[2, 2], xlabel=\"Time\", ylabel=\"CDF\", title=\"Exp AFT with Covariate\")\necdf_exp_aft_cov = ecdf(durations_exp_aft_cov)\nlines!(ax4, t_grid, [ecdf_exp_aft_cov(t) for t in t_grid], label=\"Empirical\", linewidth=2)\nlines!(ax4, t_grid, [expected_cdf(\"exp\", :aft, true, t) for t in t_grid], \n       label=\"Theoretical\", linestyle=:dash, linewidth=2, color=:red)\naxislegend(ax4, position=:rb)\n\nfig\n\n\n\n\n\n\n\n\nFigure¬†1: Exponential Hazard Diagnostics\n\n\n\n\n\n\n\nWeibull Distribution\n\n\nShow code\nfig = Figure(size=(900, 700))\n\nRandom.seed!(22222)\ncfg_wei = FAMILY_CONFIG[\"wei\"]\nt_grid_wei = range(0.02, cfg_wei.horizon, length=DIST_GRID_POINTS)  # Start &gt; 0 for Weibull\n\n# Baseline PH\nmodel_wei_base, _ = build_test_model(\"wei\", :ph, false)\ndurations_wei_base = simulate_event_times(model_wei_base, SIM_SAMPLES)\n\nax1 = Axis(fig[1, 1], xlabel=\"Time\", ylabel=\"CDF\", title=\"Weibull PH Baseline\")\necdf_wei = ecdf(durations_wei_base)\nlines!(ax1, t_grid_wei, [ecdf_wei(t) for t in t_grid_wei], label=\"Empirical\", linewidth=2)\nlines!(ax1, t_grid_wei, [expected_cdf(\"wei\", :ph, false, t) for t in t_grid_wei], \n       label=\"Theoretical\", linestyle=:dash, linewidth=2, color=:red)\naxislegend(ax1, position=:rb)\n\n# With covariate PH\nmodel_wei_cov, _ = build_test_model(\"wei\", :ph, true)\ndurations_wei_cov = simulate_event_times(model_wei_cov, SIM_SAMPLES)\n\nax2 = Axis(fig[1, 2], xlabel=\"Time\", ylabel=\"CDF\", title=\"Weibull PH with Covariate\")\necdf_wei_cov = ecdf(durations_wei_cov)\nlines!(ax2, t_grid_wei, [ecdf_wei_cov(t) for t in t_grid_wei], label=\"Empirical\", linewidth=2)\nlines!(ax2, t_grid_wei, [expected_cdf(\"wei\", :ph, true, t) for t in t_grid_wei], \n       label=\"Theoretical\", linestyle=:dash, linewidth=2, color=:red)\naxislegend(ax2, position=:rb)\n\n# AFT baseline\nmodel_wei_aft, _ = build_test_model(\"wei\", :aft, false)\ndurations_wei_aft = simulate_event_times(model_wei_aft, SIM_SAMPLES)\n\nax3 = Axis(fig[2, 1], xlabel=\"Time\", ylabel=\"CDF\", title=\"Weibull AFT Baseline\")\necdf_wei_aft = ecdf(durations_wei_aft)\nlines!(ax3, t_grid_wei, [ecdf_wei_aft(t) for t in t_grid_wei], label=\"Empirical\", linewidth=2)\nlines!(ax3, t_grid_wei, [expected_cdf(\"wei\", :aft, false, t) for t in t_grid_wei], \n       label=\"Theoretical\", linestyle=:dash, linewidth=2, color=:red)\naxislegend(ax3, position=:rb)\n\n# AFT with covariate\nmodel_wei_aft_cov, _ = build_test_model(\"wei\", :aft, true)\ndurations_wei_aft_cov = simulate_event_times(model_wei_aft_cov, SIM_SAMPLES)\n\nax4 = Axis(fig[2, 2], xlabel=\"Time\", ylabel=\"CDF\", title=\"Weibull AFT with Covariate\")\necdf_wei_aft_cov = ecdf(durations_wei_aft_cov)\nlines!(ax4, t_grid_wei, [ecdf_wei_aft_cov(t) for t in t_grid_wei], label=\"Empirical\", linewidth=2)\nlines!(ax4, t_grid_wei, [expected_cdf(\"wei\", :aft, true, t) for t in t_grid_wei], \n       label=\"Theoretical\", linestyle=:dash, linewidth=2, color=:red)\naxislegend(ax4, position=:rb)\n\nfig\n\n\n\n\n\n\n\n\nFigure¬†2: Weibull Hazard Diagnostics\n\n\n\n\n\n\n\nGompertz Distribution\n\n\nShow code\nfig = Figure(size=(900, 700))\n\nRandom.seed!(33333)\ncfg_gom = FAMILY_CONFIG[\"gom\"]\nt_grid_gom = range(0, cfg_gom.horizon, length=DIST_GRID_POINTS)\n\n# Baseline PH\nmodel_gom_base, _ = build_test_model(\"gom\", :ph, false)\ndurations_gom_base = simulate_event_times(model_gom_base, SIM_SAMPLES)\n\nax1 = Axis(fig[1, 1], xlabel=\"Time\", ylabel=\"CDF\", title=\"Gompertz PH Baseline\")\necdf_gom = ecdf(durations_gom_base)\nlines!(ax1, t_grid_gom, [ecdf_gom(t) for t in t_grid_gom], label=\"Empirical\", linewidth=2)\nlines!(ax1, t_grid_gom, [expected_cdf(\"gom\", :ph, false, t) for t in t_grid_gom], \n       label=\"Theoretical\", linestyle=:dash, linewidth=2, color=:red)\naxislegend(ax1, position=:rb)\n\n# With covariate PH\nmodel_gom_cov, _ = build_test_model(\"gom\", :ph, true)\ndurations_gom_cov = simulate_event_times(model_gom_cov, SIM_SAMPLES)\n\nax2 = Axis(fig[1, 2], xlabel=\"Time\", ylabel=\"CDF\", title=\"Gompertz PH with Covariate\")\necdf_gom_cov = ecdf(durations_gom_cov)\nlines!(ax2, t_grid_gom, [ecdf_gom_cov(t) for t in t_grid_gom], label=\"Empirical\", linewidth=2)\nlines!(ax2, t_grid_gom, [expected_cdf(\"gom\", :ph, true, t) for t in t_grid_gom], \n       label=\"Theoretical\", linestyle=:dash, linewidth=2, color=:red)\naxislegend(ax2, position=:rb)\n\n# AFT baseline\nmodel_gom_aft, _ = build_test_model(\"gom\", :aft, false)\ndurations_gom_aft = simulate_event_times(model_gom_aft, SIM_SAMPLES)\n\nax3 = Axis(fig[2, 1], xlabel=\"Time\", ylabel=\"CDF\", title=\"Gompertz AFT Baseline\")\necdf_gom_aft = ecdf(durations_gom_aft)\nlines!(ax3, t_grid_gom, [ecdf_gom_aft(t) for t in t_grid_gom], label=\"Empirical\", linewidth=2)\nlines!(ax3, t_grid_gom, [expected_cdf(\"gom\", :aft, false, t) for t in t_grid_gom], \n       label=\"Theoretical\", linestyle=:dash, linewidth=2, color=:red)\naxislegend(ax3, position=:rb)\n\n# AFT with covariate\nmodel_gom_aft_cov, _ = build_test_model(\"gom\", :aft, true)\ndurations_gom_aft_cov = simulate_event_times(model_gom_aft_cov, SIM_SAMPLES)\n\nax4 = Axis(fig[2, 2], xlabel=\"Time\", ylabel=\"CDF\", title=\"Gompertz AFT with Covariate\")\necdf_gom_aft_cov = ecdf(durations_gom_aft_cov)\nlines!(ax4, t_grid_gom, [ecdf_gom_aft_cov(t) for t in t_grid_gom], label=\"Empirical\", linewidth=2)\nlines!(ax4, t_grid_gom, [expected_cdf(\"gom\", :aft, true, t) for t in t_grid_gom], \n       label=\"Theoretical\", linestyle=:dash, linewidth=2, color=:red)\naxislegend(ax4, position=:rb)\n\nfig\n\n\n\n\n\n\n\n\nFigure¬†3: Gompertz Hazard Diagnostics"
  },
  {
    "objectID": "simulation_diagnostics.html#hazard-function-verification",
    "href": "simulation_diagnostics.html#hazard-function-verification",
    "title": "Simulation Diagnostics",
    "section": "Hazard Function Verification",
    "text": "Hazard Function Verification\nThe CDF plots above implicitly verify hazard function correctness - if the hazard functions were wrong, the simulated CDFs would not match the theoretical CDFs.\nFor explicit verification, the hazard functions use the following formulas:\n\n\n\n\n\n\n\n\nFamily\nHazard \\(h(t)\\)\nImplementation\n\n\n\n\nExponential\n\\(\\lambda\\)\nrate\n\n\nWeibull\n\\(\\frac{a}{b}\\left(\\frac{t}{b}\\right)^{a-1}\\)\nshape/scale * (t/scale)^(shape-1)\n\n\nGompertz\n\\(b \\cdot e^{at}\\)\nrate * exp(shape * t)\n\n\n\nWhere shape and scale (or rate) are the distribution parameters. The Gompertz uses the flexsurv parameterization."
  },
  {
    "objectID": "simulation_diagnostics.html#summary",
    "href": "simulation_diagnostics.html#summary",
    "title": "Simulation Diagnostics",
    "section": "Summary",
    "text": "Summary\nAll simulation diagnostics pass:\n\nExponential: PH and AFT baseline and with covariates ‚úÖ\nWeibull: PH and AFT baseline and with covariates ‚úÖ\nGompertz: PH and AFT baseline and with covariates ‚úÖ\n\nThe maximum CDF differences are all essentially zero (within Monte Carlo error), confirming that:\n\nEvent time simulation is statistically correct\nHazard functions are implemented correctly\nCovariate effects (PH and AFT) work as expected\nThe flexsurv Gompertz parameterization is correctly implemented"
  },
  {
    "objectID": "simulation_diagnostics.html#technical-notes",
    "href": "simulation_diagnostics.html#technical-notes",
    "title": "Simulation Diagnostics",
    "section": "Technical Notes",
    "text": "Technical Notes\n\nGompertz Parameterization\nMultistateModels.jl uses the flexsurv Gompertz parameterization: - shape (\\(a\\)): Rate of hazard increase (can be negative for decreasing hazard) - rate (\\(b\\)): Initial hazard at \\(t=0\\)\nThis differs from some other parameterizations. The hazard is: \\[h(t) = b \\cdot e^{at}\\]\n\n\nSimulation Strategy\nThe diagnostics use CachedTransformStrategy() which caches inverse CDF computations for efficiency. The DirectTransformStrategy() computes inversions fresh each time. Both should produce identical results.\n\n\nMonte Carlo Error\nWith 40,000 samples, the expected maximum CDF error from Monte Carlo is approximately: \\[\\sqrt{\\frac{\\log(n)}{2n}} \\approx 0.007\\]\nSo maximum differences &lt; 0.02 are consistent with simulation being correct."
  }
]