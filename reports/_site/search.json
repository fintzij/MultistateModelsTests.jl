[
  {
    "objectID": "02_unit_tests.html",
    "href": "02_unit_tests.html",
    "title": "Unit Test Coverage",
    "section": "",
    "text": "Metric\nValue\n\n\n\n\nTotal Test Files\n26\n\n\nTotal Test Cases\n~2831\n\n\nPassing\n~2831\n\n\nFailing\n0\n\n\nBranch\npenalized_splines\n\n\nRuntime\n~8.5 minutes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArea\nTest Files\nDescription\n\n\n\n\nHazard Evaluation\ntest_hazards.jl, test_compute_hazard.jl\nHazard functions, cumulative hazards, survival\n\n\nSpline Infrastructure\ntest_splines.jl, test_penalty_infrastructure.jl\nB-spline basis, penalization\n\n\nSimulation\ntest_simulation.jl\nPath simulation, distribution fidelity\n\n\nMCEM Algorithm\ntest_mcem.jl\nMonte Carlo EM helper functions\n\n\nPhase-Type\ntest_phasetype.jl\nState expansion, FFBS, emission matrices, Coxian math\n\n\nSurrogates\ntest_surrogates.jl\nMarkov and phase-type surrogate fitting\n\n\nSIR Resampling\ntest_sir.jl\nMultinomial and LHS resampling\n\n\nVariance Estimation\ntest_variance.jl\nModel, IJ, jackknife variance\n\n\nWeights\ntest_subject_weights.jl\nSubject and observation weights\n\n\nPIJCV\ntest_pijcv.jl\nSmoothing parameter selection\n\n\nLikelihood\ntest_loglik_analytical.jl\nExact, panel, penalized likelihoods\n\n\nModel Construction\ntest_hazard_macro.jl, test_initialization.jl\nModel building, parameter init\n\n\nInfrastructure\ntest_bounds.jl, test_constraints.jl, test_ad_backends.jl\nParameter bounds, optimization, AD compatibility"
  },
  {
    "objectID": "02_unit_tests.html#coverage-summary",
    "href": "02_unit_tests.html#coverage-summary",
    "title": "Unit Test Coverage",
    "section": "",
    "text": "Metric\nValue\n\n\n\n\nTotal Test Files\n26\n\n\nTotal Test Cases\n~2831\n\n\nPassing\n~2831\n\n\nFailing\n0\n\n\nBranch\npenalized_splines\n\n\nRuntime\n~8.5 minutes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArea\nTest Files\nDescription\n\n\n\n\nHazard Evaluation\ntest_hazards.jl, test_compute_hazard.jl\nHazard functions, cumulative hazards, survival\n\n\nSpline Infrastructure\ntest_splines.jl, test_penalty_infrastructure.jl\nB-spline basis, penalization\n\n\nSimulation\ntest_simulation.jl\nPath simulation, distribution fidelity\n\n\nMCEM Algorithm\ntest_mcem.jl\nMonte Carlo EM helper functions\n\n\nPhase-Type\ntest_phasetype.jl\nState expansion, FFBS, emission matrices, Coxian math\n\n\nSurrogates\ntest_surrogates.jl\nMarkov and phase-type surrogate fitting\n\n\nSIR Resampling\ntest_sir.jl\nMultinomial and LHS resampling\n\n\nVariance Estimation\ntest_variance.jl\nModel, IJ, jackknife variance\n\n\nWeights\ntest_subject_weights.jl\nSubject and observation weights\n\n\nPIJCV\ntest_pijcv.jl\nSmoothing parameter selection\n\n\nLikelihood\ntest_loglik_analytical.jl\nExact, panel, penalized likelihoods\n\n\nModel Construction\ntest_hazard_macro.jl, test_initialization.jl\nModel building, parameter init\n\n\nInfrastructure\ntest_bounds.jl, test_constraints.jl, test_ad_backends.jl\nParameter bounds, optimization, AD compatibility"
  },
  {
    "objectID": "02_unit_tests.html#hazard-functions-test_hazards.jl",
    "href": "02_unit_tests.html#hazard-functions-test_hazards.jl",
    "title": "Unit Test Coverage",
    "section": "2. Hazard Functions (test_hazards.jl)",
    "text": "2. Hazard Functions (test_hazards.jl)\nTests verify that all parametric hazard families return mathematically correct values against analytical formulas.\n\n2.1 Mathematical Reference\n\n\n\n\n\n\nNoteAnalytical Hazard Formulas (Click to expand)\n\n\n\n\n\nExponential:\n\n\\(h(t) = \\lambda\\) (constant hazard)\n\\(H(t) = \\lambda t\\) (cumulative hazard)\n\\(S(t) = \\exp(-\\lambda t)\\) (survival function)\n\nWeibull (shape \\(\\kappa\\), scale \\(\\lambda\\)):\n\n\\(h(t) = \\lambda \\kappa t^{\\kappa-1}\\) (increasing/decreasing with \\(t\\))\n\\(H(t) = \\lambda t^\\kappa\\)\n\\(S(t) = \\exp(-\\lambda t^\\kappa)\\)\n\nGompertz (shape \\(a\\), rate \\(b\\)) - flexsurv parameterization:\n\n\\(h(t) = b \\exp(at)\\) (\\(a&gt;0\\): increasing, \\(a&lt;0\\): decreasing)\n\\(H(t) = \\frac{b}{a}(\\exp(at) - 1)\\) for \\(a \\neq 0\\)\n\\(S(t) = \\exp\\left(-\\frac{b}{a}(\\exp(at) - 1)\\right)\\)\n\nCovariate Effects:\n\nProportional Hazards (PH): \\(h(t|x) = h_0(t) \\exp(\\beta'x)\\)\nAccelerated Failure Time (AFT): \\(h(t|x) = h_0(t \\cdot e^{-\\beta'x}) e^{-\\beta'x}\\)\n\n\n\n\n\n\n2.2 Test Criteria\n\n\n\n\n\n\nNoteDetailed Test Criteria (Click to expand)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTest\nCriterion\nFormula Verified\nTolerance\n\n\n\n\nSurvival probability\n\\(1 - S(0,t) = F(t)\\) matches Distributions.Exponential\nTotal hazard rate = 0.2, \\(F(2) = 1 - e^{-0.4}\\)\nrtol=1e-6\n\n\nExponential baseline\n\\(h(t) = \\lambda\\) constant in \\(t\\)\neval_hazard = rate parameter\nrtol=1e-6\n\n\nExponential with covariates\n\\(\\log h = \\log\\lambda + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_{12} x_1 x_2\\)\nFull linear predictor formula\nExact match\n\n\nWeibull baseline\n\\(\\log h = \\log\\lambda + \\log\\kappa + (\\kappa-1)\\log t\\)\nLog-hazard at \\(t=1.7\\)\nrtol=1e-6\n\n\nWeibull with covariates\nPH and AFT covariate effects\nShape and scale adjustments\nrtol=1e-6\n\n\nGompertz baseline\n\\(h(t) = b\\exp(at)\\)\nRate exponentially increasing in \\(t\\)\nrtol=1e-6\n\n\nGompertz cumhaz\n\\(H(t) = (b/a)(\\exp(at)-1)\\)\nAnalytical integration\nrtol=1e-6\n\n\nTime transform parity\neval_hazard(apply_transform=true) = eval_hazard(apply_transform=false)\nTang optimization produces identical values\nExact match\n\n\nCumulative hazard additivity\n\\(H(a,c) = H(a,b) + H(b,c)\\)\nInterval splitting\nrtol=1e-8\n\n\n\nTolerance Justification: rtol=1e-6 accounts for ParameterHandling.positive() using \\(\\varepsilon \\approx 1.49 \\times 10^{-8}\\) safety margin in exp/log round-trips.\n\n\n\n\n\n2.3 Summary Table\n\n\n\nFamily\nBaseline\nCovariates\nAFT\nTime Transform\nStatus\n\n\n\n\nExponential\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\nPass\n\n\nWeibull\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\nPass\n\n\nGompertz\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\nPass\n\n\nSpline\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\nPass"
  },
  {
    "objectID": "02_unit_tests.html#spline-hazards-test_splines.jl",
    "href": "02_unit_tests.html#spline-hazards-test_splines.jl",
    "title": "Unit Test Coverage",
    "section": "3. Spline Hazards (test_splines.jl)",
    "text": "3. Spline Hazards (test_splines.jl)\nTests verify B-spline basis construction, numerical integration accuracy, and penalization infrastructure.\n\n3.1 Mathematical Reference\n\n\n\n\n\n\nNoteSpline Mathematical Background (Click to expand)\n\n\n\n\n\nB-Spline Hazard:\n\n\\(h(t) = \\sum_i \\beta_i B_i(t)\\) where \\(B_i\\) are B-spline basis functions\nCoefficients \\(\\beta_i &gt; 0\\) ensure non-negative hazard\n\nCumulative Hazard:\n\n\\(H(a,b) = \\int_a^b h(t) dt = \\sum_i \\beta_i \\int_a^b B_i(t) dt\\)\nUses analytical B-spline antiderivative (not numerical quadrature)\n\nSecond-Order Penalty Matrix:\n\n\\(\\mathbf{P} = \\int \\mathbf{B}''(t) \\mathbf{B}''(t)' dt\\)\nPenalizes deviation from linearity (Wood, 2017)\nMust be symmetric, positive semi-definite with \\(\\text{rank} = k - m\\) where \\(m\\) = penalty order\n\nTensor Product (te()):\n\n\\(f(x,y) = \\sum_i \\sum_j \\beta_{ij} B_i^{(x)}(x) B_j^{(y)}(y)\\)\nBasis: Kronecker product \\(\\mathbf{B}^{(x)} \\otimes \\mathbf{B}^{(y)}\\)\nPenalty: Row + column penalties on coefficient matrix\n\n\n\n\n\n\n3.2 Test Criteria\n\n\n\n\n\n\nNoteDetailed Spline Test Criteria (Click to expand)\n\n\n\n\n\nNumerical Integration Verification:\n\n\n\n\n\n\n\n\nTest\nCriterion\nMethod\n\n\n\n\nCumhaz vs QuadGK\n\\(H(a,b)_\\text{impl} \\approx \\int_a^b h(t)dt\\)\nQuadGK adaptive quadrature, 5 random parameter configs, 5 intervals each\n\n\nB-spline antiderivative\nBSplineKit integral() matches QuadGK\nrtol=1e-10 (machine precision for polynomials)\n\n\nCovariate effect\n\\(H(a,b|x) = H_0(a,b) \\exp(\\beta'x)\\) for PH\nMultiple covariate values\n\n\n\nPenalty Matrix Properties:\n\n\n\n\n\n\n\nProperty\nCriterion\n\n\n\n\nSymmetry\n\\(\\mathbf{P} = \\mathbf{P}'\\) exactly\n\n\nPositive semi-definiteness\nAll eigenvalues \\(\\geq 0\\)\n\n\nNull space dimension\n\\(\\text{dim}(\\text{null}(\\mathbf{P})) = m\\) (penalty order)\n\n\nNull space basis\nContains polynomials of degree \\(&lt; m\\)\n\n\n\nSmooth Term Parsing:\n\n\n\nTest\nInput\nExpected Output\n\n\n\n\nBasic s(x)\n@formula(0 ~ s(x))\nSmoothTerm with default k=10\n\n\nCustom k\n@formula(0 ~ s(x, k=15))\nSmoothTerm with k=15\n\n\nCustom m\n@formula(0 ~ s(x, m=1))\nFirst-order penalty (ridge)\n\n\nTensor product\n@formula(0 ~ te(x,y))\nTensorProductTerm\n\n\n\nAutomatic Knot Placement:\n\n\n\nTest\nCriterion\n\n\n\n\nInterior knots\nPlaced at quantiles of transition times\n\n\nBoundary knots\nSpan observed time range\n\n\nEdge case handling\nk &gt; n_unique_times handled gracefully\n\n\n\n\n\n\n\n\n3.3 Summary Table\n\n\n\n\n\n\n\n\n\nComponent\nTests\nTolerance\nStatus\n\n\n\n\nCumhaz vs numerical integration\n25 random configs\nrtol=1e-6\n‚úÖ Pass\n\n\nB-spline antiderivative\n6 test cases\nrtol=1e-10\n‚úÖ Pass\n\n\nPenalty matrix symmetry\nExact equality\nExact\n‚úÖ Pass\n\n\nPenalty matrix eigenvalues\nAll ‚â• 0\natol=1e-10\n‚úÖ Pass\n\n\nPenalty null space\nDimension = m\nExact count\n‚úÖ Pass\n\n\ns() term parsing\n10+ variants\nN/A\n‚úÖ Pass\n\n\nte() tensor product\nKronecker construction\nExact\n‚úÖ Pass\n\n\nAutomatic knot placement\nQuantile-based\nN/A\n‚úÖ Pass\n\n\nMonotone rectification\nCoefficients non-decreasing\nN/A\n‚úÖ Pass"
  },
  {
    "objectID": "02_unit_tests.html#simulation-test_simulation.jl",
    "href": "02_unit_tests.html#simulation-test_simulation.jl",
    "title": "Unit Test Coverage",
    "section": "4. Simulation (test_simulation.jl)",
    "text": "4. Simulation (test_simulation.jl)\nTests verify simulation produces statistically correct output with correct schema.\n\n4.1 Mathematical Reference\n\n\n\n\n\n\nNoteSimulation Statistical Properties (Click to expand)\n\n\n\n\n\nExponential Distribution (\\(\\text{Exp}(\\lambda)\\)):\n\nMean: \\(E[T] = 1/\\lambda\\)\nVariance: \\(\\text{Var}(T) = 1/\\lambda^2\\)\nStandard error of sample mean: \\(\\text{SE}(\\bar{T}) = 1/(\\lambda\\sqrt{n})\\)\n\nStatistical Tests Applied:\n\nSample mean within 10% of true mean (relative tolerance)\nSample variance within 15% of true variance\nSample mean within 3 standard errors of true mean (statistical validity)\n\n\n\n\n\n\n4.2 Test Criteria\n\n\n\n\n\n\nNoteDetailed Simulation Test Criteria (Click to expand)\n\n\n\n\n\nDistribution Correctness (n=500 simulations):\n\n\n\n\n\n\n\n\nTest\nCriterion\nParameters\n\n\n\n\nMean\n\\(\\|\\bar{T} - 1/\\lambda\\| / (1/\\lambda) &lt; 0.10\\)\n\\(\\lambda=0.5\\), true mean = 2.0\n\n\nVariance\n\\(\\|s^2 - 1/\\lambda^2\\| / (1/\\lambda^2) &lt; 0.15\\)\ntrue var = 4.0\n\n\nStandard error\n\\(\\|\\bar{T} - 2.0\\| &lt; 3 \\times \\text{SE}\\)\n\\(\\text{SE} \\approx 0.0894\\)\n\n\n\nDataFrame Schema:\n\n\n\nColumn\nRequired Type\nConstraint\n\n\n\n\nid\nInt\nSubject identifier\n\n\ntstart\nFloat64\nInterval start\n\n\ntstop\nFloat64\nInterval end, tstop &gt; tstart\n\n\nstatefrom\nInt\nOrigin state\n\n\nstateto\nInt\nDestination state\n\n\nobstype\nInt\nObservation type\n\n\n\nPath Validity:\n\n\n\nTest\nCriterion\n\n\n\n\nMonotonic times\ntimes[i+1] &gt; times[i] for all \\(i\\)\n\n\nLegal transitions\ntmat[from, to] &gt; 0 when state changes\n\n\nInitial state\nMatches statefrom in data\n\n\nAbsorbing states\nNo transitions out of absorbing states\n\n\n\nTransform Strategy Equivalence:\n\n\n\n\n\n\n\nTest\nCriterion\n\n\n\n\nCachedTransformStrategy vs DirectTransformStrategy\nIdentical distribution (same RNG seed)\n\n\n\n\n\n\n\n\n4.3 Summary Table\n\n\n\nTest Category\nTests\nStatus\n\n\n\n\nMean correctness\n1\n‚úÖ Pass\n\n\nVariance correctness\n1\n‚úÖ Pass\n\n\nSchema validation\n3 simulations √ó 6 columns\n‚úÖ Pass\n\n\nMonotonic times\n50 paths\n‚úÖ Pass\n\n\nLegal transitions\n50 paths\n‚úÖ Pass\n\n\nInitial state\n20 paths\n‚úÖ Pass\n\n\nTransform strategy parity\nSame RNG\n‚úÖ Pass"
  },
  {
    "objectID": "02_unit_tests.html#mcem-algorithm-test_mcem.jl",
    "href": "02_unit_tests.html#mcem-algorithm-test_mcem.jl",
    "title": "Unit Test Coverage",
    "section": "5. MCEM Algorithm (test_mcem.jl)",
    "text": "5. MCEM Algorithm (test_mcem.jl)\nTests verify Monte Carlo EM helper functions.\n\n5.1 Mathematical Reference\n\n\n\n\n\n\nNoteMCEM Mathematical Background (Click to expand)\n\n\n\n\n\nMCEM Marginal Log-Likelihood:\n\\[Q(\\theta) = \\sum_{i=1}^n w_i \\sum_{j=1}^{m_i} p_{ij} \\ell(\\theta; y_i, z_{ij})\\]\nwhere \\(w_i\\) = subject weight, \\(p_{ij}\\) = importance weight for path \\(j\\) of subject \\(i\\).\nMCEM Asymptotic Standard Error:\n\\[\\text{ASE} = \\sqrt{\\sum_{i=1}^n w_i^2 \\cdot \\text{Var}_{p_i}[\\ell(\\theta; y_i, z_i) - \\ell(\\theta_0; y_i, z_i)]}\\]\n\n\n\n\n\n5.2 Test Criteria\n\n\n\n\n\n\nNoteDetailed MCEM Test Criteria (Click to expand)\n\n\n\n\n\nForwardDiff Gradient Compatibility:\n\n\n\n\n\n\n\nTest\nCriterion\n\n\n\n\nExactData gradient\nForwardDiff.gradient() produces finite values\n\n\nSMPanelData gradient\nForwardDiff.gradient() produces finite values\n\n\nGradient length\nMatches parameter count\n\n\n\nMCEM Helper Functions:\n\n\n\nFunction\nTest\nExpected Value\n\n\n\n\nmcem_mll\nUnit weights\nSum of weighted log-liks\n\n\nmcem_mll\nNon-unit weights\nSubject-weighted sum\n\n\nmcem_ase\nIdentical logliks\n0.0 (no variance)\n\n\nvar_ris\nAll zeros\n0.0\n\n\n\nMCEM Monotonicity (Limited Iterations):\n\n\n\nTest\nCriterion\n\n\n\n\nQ-function increase\nFinal MLL not substantially worse than initial\n\n\nMC noise allowance\nAllow 10% relative decrease\n\n\n\n\n\n\n\n\n5.3 Summary Table\n\n\n\nComponent\nTests\nStatus\n\n\n\n\nAD gradient compatibility\n2 data types\n‚úÖ Pass\n\n\nmcem_mll correctness\n2 weight configs\n‚úÖ Pass\n\n\nmcem_ase correctness\n1 edge case\n‚úÖ Pass\n\n\nMCEM monotonicity\n3 iterations\n‚úÖ Pass"
  },
  {
    "objectID": "02_unit_tests.html#sir-resampling-test_sir.jl",
    "href": "02_unit_tests.html#sir-resampling-test_sir.jl",
    "title": "Unit Test Coverage",
    "section": "6. SIR Resampling (test_sir.jl)",
    "text": "6. SIR Resampling (test_sir.jl)\nTests verify Sampling Importance Resampling functions for MCEM.\n\n6.1 Mathematical Reference\n\n\n\n\n\n\nNoteSIR Mathematical Background (Click to expand)\n\n\n\n\n\nPool Size Formula:\n\\[m_\\text{pool} = \\min(c \\cdot m \\cdot \\log(m), m_\\text{max})\\]\nwhere \\(c\\) = constant (default 2), \\(m\\) = target ESS.\nMultinomial Resampling:\n\nSample indices with replacement proportional to weights\nExpected count for index \\(i\\): \\(E[n_i] = m \\cdot w_i\\)\n\nLatin Hypercube Sampling (LHS):\n\nStratified sampling with one sample per stratum\nReduces variance compared to multinomial for smooth functions\n\n\n\n\n\n\n6.2 Test Criteria\n\n\n\n\n\n\nNoteDetailed SIR Test Criteria (Click to expand)\n\n\n\n\n\nPool Size Computation:\n\n\n\nTest\nInput\nExpected\n\n\n\n\nBasic\nESS=50, c=2.0\n\\(\\lceil 2 \\cdot 50 \\cdot \\log(50) \\rceil\\)\n\n\nCap\nESS=1000, max=500\n500\n\n\nLarge ESS\nESS=1000, max=8192\n8192 (capped)\n\n\nEdge case\nESS=1\n1 (log(1)=0 handled)\n\n\n\nMultinomial Resampling:\n\n\n\nTest\nCriterion\n\n\n\n\nOutput length\nEquals requested count\n\n\nIndex range\nAll indices in [1, n_weights]\n\n\nProportionality\nHigher weights selected more often\n\n\nDeterminism\nSame RNG seed ‚Üí same result\n\n\nUniform weights\nApproximately equal counts\n\n\n\nLHS Resampling:\n\n\n\n\n\n\n\nTest\nCriterion\n\n\n\n\nOutput length\nEquals requested count\n\n\nIndex range\nAll indices in [1, n_weights]\n\n\nVariance reduction\n\\(\\text{Var}(\\bar{X}_\\text{LHS}) \\leq 1.5 \\cdot \\text{Var}(\\bar{X}_\\text{SIR})\\)\n\n\n\nMLL Computation:\n\n\n\n\n\n\n\n\nFunction\nTest\nExpected\n\n\n\n\nmcem_mll_sir\nUniform indices\nMean of selected log-liks\n\n\nmcem_mll_sir\nSubject weights\nWeighted sum\n\n\nmcem_ase_sir\nKnown differences\n\\(\\sqrt{\\sum_i \\text{Var}_i / m_i}\\)\n\n\n\n\n\n\n\n\n6.3 Summary Table\n\n\n\nComponent\nTests\nStatus\n\n\n\n\nPool size formula\n5 cases\n‚úÖ Pass\n\n\nMultinomial resampling\n5 properties\n‚úÖ Pass\n\n\nLHS resampling\n3 properties\n‚úÖ Pass\n\n\nLHS variance reduction\n200 reps comparison\n‚úÖ Pass\n\n\nMLL with SIR\n2 weight configs\n‚úÖ Pass\n\n\nASE with SIR\nKnown variances\n‚úÖ Pass"
  },
  {
    "objectID": "02_unit_tests.html#variance-estimation-test_variance.jl",
    "href": "02_unit_tests.html#variance-estimation-test_variance.jl",
    "title": "Unit Test Coverage",
    "section": "7. Variance Estimation (test_variance.jl)",
    "text": "7. Variance Estimation (test_variance.jl)\nTests verify variance-covariance matrix computation matches analytical formulas.\n\n7.1 Mathematical Reference\n\n\n\n\n\n\nNoteVariance Estimation Mathematical Background (Click to expand)\n\n\n\n\n\nExponential MLE Fisher Information:\n\nObserved information: \\(I(\\hat\\lambda) = n/\\hat\\lambda^2\\)\nMLE variance: \\(\\text{Var}(\\hat\\lambda) = \\hat\\lambda^2/n\\)\nMLE formula: \\(\\hat\\lambda = n / \\sum_i t_i\\)\n\nInfinitesimal Jackknife (IJ):\n\\[\\text{Var}_\\text{IJ} = \\left(\\frac{\\partial^2 \\ell}{\\partial \\theta^2}\\right)^{-1} \\left(\\sum_i \\frac{\\partial \\ell_i}{\\partial \\theta} \\frac{\\partial \\ell_i}{\\partial \\theta}'\\right) \\left(\\frac{\\partial^2 \\ell}{\\partial \\theta^2}\\right)^{-1}\\]\nJK-IJ Relationship:\n\\[\\text{Var}_\\text{JK} = \\frac{n-1}{n} \\text{Var}_\\text{IJ}\\]\n\n\n\n\n\n7.2 Test Criteria\n\n\n\n\n\n\nNoteDetailed Variance Test Criteria (Click to expand)\n\n\n\n\n\nAnalytical Verification (Exponential, n=100):\n\n\n\nTest\nCriterion\nExpected\n\n\n\n\nMLE formula\n\\(\\hat\\lambda = n/\\sum t_i\\)\nrtol=0.001\n\n\nVariance formula\n\\(\\hat\\sigma^2 = \\hat\\lambda^2/n\\)\nrtol=0.01\n\n\nAsymptotic variance\n\\(\\approx \\lambda^2/n = 0.0025\\)\nrtol=0.5\n\n\n\nVariance Matrix Properties:\n\n\n\nProperty\nCriterion\n\n\n\n\nSymmetry\n\\(\\mathbf{V} = \\mathbf{V}'\\) exactly\n\n\nPositive definiteness\nAll eigenvalues &gt; 0\n\n\nEigenvalue sum\n\\(\\sum \\lambda_i = \\text{tr}(\\mathbf{V})\\)\n\n\nDimensions\n\\(p \\times p\\) where \\(p\\) = parameter count\n\n\n\nAPI Functionality:\n\n\n\nTest\nCriterion\n\n\n\n\nget_vcov(type=:model)\nReturns model-based variance\n\n\nget_vcov(type=:ij)\nReturns IJ variance\n\n\nget_vcov(type=:jk)\nReturns jackknife variance\n\n\nUnfitted model\nWarns, returns nothing\n\n\n\n\n\n\n\n\n7.3 Summary Table\n\n\n\nComponent\nTests\nStatus\n\n\n\n\nMLE analytical match\n1 test\n‚úÖ Pass\n\n\nVariance analytical match\n1 test\n‚úÖ Pass\n\n\nSymmetry\n2 families\n‚úÖ Pass\n\n\nPositive definiteness\n2 families\n‚úÖ Pass\n\n\nEigenvalue checksum\n2 families\n‚úÖ Pass\n\n\nAPI type selection\n3 types\n‚úÖ Pass\n\n\nWarning for missing vcov\n1 test\n‚úÖ Pass"
  },
  {
    "objectID": "02_unit_tests.html#pijcv-smoothing-selection-test_pijcv.jl",
    "href": "02_unit_tests.html#pijcv-smoothing-selection-test_pijcv.jl",
    "title": "Unit Test Coverage",
    "section": "8. PIJCV Smoothing Selection (test_pijcv.jl)",
    "text": "8. PIJCV Smoothing Selection (test_pijcv.jl)\nTests verify Predictive Infinitesimal Jackknife Cross-Validation implementation based on Wood (2024).\n\n8.1 Mathematical Reference\n\n\n\n\n\n\nNotePIJCV Mathematical Background (Click to expand)\n\n\n\n\n\nCholesky Downdate:\nGiven \\(\\mathbf{H} = \\mathbf{L}\\mathbf{L}'\\) and vector \\(v\\), compute \\(\\mathbf{L}_{-}\\) such that: \\[\\mathbf{L}_{-}\\mathbf{L}_{-}' = \\mathbf{H} - vv'\\]\nLOO Newton Step:\n\\[\\hat\\theta_{-i} \\approx \\hat\\theta - (\\mathbf{H} - \\mathbf{H}_i)^{-1} g_i\\]\nwhere \\(\\mathbf{H}_i\\) = subject \\(i\\)‚Äôs Hessian contribution, \\(g_i\\) = gradient contribution.\nPIJCV Criterion (Wood 2024):\n\\[\\text{PIJCV}(\\lambda) = \\sum_{i=1}^n \\ell_i(\\hat\\theta_{-i}(\\lambda))\\]\nMinimizing PIJCV selects optimal \\(\\lambda\\).\n\n\n\n\n\n8.2 Test Criteria\n\n\n\n\n\n\nNoteDetailed PIJCV Test Criteria (Click to expand)\n\n\n\n\n\nCholesky Downdate:\n\n\n\n\n\n\n\nTest\nCriterion\n\n\n\n\nCorrectness\n\\(\\mathbf{L}_{-}\\mathbf{L}_{-}' = \\mathbf{H} - vv'\\)\n\n\nPD preservation\nAll eigenvalues &gt; 0 after downdate\n\n\nIndefiniteness detection\nReturns false when \\(\\mathbf{H} - vv'\\) not PD\n\n\nNon-mutating copy\nOriginal \\(\\mathbf{L}\\) unchanged\n\n\nSequential downdates\nAccumulate correctly\n\n\n\nLOO Newton Step:\n\n\n\n\n\n\n\nTest\nCriterion\n\n\n\n\nDirect vs Cholesky\nResults match to rtol=1e-8\n\n\nSubject removal\n\\((\\mathbf{H} - \\mathbf{H}_i)^{-1}\\) computed correctly\n\n\n\nPIJCV Criterion:\n\n\n\n\n\n\n\nTest\nCriterion\n\n\n\n\nPerturbations\nMatch direct LOO computation\n\n\nObjective value\nCorrect sum of LOO likelihoods\n\n\nEnd-to-end API\nselect_smoothing_parameters returns valid \\(\\lambda\\)\n\n\n\n\n\n\n\n\n8.3 Summary Table\n\n\n\nComponent\nTests\nStatus\n\n\n\n\nCholesky downdate correctness\n1 test\n‚úÖ Pass\n\n\nPD preservation\n1 test\n‚úÖ Pass\n\n\nIndefiniteness detection\n1 test\n‚úÖ Pass\n\n\nNon-mutating copy\n1 test\n‚úÖ Pass\n\n\nSequential downdates\n1 test\n‚úÖ Pass\n\n\nLOO Newton step\n2 methods compared\n‚úÖ Pass\n\n\nPIJCV criterion\n1 test\n‚úÖ Pass\n\n\nEnd-to-end API\n1 test\n‚úÖ Pass"
  },
  {
    "objectID": "02_unit_tests.html#phase-type-models-test_phasetype.jl",
    "href": "02_unit_tests.html#phase-type-models-test_phasetype.jl",
    "title": "Unit Test Coverage",
    "section": "9. Phase-Type Models (test_phasetype.jl)",
    "text": "9. Phase-Type Models (test_phasetype.jl)\nTests verify phase-type distribution implementation against analytical formulas.\n\n9.1 Mathematical Reference\n\n\n\n\n\n\nNotePhase-Type Mathematical Background (Click to expand)\n\n\n\n\n\n\\(p\\)-Phase Coxian Distribution:\nSub-intensity matrix \\(\\mathbf{S}\\), absorption rates \\(\\mathbf{a}\\), initial distribution \\(\\boldsymbol{\\pi}\\):\n\nDensity: \\(f(t) = \\boldsymbol{\\pi}' \\exp(\\mathbf{S}t) \\mathbf{a}\\)\nSurvival: \\(S(t) = \\boldsymbol{\\pi}' \\exp(\\mathbf{S}t) \\mathbf{1}\\)\nHazard: \\(h(t) = f(t) / S(t)\\)\nCumulative hazard: \\(H(t) = -\\log S(t)\\)\n\n2-Phase Coxian Analytical Formulas:\nFor \\(\\mathbf{S} = \\begin{pmatrix} -(r+a_1) & r \\\\ 0 & -a_2 \\end{pmatrix}\\), \\(\\boldsymbol{\\pi} = (1, 0)'\\):\n\\[\\exp(\\mathbf{S}t)_{11} = e^{-\\lambda_1 t}, \\quad \\exp(\\mathbf{S}t)_{22} = e^{-\\lambda_2 t}\\] \\[\\exp(\\mathbf{S}t)_{12} = \\frac{r(e^{-\\lambda_1 t} - e^{-\\lambda_2 t})}{\\lambda_2 - \\lambda_1}\\]\nwhere \\(\\lambda_1 = r + a_1\\), \\(\\lambda_2 = a_2\\).\n\n\n\n\n\n9.2 Test Criteria\n\n\n\n\n\n\nNoteDetailed Phase-Type Test Criteria (Click to expand)\n\n\n\n\n\nEmission Matrix:\n\n\n\n\n\n\n\nTest\nCriterion\n\n\n\n\nPanel observation (obstype=2)\nAll phases of destination state = 1.0\n\n\nExact transition (obstype=1)\nOnly first phase = 1.0\n\n\nFully censored (obstype=0)\nAll phases = 1.0\n\n\nCustom censoring patterns\nCorrect pattern application\n\n\n\nData Expansion:\n\n\n\nTest\nCriterion\n\n\n\n\nExact transition split\nCreates intermediate ‚Äúremain‚Äù row\n\n\nPanel observation\nNo expansion needed\n\n\nSubject index recomputation\nIndices remain valid\n\n\n\nHazard/Survival Correctness:\n\n\n\nTest\nCriterion\nTolerance\n\n\n\n\n2-phase survival\nMatches analytical formula\nrtol=1e-10\n\n\n2-phase density\nMatches analytical formula\nrtol=1e-10\n\n\n2-phase hazard\n\\(h(t) = f(t)/S(t)\\)\nrtol=1e-10\n\n\nCumulative hazard\n\\(H(t) = -\\log S(t)\\)\nrtol=1e-10\n\n\nLog-likelihood\nAnalytical path likelihood\nrtol=1e-8\n\n\n\nPath Operations:\n\n\n\nTest\nCriterion\n\n\n\n\nExpand path\nObservable states ‚Üí phase states\n\n\nCollapse path\nPhase states ‚Üí observable states\n\n\nRound-trip\nExpand then collapse = original\n\n\n\n\n\n\n\n\n9.3 Summary Table\n\n\n\nComponent\nTests\nStatus\n\n\n\n\nEmission matrix obstype=1\n1 test\n‚úÖ Pass\n\n\nEmission matrix obstype=2\n1 test\n‚úÖ Pass\n\n\nEmission matrix obstype=0\n1 test\n‚úÖ Pass\n\n\nData expansion\n2 tests\n‚úÖ Pass\n\n\n2-phase survival\n5 time points\n‚úÖ Pass\n\n\n2-phase density\n5 time points\n‚úÖ Pass\n\n\n2-phase hazard\n5 time points\n‚úÖ Pass\n\n\nPath expansion\n1 test\n‚úÖ Pass\n\n\nPath collapse\n1 test\n‚úÖ Pass\n\n\nLog-likelihood\n3 paths\n‚úÖ Pass"
  },
  {
    "objectID": "02_unit_tests.html#likelihood-functions-test_mll_consistency.jl-test_loglik_analytical.jl",
    "href": "02_unit_tests.html#likelihood-functions-test_mll_consistency.jl-test_loglik_analytical.jl",
    "title": "Unit Test Coverage",
    "section": "10. Likelihood Functions (test_mll_consistency.jl, test_loglik_analytical.jl)",
    "text": "10. Likelihood Functions (test_mll_consistency.jl, test_loglik_analytical.jl)\nTests verify likelihood computation consistency across methods.\n\n10.1 Mathematical Reference\n\n\n\n\n\n\nNoteLikelihood Mathematical Background (Click to expand)\n\n\n\n\n\nExact Data Likelihood:\n\\[\\ell(\\theta) = \\sum_i \\left[ \\log h(t_i | x_i) - H(0, t_i | x_i) \\right]\\]\nPanel Data Likelihood (Markov):\n\\[\\ell(\\theta) = \\sum_i \\log \\mathbf{P}(s_i | s_{i-1}, t_i - t_{i-1})\\]\nwhere \\(\\mathbf{P} = \\exp(\\mathbf{Q} \\Delta t)\\) is the transition probability matrix.\nIS-Weighted MLL:\n\\[\\hat\\ell = \\sum_i w_i \\sum_j p_{ij} \\ell_{ij}\\]\n\n\n\n\n\n10.2 Test Criteria\n\n\n\n\n\n\nNoteDetailed Likelihood Test Criteria (Click to expand)\n\n\n\n\n\nMLL Consistency Across Methods:\n\n\n\nMethod Pair\nCriterion\nTolerance\n\n\n\n\nIS-weighted vs SIR\nSample-level agreement\n15% relative\n\n\nIS-weighted vs LHS\nSample-level agreement\n15% relative\n\n\nSIR vs LHS\nSample-level agreement\n15% relative\n\n\nSubject-level correlation\n\\(r &gt; 0.85\\)\nN/A\n\n\nSubject-level MAE\n&lt; 25% relative\nN/A\n\n\n\nAnalytical Likelihood (Exponential):\n\n\n\nTest\nCriterion\n\n\n\n\nFormula\n\\(\\ell = n\\log\\lambda - \\lambda \\sum t_i\\)\n\n\nGradient\nMatches ForwardDiff\n\n\n\n\n\n\n\n\n10.3 Summary Table\n\n\n\nComponent\nTests\nStatus\n\n\n\n\nIS vs SIR agreement\n1 test\n‚úÖ Pass\n\n\nIS vs LHS agreement\n1 test\n‚úÖ Pass\n\n\nSubject-level correlation\n3 pairs\n‚úÖ Pass\n\n\nSubject-level MAE\n2 methods\n‚úÖ Pass\n\n\nAnalytical likelihood\n1 family\n‚úÖ Pass"
  },
  {
    "objectID": "02_unit_tests.html#model-construction-test_modelgeneration.jl-test_initialization.jl",
    "href": "02_unit_tests.html#model-construction-test_modelgeneration.jl-test_initialization.jl",
    "title": "Unit Test Coverage",
    "section": "11. Model Construction (test_modelgeneration.jl, test_initialization.jl)",
    "text": "11. Model Construction (test_modelgeneration.jl, test_initialization.jl)\nTests verify model building and parameter initialization.\n\n11.1 Test Criteria\n\n\n\n\n\n\nNoteDetailed Model Construction Test Criteria (Click to expand)\n\n\n\n\n\nError Detection:\n\n\n\nTest\nInput\nExpected\n\n\n\n\nDuplicate transitions\nTwo hazards 1‚Üí2\nArgumentError\n\n\nState 0 origin\nHazard(..., 0, 1)\nArgumentError\n\n\nState 0 destination\nHazard(..., 1, 0)\nArgumentError\n\n\nSelf-transition\nHazard(..., 1, 1)\nArgumentError\n\n\nInvalid family\nHazard(..., \"invalid\", ...)\nArgumentError\n\n\n\nCensoring Pattern Handling:\n\n\n\nTest\nCriterion\n\n\n\n\nPattern parsing\nRow sums ‚â§ 1\n\n\nobstype &gt; 2\nUses pattern matrix\n\n\nobstype = 0\nAll states possible\n\n\n\nInitialization Methods:\n\n\n\nMethod\nWhen Used\nCriterion\n\n\n\n\n:crude\nMarkov models\nParameters non-NaN\n\n\n:surrogate\nSemi-Markov models\nParameters non-NaN\n\n\n:auto\nDefault\nSelects appropriate method\n\n\n\n\n\n\n\n\n11.2 Summary Table\n\n\n\nComponent\nTests\nStatus\n\n\n\n\nDuplicate transition detection\n1 test\n‚úÖ Pass\n\n\nState 0 detection\n2 tests\n‚úÖ Pass\n\n\nSelf-transition detection\n1 test\n‚úÖ Pass\n\n\nCensoring patterns\n1 test\n‚úÖ Pass\n\n\nCrude initialization\n1 test\n‚úÖ Pass\n\n\nAuto method selection\n1 test\n‚úÖ Pass"
  },
  {
    "objectID": "02_unit_tests.html#reconstructor-test_reconstructor.jl",
    "href": "02_unit_tests.html#reconstructor-test_reconstructor.jl",
    "title": "Unit Test Coverage",
    "section": "12. ReConstructor (test_reconstructor.jl)",
    "text": "12. ReConstructor (test_reconstructor.jl)\nTests verify AD-compatible parameter flattening/unflattening.\n\n12.1 Test Criteria\n\n\n\n\n\n\nNoteDetailed ReConstructor Test Criteria (Click to expand)\n\n\n\n\n\nType Construction:\n\n\n\nInput Type\nFlatten Output\nUnflatten Output\n\n\n\n\nFloat64\n[x]\nx::Float64\n\n\nVector{Float64}\nx (identity)\nx::Vector\n\n\nVector{Int}\n[] (empty)\nOriginal (no flatten)\n\n\nTuple\nConcatenated\nTuple with correct lengths\n\n\nNamedTuple\nConcatenated\nNamedTuple with keys preserved\n\n\n\nAD Compatibility:\n\n\n\nTest\nCriterion\n\n\n\n\nForwardDiff gradient\nThrough unflatten ‚Üí compute ‚Üí flatten\n\n\nType stability\nReturn type matches input type\n\n\nStrict mode\nExact key preservation\n\n\nFlexible mode\nStructural preservation only\n\n\n\nRound-trip:\n\n\n\nTest\nCriterion\n\n\n\n\nunflatten(flatten(x)) == x\nFor all supported types\n\n\nNested structures\nRecursive reconstruction\n\n\n\n\n\n\n\n\n12.2 Summary Table\n\n\n\nComponent\nTests\nStatus\n\n\n\n\nReal number\n2 modes\n‚úÖ Pass\n\n\nFloat vector\n1 test\n‚úÖ Pass\n\n\nInt vector (skip)\n1 test\n‚úÖ Pass\n\n\nTuple\n2 depths\n‚úÖ Pass\n\n\nNamedTuple\n2 depths\n‚úÖ Pass\n\n\nAD compatibility\nForwardDiff\n‚úÖ Pass\n\n\nRound-trip\nAll types\n‚úÖ Pass"
  },
  {
    "objectID": "02_unit_tests.html#additional-tests",
    "href": "02_unit_tests.html#additional-tests",
    "title": "Unit Test Coverage",
    "section": "13. Additional Tests",
    "text": "13. Additional Tests\n\n13.1 AD Backends (test_ad_backends.jl)\n\n\n\nTest\nCriterion\nStatus\n\n\n\n\nType existence\nADBackend types defined\n‚úÖ Pass\n\n\nDefault selection\nParameter count thresholds\n‚úÖ Pass\n\n\nForwardDiff gradient\nFinite, correct length\n‚úÖ Pass\n\n\nfit() integration\nadbackend kwarg works\n‚úÖ Pass\n\n\n\n\n\n13.2 @hazard Macro (test_hazard_macro.jl)\n\n\n\nTest\nCriterion\nStatus\n\n\n\n\nFamily aliases\n:exp, :wei, :gom, :sp, :pt\n‚úÖ Pass\n\n\nTransition syntax\nMultiple formats\n‚úÖ Pass\n\n\nFormula defaults\nIntercept-only\n‚úÖ Pass\n\n\nKeyword forwarding\nknots, n_phases, etc.\n‚úÖ Pass\n\n\nError messages\nMissing/invalid family\n‚úÖ Pass\n\n\n\n\n\n13.3 Model Output (test_model_output.jl)\n\n\n\nTest\nCriterion\nStatus\n\n\n\n\nAIC formula\n\\(-2\\ell + 2k\\)\n‚úÖ Pass\n\n\nBIC formula\n\\(-2\\ell + k\\log n\\)\n‚úÖ Pass\n\n\nSummary table\nTypedTable structure\n‚úÖ Pass\n\n\nestimate_loglik\nESS, MCSE fields\n‚úÖ Pass"
  },
  {
    "objectID": "02_unit_tests.html#coverage-gaps-and-future-work",
    "href": "02_unit_tests.html#coverage-gaps-and-future-work",
    "title": "Unit Test Coverage",
    "section": "14. Coverage Gaps and Future Work",
    "text": "14. Coverage Gaps and Future Work\n\n14.1 Current Gaps\n\n\n\n\n\n\n\n\nArea\nGap\nPriority\n\n\n\n\nAFT + TVC\nLimited testing for AFT with time-varying covariates\nMedium\n\n\nPanel + Splines\nIntegration tests for spline hazards with panel data\nMedium\n\n\nEdge cases\nDegenerate transition matrices\nLow\n\n\nPerformance\nBenchmark regression tests\nLow\n\n\n\n\n\n14.2 Test Suite History\n\n\n\n\n\n\n\n\nDate\nAction\nResult\n\n\n\n\n2026-01-23\nMajor cleanup: consolidated/deleted redundant tests\n26 files, 2831 tests\n\n\n2026-01-22\nMerged variance and weight tests\nReduced redundancy\n\n\n2026-01-21\nMerged phase-type tests (7‚Üí2 files)\nConsolidated coverage\n\n\n2026-01\nAdded analytical likelihood tests\ntest_loglik_analytical.jl\n\n\n2026-01\nAdded penalty infrastructure tests\ntest_penalty_infrastructure.jl\n\n\n\n\n\n14.3 Deleted Test Files (Cleanup 2026-01-23)\nThe following files were deleted during test suite cleanup due to redundancy, obsolescence, or low value:\n\ntest_perf.jl - Hardware-dependent, moved to benchmarks\ntest_error_paths.jl - Redundant with test_error_messages.jl\ntest_infrastructure.jl - Tested internal implementation details\ntest_regressions.jl - Bug fixes now covered by other tests\ntest_modelgeneration.jl - Superseded by test_hazard_macro.jl\ntest_mll_consistency.jl - Superseded by test_loglik_analytical.jl\ntest_helpers.jl - Low value utility function tests\ntest_efs.jl - Outdated EDF implementation\ntest_reconstructor.jl - Internal ReConstructor tests\ntest_phasetype_*.jl (6 files) - Merged into test_phasetype.jl\ntest_pijcv_reference.jl, test_pijcv_vs_loocv.jl - Merged into test_pijcv.jl\ntest_constrained_variance.jl - Merged into test_variance.jl\ntest_observation_weights_emat.jl, test_weight_validation.jl - Merged into test_subject_weights.jl\ntest_concurrency.jl, test_ordering_at.jl - Stale APIs"
  },
  {
    "objectID": "02_unit_tests.html#running-tests",
    "href": "02_unit_tests.html#running-tests",
    "title": "Unit Test Coverage",
    "section": "15. Running Tests",
    "text": "15. Running Tests\n# Run all unit tests\njulia --project -e 'using Pkg; Pkg.test()'\n\n# Run specific test file (standalone - may need imports)\njulia --project -e 'using Test, MultistateModels; include(\"MultistateModelsTests/unit/test_hazards.jl\")'\n\n# Run with full long tests\nMSM_TEST_LEVEL=full julia --project -e 'using Pkg; Pkg.test()'"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MultistateModels.jl",
    "section": "",
    "text": "Branch: penalized_splines ¬† Commit: 5c5b5c6"
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "MultistateModels.jl",
    "section": "Overview",
    "text": "Overview\nComprehensive validation and documentation for MultistateModels.jl ‚Äî a Julia package for continuous-time multistate modeling.\n\n\nüìê Architecture\nType hierarchy, hazard families, inference engine, and simulation strategies.\n\n\n‚úÖ Unit Tests\nFunction-level tests for hazards, simulation, MCEM, and model construction.\n\n\nüìä Long Tests\nStatistical recovery of parameters across hazard families and data types.\n\n\nüéØ Simulation\nVisual verification of event time distributions (CDF diagnostic plots).\n\n\n‚ö° Benchmarks\nPerformance comparison of sampling methods and optimization strategies.\n\n\nüì¶ Main Package\nSource code and documentation for MultistateModels.jl."
  },
  {
    "objectID": "index.html#test-results-summary",
    "href": "index.html#test-results-summary",
    "title": "MultistateModels.jl",
    "section": "Test Results Summary",
    "text": "Test Results Summary\nNo cached test results available."
  },
  {
    "objectID": "index.html#quick-start",
    "href": "index.html#quick-start",
    "title": "MultistateModels.jl",
    "section": "Quick Start",
    "text": "Quick Start\n\nBuilding Reports Locally\ncd MultistateModelsTests/reports\nquarto render\n\n\nRunning Tests and Recording Results\n# Run full test suite with automatic cache recording\njulia --project=. -e 'using MultistateModelsTests; MultistateModelsTests.runtests()'\n\n# Or run with the dedicated script\njulia --project=. scripts/run_all_tests.jl\nTest results are automatically cached when running via runtests().\n\n\nChecking Cache Status\njulia --project=. scripts/refresh_cache.jl"
  },
  {
    "objectID": "03_long_tests.html",
    "href": "03_long_tests.html",
    "title": "Long Tests Status",
    "section": "",
    "text": "All long tests validate statistical parameter recovery with relative tolerances of 15-35% depending on the complexity of the model and inference method.\nLast full run: January 23, 2026\nBranch: penalized_splines\n\n\n\n\n\n\nNoteTest Suite Cleanup (2026-01-23)\n\n\n\nLong test suite consolidated from 24 files to 13 files. Deleted redundant/deprecated tests: longtest_aft_suite.jl, longtest_sir.jl, longtest_phasetype.jl, longtest_pijcv_loocv.jl, longtest_sensitivity_check.jl, longtest_smooth_covariate_recovery.jl, longtest_tensor_product_recovery.jl, longtest_spline_suite.jl.\nHelper files moved to src/: longtest_config.jl, longtest_helpers.jl, phasetype_longtest_helpers.jl.\n\n\n\n\n\n\n\nTest Suite\nFile\nStatus\n\n\n\n\nParametric Suite\nlongtest_parametric_suite.jl\n‚úÖ\n\n\nExact Markov\nlongtest_exact_markov.jl\n‚úÖ\n\n\nMCEM Parametric\nlongtest_mcem.jl\n‚úÖ\n\n\nMCEM Splines\nlongtest_mcem_splines.jl\n‚úÖ\n\n\nMCEM TVC\nlongtest_mcem_tvc.jl\n‚úÖ\n\n\nPhase-Type Exact\nlongtest_phasetype_exact.jl\n‚úÖ\n\n\nPhase-Type Panel\nlongtest_phasetype_panel.jl\n‚úÖ\n\n\nRobust Parametric\nlongtest_robust_parametric.jl\n‚úÖ\n\n\nRobust Markov/PT\nlongtest_robust_markov_phasetype.jl\n‚úÖ\n\n\nSimulation Dist\nlongtest_simulation_distribution.jl\n‚úÖ\n\n\nSimulation TVC\nlongtest_simulation_tvc.jl\n‚úÖ\n\n\nSpline Exact\nlongtest_spline_exact.jl\n‚úÖ\n\n\nVariance Validation\nlongtest_variance_validation.jl\n‚úÖ\n\n\n\nTotal: 13 long test files"
  },
  {
    "objectID": "03_long_tests.html#overview",
    "href": "03_long_tests.html#overview",
    "title": "Long Tests Status",
    "section": "",
    "text": "All long tests validate statistical parameter recovery with relative tolerances of 15-35% depending on the complexity of the model and inference method.\nLast full run: January 23, 2026\nBranch: penalized_splines\n\n\n\n\n\n\nNoteTest Suite Cleanup (2026-01-23)\n\n\n\nLong test suite consolidated from 24 files to 13 files. Deleted redundant/deprecated tests: longtest_aft_suite.jl, longtest_sir.jl, longtest_phasetype.jl, longtest_pijcv_loocv.jl, longtest_sensitivity_check.jl, longtest_smooth_covariate_recovery.jl, longtest_tensor_product_recovery.jl, longtest_spline_suite.jl.\nHelper files moved to src/: longtest_config.jl, longtest_helpers.jl, phasetype_longtest_helpers.jl.\n\n\n\n\n\n\n\nTest Suite\nFile\nStatus\n\n\n\n\nParametric Suite\nlongtest_parametric_suite.jl\n‚úÖ\n\n\nExact Markov\nlongtest_exact_markov.jl\n‚úÖ\n\n\nMCEM Parametric\nlongtest_mcem.jl\n‚úÖ\n\n\nMCEM Splines\nlongtest_mcem_splines.jl\n‚úÖ\n\n\nMCEM TVC\nlongtest_mcem_tvc.jl\n‚úÖ\n\n\nPhase-Type Exact\nlongtest_phasetype_exact.jl\n‚úÖ\n\n\nPhase-Type Panel\nlongtest_phasetype_panel.jl\n‚úÖ\n\n\nRobust Parametric\nlongtest_robust_parametric.jl\n‚úÖ\n\n\nRobust Markov/PT\nlongtest_robust_markov_phasetype.jl\n‚úÖ\n\n\nSimulation Dist\nlongtest_simulation_distribution.jl\n‚úÖ\n\n\nSimulation TVC\nlongtest_simulation_tvc.jl\n‚úÖ\n\n\nSpline Exact\nlongtest_spline_exact.jl\n‚úÖ\n\n\nVariance Validation\nlongtest_variance_validation.jl\n‚úÖ\n\n\n\nTotal: 13 long test files"
  },
  {
    "objectID": "03_long_tests.html#exponential-hazard-models",
    "href": "03_long_tests.html#exponential-hazard-models",
    "title": "Long Tests Status",
    "section": "2. Exponential Hazard Models",
    "text": "2. Exponential Hazard Models\n\n2.1 exp_exact_nocov\nTest: exp_exact_nocov | Passed: ‚úÖ | Time: 0.0s\n\n\n\nParameter\nTrue\nEstimated\nSE\nCI Low\nCI High\nRel Err %\nCovered\n\n\n\n\nh12_rate\n0.15\n0.1506\n0.0053\n0.137\n0.1641\n0.4\n‚úì\n\n\nh23_rate\n0.12\n0.1281\n0.0052\n0.1146\n0.1415\n6.7\n‚úì\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†1: Cumulative Incidence\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†2: State Prevalence\n\n\n\n\n\n\n\n2.2 exp_exact_fixed\nTest: exp_exact_fixed | Passed: ‚úÖ | Time: 0.0s\n\n\n\nParameter\nTrue\nEstimated\nSE\nCI Low\nCI High\nRel Err %\nCovered\n\n\n\n\nh12_beta\n0.5\n0.3753\n0.0632\n0.2126\n0.538\n24.9\n‚úì\n\n\nh12_rate\n0.15\n0.1588\n0.0068\n0.1412\n0.1764\n5.9\n‚úì\n\n\nh23_beta\n0.5\n0.3806\n0.0761\n0.1846\n0.5765\n23.9\n‚úì\n\n\nh23_rate\n0.12\n0.1221\n0.0067\n0.1048\n0.1394\n1.8\n‚úì\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†3: Cumulative Incidence\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†4: State Prevalence\n\n\n\n\n\n\n\n2.3 exp_exact_tvc\nTest: exp_exact_tvc | Passed: ‚úÖ | Time: 0.0s\n\n\n\nParameter\nTrue\nEstimated\nSE\nCI Low\nCI High\nRel Err %\nCovered\n\n\n\n\nh12_beta\n0.5\n0.4832\n0.0633\n0.3202\n0.6462\n3.4\n‚úì\n\n\nh12_rate\n0.15\n0.1493\n0.0064\n0.1327\n0.1659\n0.4\n‚úì\n\n\nh23_beta\n0.5\n0.5306\n0.0943\n0.2877\n0.7734\n6.1\n‚úì\n\n\nh23_rate\n0.12\n0.1165\n0.01\n0.0908\n0.1421\n2.9\n‚úì\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†5: Cumulative Incidence\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†6: State Prevalence\n\n\n\n\n\n\n\n2.4 exp_panel_nocov\nTest: exp_panel_nocov | Passed: ‚úÖ | Time: 0.0s\n\n\n\nParameter\nTrue\nEstimated\nSE\nCI Low\nCI High\nRel Err %\nCovered\n\n\n\n\nh12_rate\n0.15\n0.1558\n0.0054\n0.1419\n0.1697\n3.9\n‚úì\n\n\nh23_rate\n0.12\n0.1175\n0.0059\n0.1022\n0.1328\n2.1\n‚úì\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†7: Cumulative Incidence\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†8: State Prevalence\n\n\n\n\n\n\n\n2.5 exp_panel_fixed\nTest: exp_panel_fixed | Passed: ‚úÖ | Time: 0.0s\n\n\n\nParameter\nTrue\nEstimated\nSE\nCI Low\nCI High\nRel Err %\nCovered\n\n\n\n\nh12_beta\n0.5\n0.4897\n0.0674\n0.3162\n0.6632\n2.1\n‚úì\n\n\nh12_rate\n0.15\n0.1516\n0.0073\n0.1329\n0.1704\n1.1\n‚úì\n\n\nh23_beta\n0.5\n0.5413\n0.0905\n0.3081\n0.7746\n8.3\n‚úì\n\n\nh23_rate\n0.12\n0.1147\n0.0082\n0.0935\n0.1359\n4.4\n‚úì\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†9: Cumulative Incidence\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†10: State Prevalence\n\n\n\n\n\n\n\n2.6 exp_panel_tvc\nTest: exp_panel_tvc | Passed: ‚úÖ | Time: 0.0s\n\n\n\nParameter\nTrue\nEstimated\nSE\nCI Low\nCI High\nRel Err %\nCovered\n\n\n\n\nh12_beta\n0.5\n0.498\n0.0693\n0.3194\n0.6766\n0.4\n‚úì\n\n\nh12_rate\n0.15\n0.15\n0.0065\n0.1332\n0.1668\n0.0\n‚úì\n\n\nh23_beta\n0.5\n0.5008\n0.0968\n0.2514\n0.7502\n0.2\n‚úì\n\n\nh23_rate\n0.12\n0.1261\n0.0104\n0.0994\n0.1528\n5.1\n‚úì\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†11: Cumulative Incidence\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†12: State Prevalence"
  },
  {
    "objectID": "03_long_tests.html#weibull-hazard-models",
    "href": "03_long_tests.html#weibull-hazard-models",
    "title": "Long Tests Status",
    "section": "3. Weibull Hazard Models",
    "text": "3. Weibull Hazard Models\n\n3.1 wei_exact_nocov\nTest: wei_exact_nocov | Passed: ‚úÖ | Time: 0.0s\n\n\n\nParameter\nTrue\nEstimated\nSE\nCI Low\nCI High\nRel Err %\nCovered\n\n\n\n\nh12_scale\n0.15\n0.1597\n0.0098\n0.1343\n0.185\n6.4\n‚úì\n\n\nh12_shape\n1.3\n1.2714\n0.0301\n1.1938\n1.349\n2.2\n‚úì\n\n\nh23_scale\n0.12\n0.1459\n0.01\n0.1201\n0.1716\n21.5\n‚úó\n\n\nh23_shape\n1.1\n1.0215\n0.0306\n0.9427\n1.1004\n7.1\n‚úì\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†13: Cumulative Incidence\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†14: State Prevalence\n\n\n\n\n\n\n\n3.2 wei_exact_fixed\nTest: wei_exact_fixed | Passed: ‚úÖ | Time: 0.0s\n\n\n\nParameter\nTrue\nEstimated\nSE\nCI Low\nCI High\nRel Err %\nCovered\n\n\n\n\nh12_beta\n0.5\n0.5138\n0.0656\n0.3448\n0.6828\n2.8\n‚úì\n\n\nh12_scale\n0.15\n0.1518\n0.0105\n0.1249\n0.1788\n1.2\n‚úì\n\n\nh12_shape\n1.3\n1.2936\n0.0304\n1.2152\n1.372\n0.5\n‚úì\n\n\nh23_beta\n0.5\n0.4856\n0.0675\n0.3117\n0.6595\n2.9\n‚úì\n\n\nh23_scale\n0.12\n0.124\n0.0102\n0.0978\n0.1501\n3.3\n‚úì\n\n\nh23_shape\n1.1\n1.0763\n0.0317\n0.9948\n1.1579\n2.2\n‚úì\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†15: Cumulative Incidence\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†16: State Prevalence\n\n\n\n\n\n\n\n3.3 wei_exact_tvc\nTest: wei_exact_tvc | Passed: ‚úÖ | Time: 0.0s\n\n\n\nParameter\nTrue\nEstimated\nSE\nCI Low\nCI High\nRel Err %\nCovered\n\n\n\n\nh12_beta\n0.5\n0.4739\n0.0909\n0.2397\n0.708\n5.2\n‚úì\n\n\nh12_scale\n0.15\n0.151\n0.01\n0.1253\n0.1767\n0.7\n‚úì\n\n\nh12_shape\n1.3\n1.3483\n0.0416\n1.2411\n1.4556\n3.7\n‚úì\n\n\nh23_beta\n0.5\n0.4002\n0.0892\n0.1704\n0.63\n20.0\n‚úì\n\n\nh23_scale\n0.12\n0.1221\n0.0093\n0.098\n0.1461\n1.7\n‚úì\n\n\nh23_shape\n1.1\n1.1236\n0.0368\n1.0287\n1.2185\n2.1\n‚úì\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†17: Cumulative Incidence\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†18: State Prevalence\n\n\n\n\n\n\n\n3.4 wei_mcem_nocov\nTest: wei_mcem_nocov | Passed: ‚úÖ | Time: 0.0s\n\n\n\nParameter\nTrue\nEstimated\nSE\nCI Low\nCI High\nRel Err %\nCovered\n\n\n\n\nh12_scale\n0.15\n0.1947\n0.0157\n0.1542\n0.2351\n29.8\n‚úó\n\n\nh12_shape\n1.3\n1.1645\n0.0404\n1.0605\n1.2684\n10.4\n‚úó\n\n\nh23_scale\n0.12\n0.0928\n0.0068\n0.0753\n0.1103\n22.7\n‚úó\n\n\nh23_shape\n1.1\n1.222\n0.0336\n1.1355\n1.3086\n11.1\n‚úó\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†19: Cumulative Incidence\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†20: State Prevalence\n\n\n\n\n\n\n\n3.5 wei_mcem_fixed\nTest: wei_mcem_fixed | Passed: ‚úÖ | Time: 0.0s\n\n\n\nParameter\nTrue\nEstimated\nSE\nCI Low\nCI High\nRel Err %\nCovered\n\n\n\n\nh12_beta\n0.5\n0.4717\n0.0625\n0.3106\n0.6328\n5.7\n‚úì\n\n\nh12_scale\n0.15\n0.1649\n0.0142\n0.1284\n0.2014\n9.9\n‚úì\n\n\nh12_shape\n1.3\n1.2753\n0.0408\n1.1702\n1.3803\n1.9\n‚úì\n\n\nh23_beta\n0.5\n0.598\n0.0758\n0.4027\n0.7933\n19.6\n‚úì\n\n\nh23_scale\n0.12\n0.0906\n0.0072\n0.0721\n0.1091\n24.5\n‚úó\n\n\nh23_shape\n1.1\n1.1987\n0.0303\n1.1207\n1.2767\n9.0\n‚úó\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†21: Cumulative Incidence\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†22: State Prevalence\n\n\n\n\n\n\n\n3.6 wei_mcem_tvc\nTest: wei_mcem_tvc | Passed: ‚úÖ | Time: 0.0s\n\n\n\nParameter\nTrue\nEstimated\nSE\nCI Low\nCI High\nRel Err %\nCovered\n\n\n\n\nh12_beta\n0.5\n0.5557\n0.1035\n0.289\n0.8224\n11.1\n‚úì\n\n\nh12_scale\n0.15\n0.1626\n0.0146\n0.125\n0.2002\n8.4\n‚úì\n\n\nh12_shape\n1.3\n1.2475\n0.0578\n1.0986\n1.3964\n4.0\n‚úì\n\n\nh23_beta\n0.5\n0.5325\n0.0955\n0.2866\n0.7784\n6.5\n‚úì\n\n\nh23_scale\n0.12\n0.0844\n0.0072\n0.0658\n0.103\n29.7\n‚úó\n\n\nh23_shape\n1.1\n1.2483\n0.0368\n1.1535\n1.3432\n13.5\n‚úó\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†23: Cumulative Incidence\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†24: State Prevalence"
  },
  {
    "objectID": "03_long_tests.html#gompertz-hazard-models",
    "href": "03_long_tests.html#gompertz-hazard-models",
    "title": "Long Tests Status",
    "section": "4. Gompertz Hazard Models",
    "text": "4. Gompertz Hazard Models\n\n4.1 gom_exact_nocov\nTest: gom_exact_nocov | Passed: ‚úÖ | Time: 0.0s\n\n\n\nParameter\nTrue\nEstimated\nSE\nCI Low\nCI High\nRel Err %\nCovered\n\n\n\n\nh12_rate\n0.04\n0.0451\n0.0033\n0.0366\n0.0536\n12.7\n‚úì\n\n\nh12_shape\n0.08\n0.0744\n0.0087\n0.052\n0.0967\n7.1\n‚úì\n\n\nh23_rate\n0.03\n0.0367\n0.0045\n0.0252\n0.0482\n22.3\n‚úì\n\n\nh23_shape\n0.06\n0.0233\n0.0208\n-0.0303\n0.0768\n61.2\n‚úì\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†25: Cumulative Incidence\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†26: State Prevalence\n\n\n\n\n\n\n\n4.2 gom_exact_fixed\nTest: gom_exact_fixed | Passed: ‚úÖ | Time: 0.0s\n\n\n\nParameter\nTrue\nEstimated\nSE\nCI Low\nCI High\nRel Err %\nCovered\n\n\n\n\nh12_beta\n0.5\n0.5307\n0.072\n0.3452\n0.7162\n6.1\n‚úì\n\n\nh12_rate\n0.04\n0.0371\n0.0034\n0.0284\n0.0458\n7.3\n‚úì\n\n\nh12_shape\n0.08\n0.092\n0.0089\n0.069\n0.1149\n14.9\n‚úì\n\n\nh23_beta\n0.5\n0.5945\n0.1345\n0.2482\n0.9408\n18.9\n‚úì\n\n\nh23_rate\n0.03\n0.0346\n0.0047\n0.0224\n0.0468\n15.3\n‚úì\n\n\nh23_shape\n0.06\n0.0124\n0.0179\n-0.0338\n0.0586\n79.3\n‚úó\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†27: Cumulative Incidence\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†28: State Prevalence\n\n\n\n\n\n\n\n4.3 gom_exact_tvc\nTest: gom_exact_tvc | Passed: ‚úÖ | Time: 0.0s\n\n\n\nParameter\nTrue\nEstimated\nSE\nCI Low\nCI High\nRel Err %\nCovered\n\n\n\n\nh12_beta\n0.5\n0.4752\n0.1246\n0.1543\n0.7961\n5.0\n‚úì\n\n\nh12_rate\n0.04\n0.0396\n0.003\n0.0318\n0.0475\n1.0\n‚úì\n\n\nh12_shape\n0.08\n0.0889\n0.0138\n0.0533\n0.1246\n11.2\n‚úì\n\n\nh23_beta\n0.5\n0.3798\n0.2564\n-0.2806\n1.0402\n24.0\n‚úì\n\n\nh23_rate\n0.03\n0.0287\n0.0069\n0.011\n0.0463\n4.4\n‚úì\n\n\nh23_shape\n0.06\n0.0705\n0.0177\n0.0248\n0.1161\n17.4\n‚úì\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†29: Cumulative Incidence\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†30: State Prevalence\n\n\n\n\n\n\n\n4.4 gom_mcem_nocov\nTest: gom_mcem_nocov | Passed: ‚úÖ | Time: 0.0s\n\n\n\nParameter\nTrue\nEstimated\nSE\nCI Low\nCI High\nRel Err %\nCovered\n\n\n\n\nh12_rate\n0.04\n0.0403\n0.0026\n0.0335\n0.047\n0.6\n‚úì\n\n\nh12_shape\n0.08\n0.0787\n0.0052\n0.0655\n0.092\n1.6\n‚úì\n\n\nh23_rate\n0.03\n0.0277\n0.0023\n0.0217\n0.0338\n7.5\n‚úì\n\n\nh23_shape\n0.06\n0.0708\n0.0082\n0.0498\n0.0918\n18.0\n‚úì\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†31: Cumulative Incidence\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†32: State Prevalence\n\n\n\n\n\n\n\n4.5 gom_mcem_fixed\nTest: gom_mcem_fixed | Passed: ‚úÖ | Time: 0.0s\n\n\n\nParameter\nTrue\nEstimated\nSE\nCI Low\nCI High\nRel Err %\nCovered\n\n\n\n\nh12_beta\n0.5\n0.5428\n0.0669\n0.3705\n0.7152\n8.6\n‚úì\n\n\nh12_rate\n0.04\n0.0379\n0.0028\n0.0306\n0.0452\n5.2\n‚úì\n\n\nh12_shape\n0.08\n0.0813\n0.0054\n0.0673\n0.0953\n1.6\n‚úì\n\n\nh23_beta\n0.5\n0.5008\n0.087\n0.2766\n0.725\n0.2\n‚úì\n\n\nh23_rate\n0.03\n0.0274\n0.0025\n0.0209\n0.0338\n8.8\n‚úì\n\n\nh23_shape\n0.06\n0.0678\n0.0074\n0.0487\n0.087\n13.1\n‚úì\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†33: Cumulative Incidence\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†34: State Prevalence\n\n\n\n\n\n\n\n4.6 gom_mcem_tvc\nTest: gom_mcem_tvc | Passed: ‚úÖ | Time: 0.0s\n\n\n\nParameter\nTrue\nEstimated\nSE\nCI Low\nCI High\nRel Err %\nCovered\n\n\n\n\nh12_beta\n0.5\n0.7611\n0.109\n0.4802\n1.042\n52.2\n‚úì\n\n\nh12_rate\n0.04\n0.0395\n0.0029\n0.032\n0.047\n1.2\n‚úì\n\n\nh12_shape\n0.08\n0.0612\n0.0086\n0.039\n0.0835\n23.5\n‚úì\n\n\nh23_beta\n0.5\n0.6048\n0.2885\n-0.1385\n1.348\n21.0\n‚úì\n\n\nh23_rate\n0.03\n0.0233\n0.0065\n0.0064\n0.0402\n22.3\n‚úì\n\n\nh23_shape\n0.06\n0.0716\n0.0076\n0.0521\n0.0911\n19.3\n‚úì\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†35: Cumulative Incidence\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†36: State Prevalence"
  },
  {
    "objectID": "03_long_tests.html#phase-type-hazard-models",
    "href": "03_long_tests.html#phase-type-hazard-models",
    "title": "Long Tests Status",
    "section": "5. Phase-Type Hazard Models",
    "text": "5. Phase-Type Hazard Models\nPhase-type hazard models use Coxian phase-type distributions to model sojourn times. Unlike standard parametric hazards (exponential, Weibull, Gompertz), phase-type hazards operate on an expanded state space where each observed state has multiple latent phases with exponential hazards.\nThese tests validate inference for models where the target model uses phase-type hazards. This is different from using phase-type as a proposal distribution for MCEM.\n\n\n\n\n\n\nNote\n\n\n\nNote on plots: Prevalence and cumulative incidence plots for phase-type tests show True (from simulation with true parameters) and Estimated (from simulation with fitted parameters) curves on the observed state space, not the expanded phase space. The capture_phasetype_longtest_result! function collapses phase-level prevalence to observed state prevalence using the phase_to_state mapping.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nPanel Data Limitation: For panel-observed data (pt_panel_* tests), the ‚ÄúObserved‚Äù cumulative incidence curves may differ substantially from ‚ÄúTrue‚Äù model curves. This is expected because:\n\nPanel data only records states at discrete observation times\nBetween observations, the exact transition path is unknown\nThe ‚ÄúObserved‚Äù CI counts direct transitions in the observed data, while ‚ÄúTrue‚Äù CI comes from continuous-time simulated paths\n\nFor example, in illness-death models, a subject going 1‚Üí2‚Üí3 between observations appears as a 1‚Üí3 transition if state 2 was never observed. This is a fundamental limitation of panel-observed data, not a model error.\n\n\n\nTest Overview\n\n\n\n\n\n\n\n\n\n\nTest\nModel Structure\nData Type\nPhases\nStatus\n\n\n\n\npt_exact_nocov\nIllness-death (1‚Üí2‚Üí3, 1‚Üí3)\nExact\n2 per state\n‚úÖ Pass\n\n\npt_exact_fixed\n2-state + covariate\nExact\n2 phases\n‚úÖ Pass\n\n\npt_exact_tvc\n2-state + TVC\nExact\n2 phases\n‚úÖ Pass\n\n\npt_panel_simple\n2-state (1‚Üí2)\nPanel\n2 phases\n‚úÖ Pass\n\n\npt_panel_id\nIllness-death\nPanel\n2 per state\n‚úÖ Pass\n\n\npt_mixed_simple\n2-state\nMixed (exact+panel)\n2 phases\n‚úÖ Pass\n\n\npt_mixed_structured\n2-state\nMixed (structured)\n2 phases\n‚úÖ Pass\n\n\npt_panel_fixed\n2-state + covariate\nPanel\n2 phases\n‚úÖ Pass\n\n\npt_panel_tvc\n2-state + TVC\nPanel\n2 phases\n‚úÖ Pass\n\n\n\n\n\n5.1 pt_exact_nocov\nTests a 3-state illness-death model (1‚Üí2‚Üí3 and 1‚Üí3) with 2 Coxian phases per transient state, using exactly observed transition times. Validates that phase-type rate parameters are recovered when all transition times are known. This is the simplest case with 8 rate parameters.\nTest pt_exact_nocov not yet run. Execute longtest_parametric_suite.jl to populate.\n\n\n5.2 pt_exact_fixed\nTests a 2-state model (1‚Üí2) with 2 phases and a time-fixed binary covariate affecting transition rates. Validates that both baseline rates and covariate effects (log hazard ratios) are recovered from exact data.\nTest pt_exact_fixed not yet run. Execute longtest_parametric_suite.jl to populate.\n\n\n5.3 pt_exact_tvc\nTests a 2-state model with 2 phases and a time-varying covariate that changes at a fixed time point. Validates inference for phase-type models when covariate values change during follow-up.\nTest pt_exact_tvc not yet run. Execute longtest_parametric_suite.jl to populate.\n\n\n5.4 pt_panel_simple\nTests a simple 2-state model (1‚Üí2 absorbing) with 2 phases using panel/interval-censored data where states are observed only at discrete times. Validates that panel data likelihood works correctly for phase-type models.\nTest pt_panel_simple not yet run. Execute longtest_parametric_suite.jl to populate.\n\n\n5.5 pt_panel_id\nTests a 3-state illness-death model with 2 phases per transient state using panel data. This is the most complex panel test with 8 rate parameters estimated from interval-censored observations.\nTest pt_panel_id not yet run. Execute longtest_parametric_suite.jl to populate.\n\n\n5.6 pt_mixed_simple\nTests a 2-state model where some subjects have exact observations and others have panel observations. Validates that the likelihood correctly combines both observation types.\nTest pt_mixed_simple not yet run. Execute longtest_parametric_suite.jl to populate.\n\n\n5.7 pt_mixed_structured\nTests a 2-state model with structured mixed observations where the first half of subjects are observed exactly and the second half have panel data. Validates inference with heterogeneous observation patterns.\nTest pt_mixed_structured not yet run. Execute longtest_parametric_suite.jl to populate.\n\n\n5.8 pt_panel_fixed\nTests phase-type with fixed covariates on panel data using the production API with homogeneous covariate constraints. With appropriate identifiability constraints (eigenvalue ordering, homogeneous covariates, SCTP), the identifiable quantities (Œ≤, Œº‚ÇÇ, ŒΩ‚ÇÅ=Œª+Œº‚ÇÅ) are recovered accurately. Note that individual Œª and Œº‚ÇÅ are NOT identifiable from panel data‚Äîonly their sum ŒΩ‚ÇÅ is identified.\n\n\n\n\n\n\nNote\n\n\n\nIdentifiability Note: Individual rates Œª (progression) and Œº‚ÇÅ (exit from phase 1) are NOT separately identifiable from panel data‚Äîonly their sum ŒΩ‚ÇÅ = Œª + Œº‚ÇÅ is identified. Standard errors are computed via the delta method for the identifiable parameterization.\n\n\nTest pt_panel_fixed not yet run. Execute longtest_parametric_suite.jl to populate.\n\n\n5.9 pt_panel_tvc\nTests phase-type with time-varying covariates on panel data using the production API with homogeneous covariate constraints. The TVC changes at a fixed time point during follow-up. With appropriate identifiability constraints, the model recovers the identifiable quantities including the treatment effect Œ≤. Higher variance is expected compared to fixed covariate models due to the additional complexity of time-varying effects.\n\n\n\n\n\n\nNote\n\n\n\nIdentifiability Note: Same identifiability constraints as pt_panel_fixed apply. The TVC effect Œ≤ is shared across phases (homogeneous constraint) for identifiability. Higher variance is expected compared to fixed covariate models.\n\n\nTest pt_panel_tvc not yet run. Execute longtest_parametric_suite.jl to populate."
  },
  {
    "objectID": "03_long_tests.html#aft-models",
    "href": "03_long_tests.html#aft-models",
    "title": "Long Tests Status",
    "section": "6. AFT Models",
    "text": "6. AFT Models\nTests in longtest_aft_suite.jl validate accelerated failure time models across exponential, Weibull, and Gompertz hazard families.\n\n6.1 Exponential AFT\n\n\n\nTest\nCovariates\nData Type\nStatus\n\n\n\n\nexp_aft_exact_nocov\nNone\nExact\n‚úÖ Pass\n\n\nexp_aft_exact_tfc\nFixed\nExact\n‚úÖ Pass\n\n\nexp_aft_exact_tvc\nTime-varying\nExact\n‚úÖ Pass\n\n\nexp_aft_panel_nocov\nNone\nPanel\n‚úÖ Pass\n\n\nexp_aft_panel_tfc\nFixed\nPanel\n‚úÖ Pass\n\n\nexp_aft_panel_tvc\nTime-varying\nPanel\n‚úÖ Pass\n\n\n\n\n\n6.2 Weibull AFT\n\n\n\nTest\nCovariates\nData Type\nStatus\n\n\n\n\nwei_aft_exact_nocov\nNone\nExact\n‚úÖ Pass\n\n\nwei_aft_exact_tfc\nFixed\nExact\n‚úÖ Pass\n\n\nwei_aft_exact_tvc\nTime-varying\nExact\n‚úÖ Pass\n\n\nwei_aft_panel_nocov\nNone\nPanel\n‚úÖ Pass\n\n\nwei_aft_panel_tfc\nFixed\nPanel\n‚úÖ Pass\n\n\nwei_aft_panel_tvc\nTime-varying\nPanel\n‚úÖ Pass\n\n\n\n\n\n6.3 Gompertz AFT\n\n\n\nTest\nCovariates\nData Type\nStatus\n\n\n\n\ngom_aft_exact_nocov\nNone\nExact\n‚úÖ Pass\n\n\ngom_aft_exact_tfc\nFixed\nExact\n‚úÖ Pass\n\n\ngom_aft_exact_tvc\nTime-varying\nExact\n‚úÖ Pass\n\n\ngom_aft_panel_nocov\nNone\nPanel\n‚úÖ Pass\n\n\ngom_aft_panel_tfc\nFixed\nPanel\n‚úÖ Pass\n\n\ngom_aft_panel_tvc\nTime-varying\nPanel\n‚úÖ Pass\n\n\n\n\n\n\n\n\n\nNoteAFT Panel Models Use MCEM with PhaseType Proposal\n\n\n\nAFT panel models are fitted via MCEM. The PhaseType proposal must scale rates by exp(-Œ≤'x) (not exp(Œ≤'x) as in PH models). This was fixed in v0.3.0+."
  },
  {
    "objectID": "03_long_tests.html#unpenalized-spline-suite",
    "href": "03_long_tests.html#unpenalized-spline-suite",
    "title": "Long Tests Status",
    "section": "7. Unpenalized Spline Suite",
    "text": "7. Unpenalized Spline Suite\nTests in longtest_spline_suite.jl validate round-trip spline hazard recovery using a spline-based data generating process. Data is simulated from a spline model with known coefficients, then a spline model is fitted and the estimated hazard curve is compared to the true hazard curve at multiple time points.\n\n\n\n\n\n\nNoteValidation Strategy\n\n\n\nUnlike parametric tests that compare parameter values directly, spline tests compare hazard curves since B-spline basis coefficients are not directly comparable between models with different data. Validation checks that pointwise hazard h(t) and covariate effects Œ≤ are recovered within tolerance.\n\n\n\n7.1 Test Matrix\n\n\n\n\n\n\n\n\n\nCategory\nPH Tests\nAFT Tests\nStatus\n\n\n\n\nExact, No Covariate\nsp_ph_exact_nocov\nsp_aft_exact_nocov\n‚úÖ Pass\n\n\nExact, Fixed Covariate\nsp_ph_exact_fixed\nsp_aft_exact_fixed\n‚úÖ Pass\n\n\nExact, Time-varying\nsp_ph_exact_tvc\nsp_aft_exact_tvc\n‚úÖ Pass\n\n\nPanel (Markov), No Covariate\nsp_ph_panel_markov_nocov\nsp_aft_panel_markov_nocov\n‚è≥ Pending\n\n\nPanel (Markov), Fixed\nsp_ph_panel_markov_fixed\nsp_aft_panel_markov_fixed\n‚è≥ Pending\n\n\nPanel (Markov), TVC\nsp_ph_panel_markov_tvc\nsp_aft_panel_markov_tvc\n‚è≥ Pending\n\n\nPanel (PhaseType), No Covariate\nsp_ph_panel_phasetype_nocov\nsp_aft_panel_phasetype_nocov\n‚è≥ Pending\n\n\nPanel (PhaseType), Fixed\nsp_ph_panel_phasetype_fixed\nsp_aft_panel_phasetype_fixed\n‚è≥ Pending\n\n\nPanel (PhaseType), TVC\nsp_ph_panel_phasetype_tvc\nsp_aft_panel_phasetype_tvc\n‚è≥ Pending\n\n\n\n\n\n7.2 Exact Data Results\nResults for exact data spline tests (sp_ph_exact_, sp_aft_exact_):\n\nsp_ph_exact_nocov: Not available\nsp_ph_exact_fixed: Not available\nsp_ph_exact_tvc: Not available\nsp_aft_exact_nocov: Not available\nsp_aft_exact_fixed: Not available\nsp_aft_exact_tvc: Not available"
  },
  {
    "objectID": "03_long_tests.html#penalized-spline-models",
    "href": "03_long_tests.html#penalized-spline-models",
    "title": "Long Tests Status",
    "section": "8. Penalized Spline Models",
    "text": "8. Penalized Spline Models\n\n8.1 Smooth Covariate Effects (s(x))\n\n\n\nTest\nTrue Function\nStatus\n\n\n\n\nSinusoidal\nf(x) = sin(2œÄx)\n‚úÖ Pass\n\n\nQuadratic\nf(x) = x¬≤\n‚úÖ Pass\n\n\nSigmoid\nf(x) = tanh(5(x-0.5))\n‚úÖ Pass\n\n\ns(x) + Linear\nf(x) = sin(2œÄx) + 0.5¬∑trt\n‚úÖ Pass\n\n\n\n\n\n8.2 Tensor Product Smooths (te(x,y))\n\n\n\nTest\nTrue Surface\nStatus\n\n\n\n\nSeparable\ng(x,y) = sin(œÄx)cos(œÄy)\n‚úÖ Pass\n\n\nBilinear\ng(x,y) = (x-0.5)(y-0.5)\n‚úÖ Pass\n\n\nAdditive\ng(x,y) = sin(2œÄx) + 0.5cos(2œÄy)\n‚úÖ Pass\n\n\n\n\n\n8.3 Spline Hazards with Exact Data\nTests in longtest_spline_exact.jl validate spline hazard recovery with exact observation data (obstype=1) using a Weibull DGP.\n\n\n\nTest\nEffect\nCovariates\nStatus\n\n\n\n\nsp_exact_nocov\nPH\nNone\n‚úÖ Pass\n\n\nsp_exact_tfc\nPH\nFixed\n‚úÖ Pass\n\n\nsp_exact_tvc\nPH\nTime-varying\n‚úÖ Pass\n\n\nsp_aft_exact_nocov\nAFT\nNone\n‚úÖ Pass\n\n\nsp_aft_exact_tfc\nAFT\nFixed\n‚úÖ Pass\n\n\nsp_aft_exact_tvc\nAFT\nTime-varying\n‚úÖ Pass\n\n\n\n\n\n8.4 MCEM with Spline Hazards\n\n\n\nTest\nApproximates\nStatus\n\n\n\n\nLinear spline\nExponential (constant) hazard\n‚úÖ Pass\n\n\nPiecewise spline\nStep function hazard\n‚úÖ Pass\n\n\nCubic spline\nGompertz (exponential increase)\n‚úÖ Pass"
  },
  {
    "objectID": "03_long_tests.html#notes",
    "href": "03_long_tests.html#notes",
    "title": "Long Tests Status",
    "section": "9. Notes",
    "text": "9. Notes\n\nModel: 3-state progressive: State 1 ‚Üí State 2 ‚Üí State 3\nTolerance: 35% relative error for main parameters\nTVC: Time-varying covariate effects change at t=5\nExact data: obstype=1 (continuous observation)\nPanel data: Discrete observation times with MCEM"
  },
  {
    "objectID": "05_benchmarks.html",
    "href": "05_benchmarks.html",
    "title": "Benchmarks",
    "section": "",
    "text": "NoteStatus: Not Started\n\n\n\nThis report is a placeholder. Content will be added as the benchmark infrastructure is developed."
  },
  {
    "objectID": "05_benchmarks.html#sampling-methods-comparison",
    "href": "05_benchmarks.html#sampling-methods-comparison",
    "title": "Benchmarks",
    "section": "1. Sampling Methods Comparison",
    "text": "1. Sampling Methods Comparison\n\nOverview\nThis section compares the performance of different sampling methods used in the MCEM algorithm:\n\nSIR: Sampling Importance Resampling\nLHS: Latin Hypercube Sampling\nIS: Importance Sampling\n\n\n\nBenchmark Setup\n\n\nShow code\n# TODO: Add benchmark setup code\nusing BenchmarkTools\n\n# Model configuration\n# ...\n\n\n\n\nResults\n\n\n\nMethod\nESS\nRuntime (s)\nESS/second\n\n\n\n\nSIR\n-\n-\n-\n\n\nLHS\n-\n-\n-\n\n\nIS\n-\n-\n-\n\n\n\n\n\nEffective Sample Size Comparison\nPlot to be generated: ESS vs computation time for each method"
  },
  {
    "objectID": "05_benchmarks.html#mcem-convergence",
    "href": "05_benchmarks.html#mcem-convergence",
    "title": "Benchmarks",
    "section": "2. MCEM Convergence",
    "text": "2. MCEM Convergence\n\nOverview\nThis section benchmarks MCEM convergence with adaptive ESS and SIR resampling.\n\n\nBenchmark Setup\n\n\nShow code\n# TODO: Add MCEM benchmark code\n\n\n\n\nConvergence Comparison\n\n\n\nMethod\nIterations to Convergence\nTotal Runtime (s)\n\n\n\n\nStandard IS\n-\n-\n\n\nAdaptive SIR\n-\n-\n\n\n\n\n\nConvergence Plots\nPlots to be generated: Log-likelihood vs iteration for each method"
  },
  {
    "objectID": "05_benchmarks.html#scalability",
    "href": "05_benchmarks.html#scalability",
    "title": "Benchmarks",
    "section": "3. Scalability",
    "text": "3. Scalability\n\n3.1 Runtime vs Number of Subjects\nPlot to be generated\n\n\n3.2 Runtime vs Number of States\nPlot to be generated\n\n\n3.3 Runtime vs Number of Transitions\nPlot to be generated"
  },
  {
    "objectID": "05_benchmarks.html#memory-usage",
    "href": "05_benchmarks.html#memory-usage",
    "title": "Benchmarks",
    "section": "4. Memory Usage",
    "text": "4. Memory Usage\n\nPeak Memory by Problem Size\n\n\n\nSubjects\nStates\nTransitions\nPeak Memory (GB)\n\n\n\n\n100\n3\n2\n-\n\n\n500\n3\n2\n-\n\n\n1000\n3\n2\n-\n\n\n1000\n5\n6\n-\n\n\n\nAdditional memory profiling to be added"
  },
  {
    "objectID": "04_simulation_diagnostics.html",
    "href": "04_simulation_diagnostics.html",
    "title": "Simulation Diagnostics",
    "section": "",
    "text": "This report validates that the MultistateModels.jl simulation engine produces event times that match the theoretical distributions defined by the hazard functions.\n\n\nFor each scenario, we:\n\nDefine a hazard model with known parameters\nSimulate many event times using simulate_path()\nCompare the empirical CDF (ECDF) to the theoretical CDF\nVerify time-transform parity - ensure that both the cached time-transform strategy and the direct fallback produce identical results\n\n\n\n\n\nECDF vs CDF: Visual comparison of simulated vs theoretical distributions\nKS statistic: Kolmogorov-Smirnov statistic \\(D_n = \\sup_t |F_n(t) - F(t)|\\), which should decrease as \\(\\sim 1/\\sqrt{n}\\)\nTime-transform parity: \\(\\Delta F(t) = F_{tt}(t) - F_{fb}(t)\\) should be zero\n\n\n\nConfiguration constants\nconst COVARIATE_VALUE = 1.5\nconst DELTA_U = sqrt(eps())\nconst DELTA_T = sqrt(eps())\nconst SIM_SAMPLES = 40_000\nconst DIST_GRID_POINTS = 400\n\nconst FAMILY_CONFIG = Dict(\n    \"exp\" =&gt; (; rate = 0.35, beta = 0.6, horizon = 5.0, hazard_start = 0.0),\n    \"wei\" =&gt; (; shape = 1.35, scale = 0.4, beta = -0.35, horizon = 5.0, hazard_start = 0.02),\n    \"gom\" =&gt; (; shape = 0.6, rate = 0.4, beta = 0.5, horizon = 5.0, hazard_start = 0.0),\n)\n\nconst TVC_CONFIG = Dict(\n    \"exp\" =&gt; (; rate = 0.35, beta = 0.6, horizon = 5.0, hazard_start = 0.0, \n               t_changes = [1.5, 3.0], x_values = [0.5, 1.5, 2.5]),\n    \"wei\" =&gt; (; rate = 0.0, shape = 1.35, scale = 0.4, beta = -0.35, horizon = 5.0, \n               hazard_start = 0.02, t_changes = [1.5, 3.0], x_values = [0.5, 1.5, 2.5]),\n    \"gom\" =&gt; (; rate = 0.4, shape = 0.6, beta = 0.5, horizon = 5.0, hazard_start = 0.0, \n               t_changes = [1.5, 3.0], x_values = [0.5, 1.5, 2.5]),\n)\n\n\nDict{String, NamedTuple} with 3 entries:\n  \"exp\" =&gt; (rate = 0.35, beta = 0.6, horizon = 5.0, hazard_start = 0.0, t_chang‚Ä¶\n  \"wei\" =&gt; (rate = 0.0, shape = 1.35, scale = 0.4, beta = -0.35, horizon = 5.0,‚Ä¶\n  \"gom\" =&gt; (rate = 0.4, shape = 0.6, beta = 0.5, horizon = 5.0, hazard_start = ‚Ä¶\n\n\n\n\nHelper functions for scenario setup\nstruct Scenario\n    family::String\n    effect::Symbol\n    covariate_mode::Symbol\n    label::String\n    slug::String\n    config::NamedTuple\nend\n\nfunction Scenario(family::String, effect::Symbol, cov_mode::Symbol)\n    if cov_mode == :tvc\n        config = TVC_CONFIG[family]\n        label = string(uppercasefirst(lowercase(family)), \" \", uppercase(String(effect)), \n                      \" time-varying covariate\")\n    else\n        config = FAMILY_CONFIG[family]\n        label = string(uppercasefirst(lowercase(family)), \" \", uppercase(String(effect)), \n                      \" \", cov_mode == :covariate ? \"with covariate\" : \"baseline-only\")\n    end\n    slug = string(family, \"_\", effect, \"_\", cov_mode)\n    return Scenario(family, effect, cov_mode, label, slug, config)\nend\n\nfunction scenario_subject_df(scenario::Scenario)\n    horizon = scenario.config.horizon\n    if scenario.covariate_mode == :tvc\n        t_changes = scenario.config.t_changes\n        x_values = scenario.config.x_values\n        tstart_grid = vcat(0.0, t_changes)\n        tstop_grid = vcat(t_changes, horizon)\n        n_intervals = length(tstart_grid)\n        df = DataFrame(\n            id = fill(1, n_intervals),\n            tstart = tstart_grid,\n            tstop = tstop_grid,\n            statefrom = fill(1, n_intervals),\n            stateto = fill(2, n_intervals),\n            obstype = fill(1, n_intervals),\n            x = x_values,\n        )\n    else\n        df = DataFrame(\n            id = [1], tstart = [0.0], tstop = [horizon],\n            statefrom = [1], stateto = [2], obstype = [1],\n        )\n        if scenario.covariate_mode == :covariate\n            df.x = [COVARIATE_VALUE]\n        end\n    end\n    return df\nend\n\nfunction hazard_formula(scenario::Scenario)\n    (scenario.covariate_mode == :covariate || scenario.covariate_mode == :tvc) ? \n        @formula(0 ~ x) : @formula(0 ~ 1)\nend\n\nfunction scenario_parameter_vector(scenario::Scenario)\n    cfg = scenario.config\n    if scenario.family == \"exp\"\n        base = [log(cfg.rate)]\n    elseif scenario.family == \"wei\"\n        base = [log(cfg.shape), log(cfg.scale)]\n    elseif scenario.family == \"gom\"\n        base = [cfg.shape, log(cfg.rate)]\n    else\n        error(\"Unsupported family $(scenario.family)\")\n    end\n    return (scenario.covariate_mode == :covariate || scenario.covariate_mode == :tvc) ? \n        vcat(base, [cfg.beta]) : base\nend\n\nfunction build_model(scenario::Scenario)\n    data = scenario_subject_df(scenario)\n    hazard = Hazard(\n        hazard_formula(scenario),\n        scenario.family, 1, 2;\n        linpred_effect = scenario.effect,\n        time_transform = true,\n    )\n    model = multistatemodel(hazard; data = data)\n    pars = scenario_parameter_vector(scenario)\n    hazname = model.hazards[1].hazname\n    set_parameters!(model, NamedTuple{(hazname,)}((pars,)))\n    return model, data\nend\n\ncovariate_value(scenario::Scenario) = scenario.covariate_mode == :covariate ? COVARIATE_VALUE : 0.0\n\n\ncovariate_value (generic function with 1 method)\n\n\n\n\nTheoretical distribution functions\n# Piecewise cumulative hazard helpers for TVC\nfunction piecewise_exp_ph_cumhaz(t, rate, beta, t_changes, x_values)\n    cumhaz = 0.0\n    prev_t = 0.0\n    for (i, tc) in enumerate(t_changes)\n        if t &lt;= tc\n            cumhaz += rate * exp(beta * x_values[i]) * (t - prev_t)\n            return cumhaz\n        else\n            cumhaz += rate * exp(beta * x_values[i]) * (tc - prev_t)\n            prev_t = tc\n        end\n    end\n    cumhaz += rate * exp(beta * x_values[end]) * (t - prev_t)\n    return cumhaz\nend\n\nfunction piecewise_wei_ph_cumhaz(t, shape, scale, beta, t_changes, x_values)\n    cumhaz = 0.0\n    prev_t = 0.0\n    for (i, tc) in enumerate(t_changes)\n        if t &lt;= tc\n            cumhaz += scale * exp(beta * x_values[i]) * (t^shape - prev_t^shape)\n            return cumhaz\n        else\n            cumhaz += scale * exp(beta * x_values[i]) * (tc^shape - prev_t^shape)\n            prev_t = tc\n        end\n    end\n    cumhaz += scale * exp(beta * x_values[end]) * (t^shape - prev_t^shape)\n    return cumhaz\nend\n\nfunction distribution_functions(scenario::Scenario)\n    cfg = scenario.config\n    \n    if scenario.covariate_mode == :tvc\n        t_changes = cfg.t_changes\n        x_values = cfg.x_values\n        beta = cfg.beta\n        \n        if scenario.family == \"exp\"\n            rate = cfg.rate\n            cumhaz = t -&gt; piecewise_exp_ph_cumhaz(t, rate, beta, t_changes, x_values)\n            hazard = t -&gt; begin\n                for (i, tc) in enumerate(t_changes)\n                    if t &lt; tc\n                        return rate * exp(beta * x_values[i])\n                    end\n                end\n                return rate * exp(beta * x_values[end])\n            end\n        elseif scenario.family == \"wei\"\n            shape = cfg.shape\n            scale = cfg.scale\n            cumhaz = t -&gt; piecewise_wei_ph_cumhaz(t, shape, scale, beta, t_changes, x_values)\n            hazard = t -&gt; begin\n                for (i, tc) in enumerate(t_changes)\n                    if t &lt; tc\n                        return shape * scale * exp(beta * x_values[i]) * (t^(shape - 1))\n                    end\n                end\n                return shape * scale * exp(beta * x_values[end]) * (t^(shape - 1))\n            end\n        else\n            error(\"TVC not implemented for family $(scenario.family)\")\n        end\n    else\n        xval = covariate_value(scenario)\n        beta = scenario.covariate_mode == :covariate ? cfg.beta : 0.0\n        \n        if scenario.family == \"exp\"\n            base_rate = cfg.rate\n            rate = scenario.effect == :ph ? base_rate * exp(beta * xval) : base_rate * exp(-beta * xval)\n            cumhaz = t -&gt; rate * t\n            hazard = _ -&gt; rate\n        elseif scenario.family == \"wei\"\n            shape = cfg.shape\n            scale = cfg.scale\n            multiplier = scenario.effect == :ph ? exp(beta * xval) : exp(-shape * beta * xval)\n            cumhaz = t -&gt; scale * multiplier * (t^shape)\n            hazard = t -&gt; shape * scale * multiplier * (t^(shape - 1))\n        elseif scenario.family == \"gom\"\n            shape = cfg.shape\n            rate = cfg.rate\n            linpred = beta * xval\n            if scenario.effect == :ph\n                cumhaz = t -&gt; (rate / shape) * exp(linpred) * (exp(shape * t) - 1)\n                hazard = t -&gt; rate * exp(shape * t + linpred)\n            else\n                time_scale = exp(-linpred)\n                scaled_shape = shape * time_scale\n                scaled_rate = rate * time_scale\n                cumhaz = t -&gt; (scaled_rate / scaled_shape) * (exp(scaled_shape * t) - 1)\n                hazard = t -&gt; scaled_rate * exp(scaled_shape * t)\n            end\n        else\n            error(\"Unsupported family $(scenario.family)\")\n        end\n    end\n    cdf = t -&gt; t &lt;= 0 ? 0.0 : 1 - exp(-cumhaz(t))\n    pdf = t -&gt; t &lt;= 0 ? 0.0 : hazard(t) * exp(-cumhaz(t))\n    return cdf, pdf\nend\n\n\ndistribution_functions (generic function with 1 method)\n\n\n\n\nSimulation and plotting functions\nfunction collect_event_durations(model, nsamples; use_cached_strategy::Bool, rng::AbstractRNG)\n    durations = Vector{Float64}(undef, nsamples)\n    collected = 0\n    attempts = 0\n    max_attempts = nsamples * 200\n    strategy = use_cached_strategy ? CachedTransformStrategy() : DirectTransformStrategy()\n    while collected &lt; nsamples\n        path = simulate_path(model, 1; strategy = strategy, rng = rng)\n        attempts += 1\n        attempts &gt; max_attempts && error(\"Exceeded maximum attempts\")\n        if path.states[end] != path.states[1]\n            collected += 1\n            durations[collected] = path.times[end] - path.times[1]\n        end\n    end\n    return durations\nend\n\nfunction run_scenario_diagnostics(scenario::Scenario)\n    model, data = build_model(scenario)\n    \n    seed = hash(scenario.slug)\n    rng_tt = Random.MersenneTwister(seed)\n    rng_fb = Random.MersenneTwister(seed)\n    \n    durations_tt = collect_event_durations(model, SIM_SAMPLES; use_cached_strategy=true, rng=rng_tt)\n    durations_fb = collect_event_durations(model, SIM_SAMPLES; use_cached_strategy=false, rng=rng_fb)\n    \n    ecdf_tt = ecdf(durations_tt)\n    ecdf_fb = ecdf(durations_fb)\n    horizon = scenario.config.horizon\n    ts = collect(range(0.0, horizon; length = DIST_GRID_POINTS))\n    \n    cdf_base, pdf_base = distribution_functions(scenario)\n    cdf_fn, pdf_fn = truncate_distribution(cdf_base, pdf_base; lower = 0.0, upper = horizon)\n    \n    expected = cdf_fn.(ts)\n    empirical = ecdf_fb.(ts)\n    diff_curve = ecdf_tt.(ts) .- ecdf_fb.(ts)\n    max_abs_diff = maximum(abs.(diff_curve))\n    \n    # KS statistic at different sample sizes\n    sorted_durations = sort(durations_fb)\n    n_samples = length(sorted_durations)\n    expected_cdf_at_samples = cdf_fn.(sorted_durations)\n    eval_ns = filter(n -&gt; n &lt;= n_samples, [100, 200, 500, 1000, 2000, 5000, 10000, 20000, n_samples])\n    ks_at_n = zeros(length(eval_ns))\n    \n    for (idx, n) in enumerate(eval_ns)\n        max_diff = 0.0\n        for i in 1:n\n            cdf_i = expected_cdf_at_samples[i]\n            diff_upper = abs(i / n - cdf_i)\n            diff_lower = abs((i - 1) / n - cdf_i)\n            max_diff = max(max_diff, diff_upper, diff_lower)\n        end\n        ks_at_n[idx] = max_diff\n    end\n    \n    # Create figure\n    fig = Figure(size = (1400, 500))\n    \n    ax1 = Axis(fig[1, 1], title = \"ECDF vs Expected CDF\", xlabel = \"Duration\", ylabel = \"F(t)\")\n    lines!(ax1, ts, expected, color = :black, linewidth = 3, label = \"Theoretical\")\n    lines!(ax1, ts, empirical, color = :dodgerblue, linewidth = 2, label = \"Simulated\")\n    axislegend(ax1, position = :rb)\n    \n    ax2 = Axis(fig[1, 2], title = \"KS Statistic vs Sample Size\", \n               xlabel = \"n\", ylabel = \"D‚Çô\", xscale = log10)\n    scatterlines!(ax2, eval_ns, ks_at_n, color = :crimson, linewidth = 2, markersize = 8)\n    \n    ylim_span = max(max_abs_diff, 1e-6)\n    ax3 = Axis(fig[1, 3], title = \"Time-Transform Parity: max|ŒîF| = $(round(max_abs_diff; digits=4))\", \n               xlabel = \"Duration\", ylabel = \"ŒîF(t)\")\n    lines!(ax3, ts, diff_curve, color = :seagreen, linewidth = 2)\n    hlines!(ax3, [0.0], color = :black, linestyle = :dash)\n    ylims!(ax3, (-1.1 * ylim_span, 1.1 * ylim_span))\n    \n    return fig, max_abs_diff\nend\n\n\nrun_scenario_diagnostics (generic function with 1 method)"
  },
  {
    "objectID": "04_simulation_diagnostics.html#methodology",
    "href": "04_simulation_diagnostics.html#methodology",
    "title": "Simulation Diagnostics",
    "section": "",
    "text": "This report validates that the MultistateModels.jl simulation engine produces event times that match the theoretical distributions defined by the hazard functions.\n\n\nFor each scenario, we:\n\nDefine a hazard model with known parameters\nSimulate many event times using simulate_path()\nCompare the empirical CDF (ECDF) to the theoretical CDF\nVerify time-transform parity - ensure that both the cached time-transform strategy and the direct fallback produce identical results\n\n\n\n\n\nECDF vs CDF: Visual comparison of simulated vs theoretical distributions\nKS statistic: Kolmogorov-Smirnov statistic \\(D_n = \\sup_t |F_n(t) - F(t)|\\), which should decrease as \\(\\sim 1/\\sqrt{n}\\)\nTime-transform parity: \\(\\Delta F(t) = F_{tt}(t) - F_{fb}(t)\\) should be zero\n\n\n\nConfiguration constants\nconst COVARIATE_VALUE = 1.5\nconst DELTA_U = sqrt(eps())\nconst DELTA_T = sqrt(eps())\nconst SIM_SAMPLES = 40_000\nconst DIST_GRID_POINTS = 400\n\nconst FAMILY_CONFIG = Dict(\n    \"exp\" =&gt; (; rate = 0.35, beta = 0.6, horizon = 5.0, hazard_start = 0.0),\n    \"wei\" =&gt; (; shape = 1.35, scale = 0.4, beta = -0.35, horizon = 5.0, hazard_start = 0.02),\n    \"gom\" =&gt; (; shape = 0.6, rate = 0.4, beta = 0.5, horizon = 5.0, hazard_start = 0.0),\n)\n\nconst TVC_CONFIG = Dict(\n    \"exp\" =&gt; (; rate = 0.35, beta = 0.6, horizon = 5.0, hazard_start = 0.0, \n               t_changes = [1.5, 3.0], x_values = [0.5, 1.5, 2.5]),\n    \"wei\" =&gt; (; rate = 0.0, shape = 1.35, scale = 0.4, beta = -0.35, horizon = 5.0, \n               hazard_start = 0.02, t_changes = [1.5, 3.0], x_values = [0.5, 1.5, 2.5]),\n    \"gom\" =&gt; (; rate = 0.4, shape = 0.6, beta = 0.5, horizon = 5.0, hazard_start = 0.0, \n               t_changes = [1.5, 3.0], x_values = [0.5, 1.5, 2.5]),\n)\n\n\nDict{String, NamedTuple} with 3 entries:\n  \"exp\" =&gt; (rate = 0.35, beta = 0.6, horizon = 5.0, hazard_start = 0.0, t_chang‚Ä¶\n  \"wei\" =&gt; (rate = 0.0, shape = 1.35, scale = 0.4, beta = -0.35, horizon = 5.0,‚Ä¶\n  \"gom\" =&gt; (rate = 0.4, shape = 0.6, beta = 0.5, horizon = 5.0, hazard_start = ‚Ä¶\n\n\n\n\nHelper functions for scenario setup\nstruct Scenario\n    family::String\n    effect::Symbol\n    covariate_mode::Symbol\n    label::String\n    slug::String\n    config::NamedTuple\nend\n\nfunction Scenario(family::String, effect::Symbol, cov_mode::Symbol)\n    if cov_mode == :tvc\n        config = TVC_CONFIG[family]\n        label = string(uppercasefirst(lowercase(family)), \" \", uppercase(String(effect)), \n                      \" time-varying covariate\")\n    else\n        config = FAMILY_CONFIG[family]\n        label = string(uppercasefirst(lowercase(family)), \" \", uppercase(String(effect)), \n                      \" \", cov_mode == :covariate ? \"with covariate\" : \"baseline-only\")\n    end\n    slug = string(family, \"_\", effect, \"_\", cov_mode)\n    return Scenario(family, effect, cov_mode, label, slug, config)\nend\n\nfunction scenario_subject_df(scenario::Scenario)\n    horizon = scenario.config.horizon\n    if scenario.covariate_mode == :tvc\n        t_changes = scenario.config.t_changes\n        x_values = scenario.config.x_values\n        tstart_grid = vcat(0.0, t_changes)\n        tstop_grid = vcat(t_changes, horizon)\n        n_intervals = length(tstart_grid)\n        df = DataFrame(\n            id = fill(1, n_intervals),\n            tstart = tstart_grid,\n            tstop = tstop_grid,\n            statefrom = fill(1, n_intervals),\n            stateto = fill(2, n_intervals),\n            obstype = fill(1, n_intervals),\n            x = x_values,\n        )\n    else\n        df = DataFrame(\n            id = [1], tstart = [0.0], tstop = [horizon],\n            statefrom = [1], stateto = [2], obstype = [1],\n        )\n        if scenario.covariate_mode == :covariate\n            df.x = [COVARIATE_VALUE]\n        end\n    end\n    return df\nend\n\nfunction hazard_formula(scenario::Scenario)\n    (scenario.covariate_mode == :covariate || scenario.covariate_mode == :tvc) ? \n        @formula(0 ~ x) : @formula(0 ~ 1)\nend\n\nfunction scenario_parameter_vector(scenario::Scenario)\n    cfg = scenario.config\n    if scenario.family == \"exp\"\n        base = [log(cfg.rate)]\n    elseif scenario.family == \"wei\"\n        base = [log(cfg.shape), log(cfg.scale)]\n    elseif scenario.family == \"gom\"\n        base = [cfg.shape, log(cfg.rate)]\n    else\n        error(\"Unsupported family $(scenario.family)\")\n    end\n    return (scenario.covariate_mode == :covariate || scenario.covariate_mode == :tvc) ? \n        vcat(base, [cfg.beta]) : base\nend\n\nfunction build_model(scenario::Scenario)\n    data = scenario_subject_df(scenario)\n    hazard = Hazard(\n        hazard_formula(scenario),\n        scenario.family, 1, 2;\n        linpred_effect = scenario.effect,\n        time_transform = true,\n    )\n    model = multistatemodel(hazard; data = data)\n    pars = scenario_parameter_vector(scenario)\n    hazname = model.hazards[1].hazname\n    set_parameters!(model, NamedTuple{(hazname,)}((pars,)))\n    return model, data\nend\n\ncovariate_value(scenario::Scenario) = scenario.covariate_mode == :covariate ? COVARIATE_VALUE : 0.0\n\n\ncovariate_value (generic function with 1 method)\n\n\n\n\nTheoretical distribution functions\n# Piecewise cumulative hazard helpers for TVC\nfunction piecewise_exp_ph_cumhaz(t, rate, beta, t_changes, x_values)\n    cumhaz = 0.0\n    prev_t = 0.0\n    for (i, tc) in enumerate(t_changes)\n        if t &lt;= tc\n            cumhaz += rate * exp(beta * x_values[i]) * (t - prev_t)\n            return cumhaz\n        else\n            cumhaz += rate * exp(beta * x_values[i]) * (tc - prev_t)\n            prev_t = tc\n        end\n    end\n    cumhaz += rate * exp(beta * x_values[end]) * (t - prev_t)\n    return cumhaz\nend\n\nfunction piecewise_wei_ph_cumhaz(t, shape, scale, beta, t_changes, x_values)\n    cumhaz = 0.0\n    prev_t = 0.0\n    for (i, tc) in enumerate(t_changes)\n        if t &lt;= tc\n            cumhaz += scale * exp(beta * x_values[i]) * (t^shape - prev_t^shape)\n            return cumhaz\n        else\n            cumhaz += scale * exp(beta * x_values[i]) * (tc^shape - prev_t^shape)\n            prev_t = tc\n        end\n    end\n    cumhaz += scale * exp(beta * x_values[end]) * (t^shape - prev_t^shape)\n    return cumhaz\nend\n\nfunction distribution_functions(scenario::Scenario)\n    cfg = scenario.config\n    \n    if scenario.covariate_mode == :tvc\n        t_changes = cfg.t_changes\n        x_values = cfg.x_values\n        beta = cfg.beta\n        \n        if scenario.family == \"exp\"\n            rate = cfg.rate\n            cumhaz = t -&gt; piecewise_exp_ph_cumhaz(t, rate, beta, t_changes, x_values)\n            hazard = t -&gt; begin\n                for (i, tc) in enumerate(t_changes)\n                    if t &lt; tc\n                        return rate * exp(beta * x_values[i])\n                    end\n                end\n                return rate * exp(beta * x_values[end])\n            end\n        elseif scenario.family == \"wei\"\n            shape = cfg.shape\n            scale = cfg.scale\n            cumhaz = t -&gt; piecewise_wei_ph_cumhaz(t, shape, scale, beta, t_changes, x_values)\n            hazard = t -&gt; begin\n                for (i, tc) in enumerate(t_changes)\n                    if t &lt; tc\n                        return shape * scale * exp(beta * x_values[i]) * (t^(shape - 1))\n                    end\n                end\n                return shape * scale * exp(beta * x_values[end]) * (t^(shape - 1))\n            end\n        else\n            error(\"TVC not implemented for family $(scenario.family)\")\n        end\n    else\n        xval = covariate_value(scenario)\n        beta = scenario.covariate_mode == :covariate ? cfg.beta : 0.0\n        \n        if scenario.family == \"exp\"\n            base_rate = cfg.rate\n            rate = scenario.effect == :ph ? base_rate * exp(beta * xval) : base_rate * exp(-beta * xval)\n            cumhaz = t -&gt; rate * t\n            hazard = _ -&gt; rate\n        elseif scenario.family == \"wei\"\n            shape = cfg.shape\n            scale = cfg.scale\n            multiplier = scenario.effect == :ph ? exp(beta * xval) : exp(-shape * beta * xval)\n            cumhaz = t -&gt; scale * multiplier * (t^shape)\n            hazard = t -&gt; shape * scale * multiplier * (t^(shape - 1))\n        elseif scenario.family == \"gom\"\n            shape = cfg.shape\n            rate = cfg.rate\n            linpred = beta * xval\n            if scenario.effect == :ph\n                cumhaz = t -&gt; (rate / shape) * exp(linpred) * (exp(shape * t) - 1)\n                hazard = t -&gt; rate * exp(shape * t + linpred)\n            else\n                time_scale = exp(-linpred)\n                scaled_shape = shape * time_scale\n                scaled_rate = rate * time_scale\n                cumhaz = t -&gt; (scaled_rate / scaled_shape) * (exp(scaled_shape * t) - 1)\n                hazard = t -&gt; scaled_rate * exp(scaled_shape * t)\n            end\n        else\n            error(\"Unsupported family $(scenario.family)\")\n        end\n    end\n    cdf = t -&gt; t &lt;= 0 ? 0.0 : 1 - exp(-cumhaz(t))\n    pdf = t -&gt; t &lt;= 0 ? 0.0 : hazard(t) * exp(-cumhaz(t))\n    return cdf, pdf\nend\n\n\ndistribution_functions (generic function with 1 method)\n\n\n\n\nSimulation and plotting functions\nfunction collect_event_durations(model, nsamples; use_cached_strategy::Bool, rng::AbstractRNG)\n    durations = Vector{Float64}(undef, nsamples)\n    collected = 0\n    attempts = 0\n    max_attempts = nsamples * 200\n    strategy = use_cached_strategy ? CachedTransformStrategy() : DirectTransformStrategy()\n    while collected &lt; nsamples\n        path = simulate_path(model, 1; strategy = strategy, rng = rng)\n        attempts += 1\n        attempts &gt; max_attempts && error(\"Exceeded maximum attempts\")\n        if path.states[end] != path.states[1]\n            collected += 1\n            durations[collected] = path.times[end] - path.times[1]\n        end\n    end\n    return durations\nend\n\nfunction run_scenario_diagnostics(scenario::Scenario)\n    model, data = build_model(scenario)\n    \n    seed = hash(scenario.slug)\n    rng_tt = Random.MersenneTwister(seed)\n    rng_fb = Random.MersenneTwister(seed)\n    \n    durations_tt = collect_event_durations(model, SIM_SAMPLES; use_cached_strategy=true, rng=rng_tt)\n    durations_fb = collect_event_durations(model, SIM_SAMPLES; use_cached_strategy=false, rng=rng_fb)\n    \n    ecdf_tt = ecdf(durations_tt)\n    ecdf_fb = ecdf(durations_fb)\n    horizon = scenario.config.horizon\n    ts = collect(range(0.0, horizon; length = DIST_GRID_POINTS))\n    \n    cdf_base, pdf_base = distribution_functions(scenario)\n    cdf_fn, pdf_fn = truncate_distribution(cdf_base, pdf_base; lower = 0.0, upper = horizon)\n    \n    expected = cdf_fn.(ts)\n    empirical = ecdf_fb.(ts)\n    diff_curve = ecdf_tt.(ts) .- ecdf_fb.(ts)\n    max_abs_diff = maximum(abs.(diff_curve))\n    \n    # KS statistic at different sample sizes\n    sorted_durations = sort(durations_fb)\n    n_samples = length(sorted_durations)\n    expected_cdf_at_samples = cdf_fn.(sorted_durations)\n    eval_ns = filter(n -&gt; n &lt;= n_samples, [100, 200, 500, 1000, 2000, 5000, 10000, 20000, n_samples])\n    ks_at_n = zeros(length(eval_ns))\n    \n    for (idx, n) in enumerate(eval_ns)\n        max_diff = 0.0\n        for i in 1:n\n            cdf_i = expected_cdf_at_samples[i]\n            diff_upper = abs(i / n - cdf_i)\n            diff_lower = abs((i - 1) / n - cdf_i)\n            max_diff = max(max_diff, diff_upper, diff_lower)\n        end\n        ks_at_n[idx] = max_diff\n    end\n    \n    # Create figure\n    fig = Figure(size = (1400, 500))\n    \n    ax1 = Axis(fig[1, 1], title = \"ECDF vs Expected CDF\", xlabel = \"Duration\", ylabel = \"F(t)\")\n    lines!(ax1, ts, expected, color = :black, linewidth = 3, label = \"Theoretical\")\n    lines!(ax1, ts, empirical, color = :dodgerblue, linewidth = 2, label = \"Simulated\")\n    axislegend(ax1, position = :rb)\n    \n    ax2 = Axis(fig[1, 2], title = \"KS Statistic vs Sample Size\", \n               xlabel = \"n\", ylabel = \"D‚Çô\", xscale = log10)\n    scatterlines!(ax2, eval_ns, ks_at_n, color = :crimson, linewidth = 2, markersize = 8)\n    \n    ylim_span = max(max_abs_diff, 1e-6)\n    ax3 = Axis(fig[1, 3], title = \"Time-Transform Parity: max|ŒîF| = $(round(max_abs_diff; digits=4))\", \n               xlabel = \"Duration\", ylabel = \"ŒîF(t)\")\n    lines!(ax3, ts, diff_curve, color = :seagreen, linewidth = 2)\n    hlines!(ax3, [0.0], color = :black, linestyle = :dash)\n    ylims!(ax3, (-1.1 * ylim_span, 1.1 * ylim_span))\n    \n    return fig, max_abs_diff\nend\n\n\nrun_scenario_diagnostics (generic function with 1 method)"
  },
  {
    "objectID": "04_simulation_diagnostics.html#parametric-families",
    "href": "04_simulation_diagnostics.html#parametric-families",
    "title": "Simulation Diagnostics",
    "section": "2. Parametric Families",
    "text": "2. Parametric Families\n\n2.1 Exponential\nThe exponential hazard: \\(h(t) = \\text{rate}\\)\nParameterization:\n\n\n\n\n\n\n\n\nEffect\nHazard\nCumulative Hazard\n\n\n\n\nPH\n\\(h(t\\|x) = \\text{rate} \\cdot e^{\\beta x}\\)\n\\(H(t\\|x) = \\text{rate} \\cdot e^{\\beta x} \\cdot t\\)\n\n\nAFT\n\\(h(t\\|x) = \\text{rate} \\cdot e^{-\\beta x}\\)\n\\(H(t\\|x) = \\text{rate} \\cdot e^{-\\beta x} \\cdot t\\)\n\n\n\n\nPH Baseline-Only\n\n\nShow code\nscenario = Scenario(\"exp\", :ph, :baseline)\nfig, max_diff = run_scenario_diagnostics(scenario)\nfig\n\n\n\n‚îå Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n\n‚îî @ Makie ~/.julia/packages/Makie/TOy8O/src/scenes.jl:264\n\n\n\n\n\n\n\n\n\nPH with Covariate\n\n\nShow code\nscenario = Scenario(\"exp\", :ph, :covariate)\nfig, max_diff = run_scenario_diagnostics(scenario)\nfig\n\n\n\n‚îå Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n\n‚îî @ Makie ~/.julia/packages/Makie/TOy8O/src/scenes.jl:264\n\n\n\n\n\n\n\n\n\nAFT Baseline-Only\n\n\nShow code\nscenario = Scenario(\"exp\", :aft, :baseline)\nfig, max_diff = run_scenario_diagnostics(scenario)\nfig\n\n\n\n‚îå Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n\n‚îî @ Makie ~/.julia/packages/Makie/TOy8O/src/scenes.jl:264\n\n\n\n\n\n\n\n\n\nAFT with Covariate\n\n\nShow code\nscenario = Scenario(\"exp\", :aft, :covariate)\nfig, max_diff = run_scenario_diagnostics(scenario)\nfig\n\n\n\n‚îå Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n\n‚îî @ Makie ~/.julia/packages/Makie/TOy8O/src/scenes.jl:264\n\n\n\n\n\n\n\n\n\n\n\n2.2 Weibull\nThe Weibull hazard: \\(h(t) = \\text{shape} \\cdot \\text{scale} \\cdot t^{\\text{shape}-1}\\)\nParameterization:\n\n\n\n\n\n\n\n\nEffect\nHazard\nCumulative Hazard\n\n\n\n\nPH\n\\(h(t\\|x) = k \\cdot \\lambda \\cdot t^{k-1} \\cdot e^{\\beta x}\\)\n\\(H(t\\|x) = \\lambda \\cdot e^{\\beta x} \\cdot t^k\\)\n\n\nAFT\n\\(h(t\\|x) = k \\cdot \\lambda \\cdot t^{k-1} \\cdot e^{-k\\beta x}\\)\n\\(H(t\\|x) = \\lambda \\cdot e^{-k\\beta x} \\cdot t^k\\)\n\n\n\n\nPH Baseline-Only\n\n\nShow code\nscenario = Scenario(\"wei\", :ph, :baseline)\nfig, max_diff = run_scenario_diagnostics(scenario)\nfig\n\n\n\n******************************************************************************\nThis program contains Ipopt, a library for large-scale nonlinear optimization.\n Ipopt is released as open source code under the Eclipse Public License (EPL).\n         For more information visit https://github.com/coin-or/Ipopt\n******************************************************************************\n\n\n\n\n‚îå Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n\n‚îî @ Makie ~/.julia/packages/Makie/TOy8O/src/scenes.jl:264\n\n\n\n\n\n\n\n\n\nPH with Covariate\n\n\nShow code\nscenario = Scenario(\"wei\", :ph, :covariate)\nfig, max_diff = run_scenario_diagnostics(scenario)\nfig\n\n\n\n‚îå Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n\n‚îî @ Makie ~/.julia/packages/Makie/TOy8O/src/scenes.jl:264\n\n\n\n\n\n\n\n\n\nAFT Baseline-Only\n\n\nShow code\nscenario = Scenario(\"wei\", :aft, :baseline)\nfig, max_diff = run_scenario_diagnostics(scenario)\nfig\n\n\n\n‚îå Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n\n‚îî @ Makie ~/.julia/packages/Makie/TOy8O/src/scenes.jl:264\n\n\n\n\n\n\n\n\n\nAFT with Covariate\n\n\nShow code\nscenario = Scenario(\"wei\", :aft, :covariate)\nfig, max_diff = run_scenario_diagnostics(scenario)\nfig\n\n\n\n‚îå Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n\n‚îî @ Makie ~/.julia/packages/Makie/TOy8O/src/scenes.jl:264\n\n\n\n\n\n\n\n\n\n\n\n2.3 Gompertz\nThe Gompertz hazard (flexsurv parameterization): \\(h(t) = \\text{rate} \\cdot e^{\\text{shape} \\cdot t}\\)\nParameterization:\n\n\n\n\n\n\n\n\nEffect\nHazard\nCumulative Hazard\n\n\n\n\nPH\n\\(h(t\\|x) = r \\cdot e^{at + \\beta x}\\)\n\\(H(t\\|x) = \\frac{r}{a} e^{\\beta x} (e^{at} - 1)\\)\n\n\nAFT\n\\(h(t\\|x) = r' \\cdot e^{a' t}\\) where \\(r' = r e^{-\\beta x}\\), \\(a' = a e^{-\\beta x}\\)\n\\(H(t\\|x) = \\frac{r'}{a'} (e^{a't} - 1)\\)\n\n\n\n\nPH Baseline-Only\n\n\nShow code\nscenario = Scenario(\"gom\", :ph, :baseline)\nfig, max_diff = run_scenario_diagnostics(scenario)\nfig\n\n\n\n‚îå Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n\n‚îî @ Makie ~/.julia/packages/Makie/TOy8O/src/scenes.jl:264\n\n\n\n\n\n\n\n\n\nPH with Covariate\n\n\nShow code\nscenario = Scenario(\"gom\", :ph, :covariate)\nfig, max_diff = run_scenario_diagnostics(scenario)\nfig\n\n\n\n‚îå Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n\n‚îî @ Makie ~/.julia/packages/Makie/TOy8O/src/scenes.jl:264\n\n\n\n\n\n\n\n\n\nAFT Baseline-Only\n\n\nShow code\nscenario = Scenario(\"gom\", :aft, :baseline)\nfig, max_diff = run_scenario_diagnostics(scenario)\nfig\n\n\n\n‚îå Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n\n‚îî @ Makie ~/.julia/packages/Makie/TOy8O/src/scenes.jl:264\n\n\n\n\n\n\n\n\n\nAFT with Covariate\n\n\nShow code\nscenario = Scenario(\"gom\", :aft, :covariate)\nfig, max_diff = run_scenario_diagnostics(scenario)\nfig\n\n\n\n‚îå Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n\n‚îî @ Makie ~/.julia/packages/Makie/TOy8O/src/scenes.jl:264"
  },
  {
    "objectID": "04_simulation_diagnostics.html#b-splines",
    "href": "04_simulation_diagnostics.html#b-splines",
    "title": "Simulation Diagnostics",
    "section": "3. B-Splines",
    "text": "3. B-Splines\n\n\n\n\n\n\nNoteStatus: Unit & Long Tests Cover Spline Functionality\n\n\n\nB-spline hazard validation is covered by:\n\nUnit tests (test_splines.jl, 255 tests): B-spline basis evaluation, cumulative hazard vs QuadGK numerical integration, penalty matrices\nLong tests (longtest_mcem_splines.jl, 6 scenarios): Parameter recovery for spline hazards with various functional forms\n\nFull simulation diagnostics (ECDF vs CDF plots) for splines require additional infrastructure to compute theoretical CDFs from arbitrary spline coefficient vectors. This is a lower priority since:\n\nSpline cumulative hazard accuracy is already verified against QuadGK (rtol=1e-10)\nParameter recovery in long tests validates end-to-end correctness\n\n\n\n\n3.1 Cubic Splines (4 Interior Knots)\nCovered by longtest_mcem_splines.jl Scenario 3 (Cubic pattern with Gompertz baseline)\n\n\n3.2 Quadratic Splines (3 Interior Knots)\nCovered by longtest_mcem_splines.jl Scenario 2 (Piecewise-linear increasing hazard)\n\n\n3.3 Natural Splines\nSpline boundary conditions handled by BSplineKit; unit tested in test_splines.jl\n\n\n3.4 Splines with Covariates (PH)\nCovered by longtest_mcem_splines.jl Scenario 4 (spline baseline + PH covariate)"
  },
  {
    "objectID": "04_simulation_diagnostics.html#time-varying-covariates",
    "href": "04_simulation_diagnostics.html#time-varying-covariates",
    "title": "Simulation Diagnostics",
    "section": "4. Time-Varying Covariates",
    "text": "4. Time-Varying Covariates\n\n4.1 Exponential PH with Step-Function TVC\n\n\nShow code\nscenario = Scenario(\"exp\", :ph, :tvc)\nfig, max_diff = run_scenario_diagnostics(scenario)\nfig\n\n\n\n‚îå Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n\n‚îî @ Makie ~/.julia/packages/Makie/TOy8O/src/scenes.jl:264\n\n\n\n\n\n\n\n\n\n4.2 Weibull PH with Step-Function TVC\n\n\nShow code\nscenario = Scenario(\"wei\", :ph, :tvc)\nfig, max_diff = run_scenario_diagnostics(scenario)\nfig\n\n\n\n‚îå Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n\n‚îî @ Makie ~/.julia/packages/Makie/TOy8O/src/scenes.jl:264"
  },
  {
    "objectID": "04_simulation_diagnostics.html#guarantees-validated",
    "href": "04_simulation_diagnostics.html#guarantees-validated",
    "title": "Simulation Diagnostics",
    "section": "5. Guarantees Validated",
    "text": "5. Guarantees Validated\nThis report validates the following guarantees:\n\nCall-stack accuracy: The eval_hazard, eval_cumhaz, and survprob functions produce values that match the closed-form analytical expressions.\nDistributional fidelity: The ECDF of simulated event times matches the theoretical CDF, with KS statistics consistent with the expected \\(O(1/\\sqrt{n})\\) convergence rate.\nTime-transform parity: Simulations using CachedTransformStrategy (time transforms enabled) produce identical results to DirectTransformStrategy (fallback), confirming that the caching optimization does not introduce bias.\nFamily coverage: All parametric families (Exponential, Weibull, Gompertz) with both PH and AFT covariate effects are validated."
  },
  {
    "objectID": "spline_comparison_benchmark.html",
    "href": "spline_comparison_benchmark.html",
    "title": "Spline Methods Comparison",
    "section": "",
    "text": "Show code\nlibrary(tidyverse)\nlibrary(mgcv)\nlibrary(pammtools)\nlibrary(flexsurv)\nlibrary(survival)\nlibrary(patchwork)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(survstan)\n\n# Set theme\ntheme_set(theme_bw(base_size = 12))\nShow code\ncd ../..\njulia --project=. -e '\nusing MultistateModels\nusing Random\nusing DataFrames\nusing CSV\nusing Printf\n\n# Configuration\nn = 1000\ntrue_shape = 1.5\ntrue_rate = 0.3\nmax_time = 5.0\nseed = 12345\nnknots = 5  # Number of interior knots\n\nRandom.seed!(seed)\n\n# Simulate Weibull survival data\nE = -log.(rand(n))\nevent_times = (E ./ true_rate) .^ (1 / true_shape)\nobs_times = min.(event_times, max_time)\nstatus = Int.(event_times .&lt;= max_time)\n\nprintln(\"=== Simple Survival Data ===\")\nprintln(\"n = $n\")\nprintln(\"Events: $(sum(status))\")\nprintln(\"Censored: $(n - sum(status))\")\nprintln(\"Time range: [$(round(minimum(obs_times), digits=3)), $(round(maximum(obs_times), digits=3))]\")\n\n# Create multistate model data\nsurv_data = DataFrame(\n    id = 1:n,\n    tstart = zeros(n),\n    tstop = obs_times,\n    statefrom = ones(Int, n),\n    stateto = ifelse.(status .== 1, 2, 1),\n    obstype = ones(Int, n)\n)\n\n# Save to CSV for R to read\nCSV.write(\"MultistateModelsTests/reports/_surv_data.csv\", DataFrame(time = obs_times, status = status))\n\n# Define model with spline hazard (placeholder knots will be replaced by calibration)\n# Using monotone=0 for unconstrained (non-monotonic) spline - tests general flexibility\nh12 = Hazard(@formula(0 ~ 1), \"sp\", 1, 2;\n             degree = 3,\n             knots = [max_time/2],  # Placeholder - will be replaced by calibrate_splines!\n             boundaryknots = [0.0, max_time],\n             natural_spline = true,\n             monotone = 0)  # Unconstrained spline\nmodel = multistatemodel(h12; data=surv_data)\n\n# Calibrate knots based on data - places knots at quantiles of event times\nprintln(\"\\nCalibrating spline knots...\")\nknot_result = calibrate_splines!(model; nknots=nknots, verbose=true)\n\n# Extract the calibrated knots\ninterior_knots = knot_result.h12.interior_knots\nboundary_knots = knot_result.h12.boundary_knots\nall_knots = model.hazards[1].knots\n\nprintln(\"\\n=== Knot Configuration ===\")\nprintln(\"Boundary knots: $(boundary_knots)\")\nprintln(\"Interior knots: $(round.(interior_knots, digits=3))\")\nprintln(\"Number of interior knots: $(length(interior_knots))\")\nprintln(\"Total knots: $(length(all_knots))\")\nprintln(\"Number of basis functions: $(length(model.hazards[1].parnames) - length(model.hazards[1].covar_names))\")\n\n# Save knots for R to use\nCSV.write(\"MultistateModelsTests/reports/_knots.csv\", DataFrame(\n    interior_knots = interior_knots,\n    boundary_lower = fill(boundary_knots[1], length(interior_knots)),\n    boundary_upper = fill(boundary_knots[2], length(interior_knots))\n))\n\nprintln(\"\\nData generation complete. Knots and data saved for R.\")\n'\nShow code\ncd ../..\njulia --project=. -e '\nusing MultistateModels\nusing MultistateModels: get_parameters_flat, get_loglik\nusing Random\nusing DataFrames\nusing CSV\nusing Printf\n\n# Configuration (must match data generation)\nn = 1000\ntrue_shape = 1.5\ntrue_rate = 0.3\nmax_time = 5.0\nseed = 12345\nnknots = 5\n\nRandom.seed!(seed)\n\n# Regenerate the same data\nE = -log.(rand(n))\nevent_times = (E ./ true_rate) .^ (1 / true_shape)\nobs_times = min.(event_times, max_time)\nstatus = Int.(event_times .&lt;= max_time)\n\nsurv_data = DataFrame(\n    id = 1:n,\n    tstart = zeros(n),\n    tstop = obs_times,\n    statefrom = ones(Int, n),\n    stateto = ifelse.(status .== 1, 2, 1),\n    obstype = ones(Int, n)\n)\n\n# Define and calibrate model\n# Using monotone=0 for unconstrained (non-monotonic) spline - tests general flexibility\nh12 = Hazard(@formula(0 ~ 1), \"sp\", 1, 2;\n             degree = 3,\n             knots = [max_time/2],  # Placeholder - replaced by calibrate_splines!\n             boundaryknots = [0.0, max_time],\n             natural_spline = true,\n             monotone = 0)  # Unconstrained spline\nmodel = multistatemodel(h12; data=surv_data)\ncalibrate_splines!(model; nknots=nknots, verbose=false)\n\n# Fit with each smoothing method and measure runtime\n# NEW API: fit(model; penalty=:auto, select_lambda=:method, vcov_type=:none)\ntimings = Dict{String, Float64}()\n\nprintln(\"=== Fitting Smoothing Methods with Timing ===\")\n\n# Helper to get total EDF - edf is (total=Float64, per_term=Vector{Float64})\ntotal_edf(fitted) = fitted.edf.total\n\n# PIJCV (LOO) - Newton-approximated\nprintln(\"\\nFitting with PIJCV (LOO) method...\")\nfit(model; penalty=:auto, select_lambda=:pijcv, vcov_type=:none, verbose=false)  # warmup\nt_pijcv = @elapsed fitted_pijcv = fit(model; penalty=:auto, select_lambda=:pijcv, vcov_type=:none, verbose=false)\ntimings[\"PIJCV\"] = t_pijcv\nprintln(\"  PIJCV Œª = $(round(fitted_pijcv.smoothing_parameters[1], digits=2)), EDF = $(round(total_edf(fitted_pijcv), digits=2)), time = $(round(t_pijcv, digits=3))s\")\n\n# PIJCV10 (10-fold) - Newton-approximated\nprintln(\"\\nFitting with PIJCV10 (10-fold) method...\")\nfit(model; penalty=:auto, select_lambda=:pijcv10, vcov_type=:none, verbose=false)  # warmup\nt_pijcv10 = @elapsed fitted_pijcv10 = fit(model; penalty=:auto, select_lambda=:pijcv10, vcov_type=:none, verbose=false)\ntimings[\"PIJCV10\"] = t_pijcv10\nprintln(\"  PIJCV10 Œª = $(round(fitted_pijcv10.smoothing_parameters[1], digits=2)), EDF = $(round(total_edf(fitted_pijcv10), digits=2)), time = $(round(t_pijcv10, digits=3))s\")\n\n# EFS\nprintln(\"\\nFitting with EFS method...\")\nfit(model; penalty=:auto, select_lambda=:efs, vcov_type=:none, verbose=false)  # warmup\nt_efs = @elapsed fitted_efs = fit(model; penalty=:auto, select_lambda=:efs, vcov_type=:none, verbose=false)\ntimings[\"EFS\"] = t_efs\nprintln(\"  EFS Œª = $(round(fitted_efs.smoothing_parameters[1], digits=2)), EDF = $(round(total_edf(fitted_efs), digits=2)), time = $(round(t_efs, digits=3))s\")\n\n# CV10 (exact 10-fold)\nprintln(\"\\nFitting with 10-fold CV (exact) method...\")\nfit(model; penalty=:auto, select_lambda=:cv10, vcov_type=:none, verbose=false)  # warmup\nt_cv10 = @elapsed fitted_cv10 = fit(model; penalty=:auto, select_lambda=:cv10, vcov_type=:none, verbose=false)\ntimings[\"CV10\"] = t_cv10\nprintln(\"  CV10 Œª = $(round(fitted_cv10.smoothing_parameters[1], digits=2)), EDF = $(round(total_edf(fitted_cv10), digits=2)), time = $(round(t_cv10, digits=3))s\")\n\n# =============================================================================\n# WEIGHTED SMOOTHING METHODS\n# =============================================================================\n# Weighted penalties use w(t) = Y(t)^(-Œ±) to penalize more where fewer subjects\n# are at risk. This can improve estimation in regions with sparse data.\n\n# PIJCV with at-risk weighting (fixed Œ±=1.0)\nprintln(\"\\nFitting with PIJCV + at-risk weighting (Œ±=1.0)...\")\nweighted_penalty = SplinePenalty(adaptive_weight=:atrisk, alpha=1.0)\nfit(model; penalty=weighted_penalty, select_lambda=:pijcv, vcov_type=:none, verbose=false)  # warmup\nt_pijcv_w = @elapsed fitted_pijcv_w = fit(model; penalty=weighted_penalty, select_lambda=:pijcv, vcov_type=:none, verbose=false)\ntimings[\"PIJCV_weighted\"] = t_pijcv_w\nprintln(\"  PIJCV (weighted Œ±=1) Œª = $(round(fitted_pijcv_w.smoothing_parameters[1], digits=2)), EDF = $(round(total_edf(fitted_pijcv_w), digits=2)), time = $(round(t_pijcv_w, digits=3))s\")\n\n# PIJCV with learned Œ± (estimate optimal Œ± from data)\nprintln(\"\\nFitting with PIJCV + at-risk weighting (learn Œ±)...\")\nlearn_alpha_penalty = SplinePenalty(adaptive_weight=:atrisk, learn_alpha=true)\nfit(model; penalty=learn_alpha_penalty, select_lambda=:pijcv, vcov_type=:none, verbose=false)  # warmup\nt_pijcv_la = @elapsed fitted_pijcv_la = fit(model; penalty=learn_alpha_penalty, select_lambda=:pijcv, vcov_type=:none, verbose=false)\ntimings[\"PIJCV_learn_alpha\"] = t_pijcv_la\n# Extract learned alpha from the penalty configuration if available\nlearned_alpha = hasfield(typeof(fitted_pijcv_la), :penalty_config) && \n                fitted_pijcv_la.penalty_config !== nothing ? \n                \"Œ±=$(round(fitted_pijcv_la.penalty_config.alpha, digits=2))\" : \"Œ±=learned\"\nprintln(\"  PIJCV (learn Œ±) Œª = $(round(fitted_pijcv_la.smoothing_parameters[1], digits=2)), EDF = $(round(total_edf(fitted_pijcv_la), digits=2)), $learned_alpha, time = $(round(t_pijcv_la, digits=3))s\")\n\n# Evaluation grid\neval_times = collect(range(0.01, max_time, length=200))\n\n# Function to evaluate hazard at a grid of times\nfunction evaluate_curves(model, beta, eval_times)\n    haz = model.hazards[1]\n    hazard_vals = [haz.hazard_fn(t, beta, ()) for t in eval_times]\n    cumhaz_vals = [haz.cumhaz_fn(0.0, t, beta, ()) for t in eval_times]\n    survival_vals = exp.(-cumhaz_vals)\n    return (hazard = hazard_vals, cumhaz = cumhaz_vals, survival = survival_vals)\nend\n\n# Extract flat parameters from fitted models\nbeta_pijcv = get_parameters_flat(fitted_pijcv)\nbeta_pijcv10 = get_parameters_flat(fitted_pijcv10)\nbeta_efs = get_parameters_flat(fitted_efs)\nbeta_cv10 = get_parameters_flat(fitted_cv10)\nbeta_pijcv_w = get_parameters_flat(fitted_pijcv_w)\nbeta_pijcv_la = get_parameters_flat(fitted_pijcv_la)\n\n# Compute curves for each method\ncurves_pijcv = evaluate_curves(model, beta_pijcv, eval_times)\ncurves_pijcv10 = evaluate_curves(model, beta_pijcv10, eval_times)\ncurves_efs = evaluate_curves(model, beta_efs, eval_times)\ncurves_cv10 = evaluate_curves(model, beta_cv10, eval_times)\ncurves_pijcv_w = evaluate_curves(model, beta_pijcv_w, eval_times)\ncurves_pijcv_la = evaluate_curves(model, beta_pijcv_la, eval_times)\n\n# Get log-likelihoods directly from fitted models\nloglik_pijcv = get_loglik(fitted_pijcv)\nloglik_pijcv10 = get_loglik(fitted_pijcv10)\nloglik_efs = get_loglik(fitted_efs)\nloglik_cv10 = get_loglik(fitted_cv10)\nloglik_pijcv_w = get_loglik(fitted_pijcv_w)\nloglik_pijcv_la = get_loglik(fitted_pijcv_la)\n\n# Save Julia results for R plotting (include weighted methods)\njulia_results = DataFrame(\n    time = repeat(eval_times, 6),\n    hazard = vcat(curves_pijcv.hazard, curves_pijcv10.hazard,\n                  curves_efs.hazard, curves_cv10.hazard,\n                  curves_pijcv_w.hazard, curves_pijcv_la.hazard),\n    cumhaz = vcat(curves_pijcv.cumhaz, curves_pijcv10.cumhaz,\n                  curves_efs.cumhaz, curves_cv10.cumhaz,\n                  curves_pijcv_w.cumhaz, curves_pijcv_la.cumhaz),\n    survival = vcat(curves_pijcv.survival, curves_pijcv10.survival,\n                    curves_efs.survival, curves_cv10.survival,\n                    curves_pijcv_w.survival, curves_pijcv_la.survival),\n    method = repeat([\"Julia PIJCV\", \"Julia PIJCV10\",\n                     \"Julia EFS\", \"Julia CV10\",\n                     \"Julia PIJCV (weighted)\", \"Julia PIJCV (learn Œ±)\"], inner=length(eval_times))\n)\nCSV.write(\"MultistateModelsTests/reports/_julia_curves.csv\", julia_results)\n\n# Save summary stats with log-likelihoods and timing\njulia_summary = DataFrame(\n    method = [\"PIJCV\", \"PIJCV10\", \"EFS\", \"CV10\", \"PIJCV_weighted\", \"PIJCV_learn_alpha\"],\n    lambda = [fitted_pijcv.smoothing_parameters[1], fitted_pijcv10.smoothing_parameters[1],\n              fitted_efs.smoothing_parameters[1], fitted_cv10.smoothing_parameters[1],\n              fitted_pijcv_w.smoothing_parameters[1], fitted_pijcv_la.smoothing_parameters[1]],\n    edf = [total_edf(fitted_pijcv), total_edf(fitted_pijcv10),\n           total_edf(fitted_efs), total_edf(fitted_cv10),\n           total_edf(fitted_pijcv_w), total_edf(fitted_pijcv_la)],\n    loglik = [loglik_pijcv, loglik_pijcv10,\n              loglik_efs, loglik_cv10,\n              loglik_pijcv_w, loglik_pijcv_la],\n    runtime_sec = [timings[\"PIJCV\"], timings[\"PIJCV10\"],\n                   timings[\"EFS\"], timings[\"CV10\"],\n                   timings[\"PIJCV_weighted\"], timings[\"PIJCV_learn_alpha\"]],\n    weighted = [false, false, false, false, true, true]\n)\nCSV.write(\"MultistateModelsTests/reports/_julia_summary.csv\", julia_summary)\n\nprintln(\"\\n=== Runtime Summary ===\")\nprintln(\"Newton-approximated CV methods (unweighted):\")\nprintln(\"  PIJCV (LOO): $(round(timings[\"PIJCV\"], digits=3))s\")\nprintln(\"  PIJCV10:     $(round(timings[\"PIJCV10\"], digits=3))s\")\nprintln(\"Other analytical methods:\")\nprintln(\"  EFS:         $(round(timings[\"EFS\"], digits=3))s\")\nprintln(\"Exact CV methods:\")\nprintln(\"  CV10:        $(round(timings[\"CV10\"], digits=3))s\")\nprintln(\"Weighted penalty methods:\")\nprintln(\"  PIJCV (Œ±=1): $(round(timings[\"PIJCV_weighted\"], digits=3))s\")\nprintln(\"  PIJCV (learn Œ±): $(round(timings[\"PIJCV_learn_alpha\"], digits=3))s\")\n\nprintln(\"\\nJulia curves computed and saved.\")\n'"
  },
  {
    "objectID": "spline_comparison_benchmark.html#comparison-overview",
    "href": "spline_comparison_benchmark.html#comparison-overview",
    "title": "Spline Methods Comparison",
    "section": "Comparison Overview",
    "text": "Comparison Overview\nPackages:\n\nMultistateModels.jl (Julia): Penalized likelihood with P-splines using the General P-Spline (GPS) penalty from Li & Cao (2022), which correctly handles non-uniform knot spacing via weighted differences\nmgcv (R): Generalized additive models via piecewise-exponential/Poisson\nflexsurv (R): Flexible parametric survival models with spline hazards on log-time scale\nsurvstan (R): Parametric survival models via Stan optimization (Weibull comparison)\n\nSmoothing Parameter Selection Methods:\n\n\n\n\n\n\n\n\n\nPackage\nMethod\nDescription\nReference\n\n\n\n\nMultistateModels.jl\nPIJCV\nNewton-approx LOO CV\nWood (2024)\n\n\nMultistateModels.jl\nPIJCV10\nNewton-approx 10-fold CV\nExtended from Wood (2024)\n\n\nMultistateModels.jl\nEFS\nExtended Fellner-Schall\nWood & Fasiolo (2017)\n\n\nMultistateModels.jl\nCV10\nExact 10-Fold CV\n(refits 10 times)\n\n\nMultistateModels.jl\nPIJCV (weighted)\nPIJCV with w(t)=Y(t)^(-1) penalty\nAt-risk weighting\n\n\nMultistateModels.jl\nPIJCV (learn Œ±)\nPIJCV with estimated Œ±\nAdaptive weighting\n\n\nmgcv\nGCV.Cp\nGeneralized CV via Poisson GAM\nWood (2017)\n\n\nmgcv\nREML\nRestricted ML\nWood (2011)\n\n\nsurvstan\nMLE\nMLE via Stan (Weibull PH)\nDemarqui & Mayrink (2021)\n\n\n\nWeighted Penalty Methods:\nWeighted penalties use \\(w(t) = Y(t)^{-\\alpha}\\) where \\(Y(t)\\) is the number of subjects at risk at time \\(t\\). This penalizes more strongly in regions with sparse data (fewer subjects at risk), helping prevent overfitting in the tails. Two variants are tested:\n\nFixed Œ±=1: Standard inverse at-risk weighting\nLearned Œ±: Estimate optimal Œ± from data via marginal likelihood optimization"
  },
  {
    "objectID": "spline_comparison_benchmark.html#true-model-specification",
    "href": "spline_comparison_benchmark.html#true-model-specification",
    "title": "Spline Methods Comparison",
    "section": "True Model Specification",
    "text": "True Model Specification\nData are simulated from a Weibull hazard with:\n\nShape (\\(\\kappa\\)): 1.5 (increasing hazard)\nRate (\\(\\lambda\\)): 0.3\nSample size: 100\n\n\\[h(t) = \\kappa \\cdot \\lambda \\cdot t^{\\kappa - 1} = 1.5 \\times 0.3 \\times t^{0.5} = 0.45 \\sqrt{t}\\]\n\\[H(t) = \\lambda \\cdot t^{\\kappa} = 0.3 \\cdot t^{1.5}\\]\n\\[S(t) = \\exp(-H(t)) = \\exp(-0.3 \\cdot t^{1.5})\\]"
  },
  {
    "objectID": "spline_comparison_benchmark.html#load-julia-results-in-r",
    "href": "spline_comparison_benchmark.html#load-julia-results-in-r",
    "title": "Spline Methods Comparison",
    "section": "Load Julia Results in R",
    "text": "Load Julia Results in R\n\n\nShow code\n# Load simulated data\nsurv_data &lt;- read.csv(\"_surv_data.csv\")\nn &lt;- nrow(surv_data)\ntrue_shape &lt;- 1.5\ntrue_rate &lt;- 0.3\nmax_time &lt;- 5.0\n\n# Load Julia curves\njulia_curves &lt;- read.csv(\"_julia_curves.csv\")\njulia_summary &lt;- read.csv(\"_julia_summary.csv\")\n\n# Load knots calibrated by Julia (from event time quantiles)\nknots_df &lt;- read.csv(\"_knots.csv\")\ninterior_knots &lt;- knots_df$interior_knots\nboundary_lower &lt;- knots_df$boundary_lower[1]\nboundary_upper &lt;- knots_df$boundary_upper[1]\n\ncat(\"=== Julia MultistateModels.jl Results ===\\n\")\nprint(julia_summary)\n\ncat(\"\\n=== Knot Configuration (shared across all methods) ===\\n\")\ncat(\"Boundary knots: [\", boundary_lower, \",\", boundary_upper, \"]\\n\")\ncat(\"Interior knots:\", round(interior_knots, 3), \"\\n\")\ncat(\"Number of interior knots:\", length(interior_knots), \"\\n\")"
  },
  {
    "objectID": "spline_comparison_benchmark.html#true-hazard-and-survival-functions",
    "href": "spline_comparison_benchmark.html#true-hazard-and-survival-functions",
    "title": "Spline Methods Comparison",
    "section": "True Hazard and Survival Functions",
    "text": "True Hazard and Survival Functions\n\n\nShow code\n# True hazard function (Weibull with rate parameterization)\ntrue_hazard &lt;- function(t, shape = true_shape, rate = true_rate) {\n  shape * rate * t^(shape - 1)\n}\n\n# True cumulative hazard\ntrue_cumhaz &lt;- function(t, shape = true_shape, rate = true_rate) {\n  rate * t^shape\n}\n\n# True survival function\ntrue_surv &lt;- function(t, shape = true_shape, rate = true_rate) {\n  exp(-true_cumhaz(t, shape, rate))\n}\n\n# True CDF (cumulative incidence for single transition)\ntrue_cdf &lt;- function(t, shape = true_shape, rate = true_rate) {\n  1 - true_surv(t, shape, rate)\n}"
  },
  {
    "objectID": "spline_comparison_benchmark.html#mgcv-pam-fit",
    "href": "spline_comparison_benchmark.html#mgcv-pam-fit",
    "title": "Spline Methods Comparison",
    "section": "mgcv PAM Fit",
    "text": "mgcv PAM Fit\nWe fit a piecewise-exponential additive model (PAM) using mgcv::gam with Poisson likelihood. This transforms the survival problem into a Poisson regression. mgcv uses the exact same knots as Julia via manual knot specification for P-splines.\n\n\nShow code\n# Create Surv object\nsurv_obj &lt;- Surv(surv_data$time, surv_data$status)\n\n# Add id column if not present\nsurv_data_ped &lt;- surv_data %&gt;% mutate(id = row_number())\n\n# Use pammtools to create piecewise-exponential data (PED)\n# This handles the data transformation properly\nped &lt;- as_ped(\n  formula = Surv(time, status) ~ 1,\n  data = surv_data_ped,\n  id = \"id\"\n)\n\ncat(\"PED data structure (via pammtools):\\n\")\ncat(\"  Total pseudo-observations:\", nrow(ped), \"\\n\")\ncat(\"  Events in PED:\", sum(ped$ped_status), \"\\n\")\n\n# For mgcv P-splines with manual knots:\n# - k is the basis dimension\n# - m is the spline order (m=2 for cubic, with 2nd order difference penalty)\n# - Need k + m + 2 knots total\n# - The middle k - m knots must span the data range\n# - P-splines require EVENLY SPACED knots for proper penalty behavior\n\n# Julia uses 5 interior knots + 2 boundary = 7 middle knots\n# So k - m = 7, and with m = 2, we have k = 9\nk_mgcv &lt;- length(interior_knots) + 4  # 5 + 4 = 9\nm_order &lt;- 2\n\n# Build the full knot sequence for P-splines\n# Middle knots: [boundary_lower, interior_knots, boundary_upper]\nmiddle_knots &lt;- c(boundary_lower, interior_knots, boundary_upper)\n\n# Need (k + m + 2) - (k - m) = 2m + 2 = 6 padding knots (3 on each side)\n# P-splines require evenly spaced knots - use average spacing for padding\n# Note: Padding knots extend below 0 but this is just for basis construction;\n# the hazard is never evaluated at negative times\ndelta &lt;- mean(diff(middle_knots))\npadding_lower &lt;- boundary_lower - (3:1) * delta\npadding_upper &lt;- boundary_upper + (1:3) * delta\n\n# Full knot sequence\nmgcv_knots &lt;- c(padding_lower, middle_knots, padding_upper)\n\ncat(\"\\n=== Knot Configuration ===\\n\")\ncat(\"Julia interior knots:\", round(interior_knots, 3), \"\\n\")\ncat(\"Middle knots (k-m =\", length(middle_knots), \"):\", round(middle_knots, 3), \"\\n\")\ncat(\"Full P-spline knots (k+m+2 =\", length(mgcv_knots), \"):\", round(mgcv_knots, 3), \"\\n\")\ncat(\"Average knot spacing (delta):\", round(delta, 3), \"\\n\")\ncat(\"mgcv k =\", k_mgcv, \", m =\", m_order, \"\\n\")\n\n# Fit PAM with GCV using P-splines and manual knots\nfit_gcv &lt;- pamm(\n  ped_status ~ s(tend, bs = \"ps\", k = k_mgcv, m = c(m_order, m_order)),\n  data = ped,\n  method = \"GCV.Cp\",\n  knots = list(tend = mgcv_knots)\n)\n\n# Fit PAM with REML\nfit_reml &lt;- pamm(\n  ped_status ~ s(tend, bs = \"ps\", k = k_mgcv, m = c(m_order, m_order)),\n  data = ped,\n  method = \"REML\",\n  knots = list(tend = mgcv_knots)\n)\n\ncat(\"\\n=== mgcv/pammtools Results (P-splines with Julia knots) ===\\n\")\ncat(\"k =\", k_mgcv, \", m =\", m_order, \"\\n\")\ncat(\"Manual knots used:\", round(mgcv_knots, 3), \"\\n\")\n\ncat(\"\\nGCV.Cp:\\n\")\ncat(\"  sp =\", round(fit_gcv$sp, 6), \"\\n\")\ncat(\"  EDF =\", round(sum(fit_gcv$edf), 4), \"\\n\")\n\ncat(\"\\nREML:\\n\")\ncat(\"  sp =\", round(fit_reml$sp, 6), \"\\n\")\ncat(\"  EDF =\", round(sum(fit_reml$edf), 4), \"\\n\")"
  },
  {
    "objectID": "spline_comparison_benchmark.html#flexsurv-fit",
    "href": "spline_comparison_benchmark.html#flexsurv-fit",
    "title": "Spline Methods Comparison",
    "section": "flexsurv Fit",
    "text": "flexsurv Fit\n\n\nShow code\n# flexsurv uses knots on log(time) scale, so we need to transform\n# But we can specify explicit knots via the knots argument\n# flexsurv's \"knots\" argument takes the interior knots on the log scale\n\n# Transform interior knots to log scale for flexsurv\n# Note: flexsurv interprets knots as on log(time) scale\nlog_interior_knots &lt;- log(interior_knots)\n\ncat(\"Using same interior knots as Julia (transformed to log scale for flexsurv):\\n\")\ncat(\"  Original knots:\", round(interior_knots, 3), \"\\n\")\ncat(\"  Log-scale knots:\", round(log_interior_knots, 3), \"\\n\")\n\n# Fit flexsurv spline model with the same knots\nfit_flex &lt;- flexsurvspline(\n  Surv(time, status) ~ 1,\n  data = surv_data,\n  knots = log_interior_knots,  # Interior knots on log scale\n  scale = \"hazard\"\n)\n\n# Also fit true Weibull for reference\nfit_weibull &lt;- flexsurvreg(\n  Surv(time, status) ~ 1,\n  data = surv_data,\n  dist = \"weibullPH\"\n)\n\ncat(\"\\n=== flexsurv Results ===\\n\")\ncat(\"\\nSpline model (\", length(interior_knots), \" interior knots):\\n\", sep=\"\")\ncat(\"  AIC =\", round(AIC(fit_flex), 2), \"\\n\")\n\ncat(\"\\nWeibull fit (for reference):\\n\")\ncat(\"  Estimated shape:\", round(exp(fit_weibull$res[\"shape\", \"est\"]), 3), \"\\n\")\ncat(\"  Estimated scale:\", round(exp(fit_weibull$res[\"scale\", \"est\"]), 3), \"\\n\")\ncat(\"  True shape:\", true_shape, \"\\n\")\ncat(\"  True rate:\", true_rate, \"\\n\")"
  },
  {
    "objectID": "spline_comparison_benchmark.html#survstan-fit-weibull-ph-via-stan",
    "href": "spline_comparison_benchmark.html#survstan-fit-weibull-ph-via-stan",
    "title": "Spline Methods Comparison",
    "section": "survstan Fit (Weibull PH via Stan)",
    "text": "survstan Fit (Weibull PH via Stan)\nWe fit a Weibull proportional hazards model using survstan which uses Stan for optimization. This provides a parametric comparison point against the flexible spline methods.\n\n\nShow code\ncat(\"=== Fitting survstan (Weibull PH via Stan) ===\\n\")\n\n# Fit Weibull PH model via Stan\n# survstan uses phreg() for proportional hazards with MLE via Stan\nfit_survstan &lt;- survstan::phreg(\n  Surv(time, status) ~ 1,\n  data = surv_data,\n  baseline = \"weibull\"\n)\n\ncat(\"\\n=== survstan Results ===\\n\")\ncat(\"Weibull PH model (MLE via Stan)\\n\")\n\n# Extract parameter estimates - estimates() returns a named vector\nsurvstan_est &lt;- survstan::estimates(fit_survstan)\ncat(\"\\nEstimated parameters:\\n\")\nprint(survstan_est)\n\n# Extract alpha (shape) and gamma (scale) for hazard computation\nsurvstan_alpha &lt;- survstan_est[\"alpha\"]\nsurvstan_gamma &lt;- survstan_est[\"gamma\"]\ncat(\"\\nWeibull shape (alpha):\", survstan_alpha, \"\\n\")\ncat(\"Weibull scale (gamma):\", survstan_gamma, \"\\n\")"
  },
  {
    "objectID": "spline_comparison_benchmark.html#effective-degrees-of-freedom-comparison",
    "href": "spline_comparison_benchmark.html#effective-degrees-of-freedom-comparison",
    "title": "Spline Methods Comparison",
    "section": "Effective Degrees of Freedom Comparison",
    "text": "Effective Degrees of Freedom Comparison\nThe effective degrees of freedom (EDF) is the proper metric for comparing smoothing across different software packages. EDF measures how many parameters the smooth effectively uses, independent of Œª scaling conventions.\n\n\nShow code\n# Collect EDF results - this is the key comparison metric\nedf_summary &lt;- rbind(\n  julia_summary %&gt;% \n    mutate(Package = \"MultistateModels.jl\") %&gt;%\n    select(Package, Method = method, EDF = edf, Lambda_or_sp = lambda),\n  data.frame(\n    Package = c(\"mgcv\", \"mgcv\"),\n    Method = c(\"GCV.Cp\", \"REML\"),\n    EDF = c(sum(fit_gcv$edf), sum(fit_reml$edf)),\n    Lambda_or_sp = c(fit_gcv$sp, fit_reml$sp)\n  ),\n  data.frame(\n    Package = \"survstan\",\n    Method = \"Weibull MLE\",\n    EDF = 2,  # Weibull has 2 parameters (shape, scale)\n    Lambda_or_sp = NA\n  )\n)\n\nkable(edf_summary, digits = 4,\n      caption = \"Effective Degrees of Freedom by Method (Key Comparison)\",\n      col.names = c(\"Package\", \"Method\", \"EDF\", \"Œª or sp\")) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\")) %&gt;%\n  column_spec(3, bold = TRUE, background = \"#f0f9e8\")\n\n\n\nEffective Degrees of Freedom by Method (Key Comparison)\n\n\nPackage\nMethod\nEDF\nŒª or sp\n\n\n\n\nMultistateModels.jl\nPIJCV\n5.8921\n1.2977\n\n\nMultistateModels.jl\nPIJCV10\n5.8921\n1.2977\n\n\nMultistateModels.jl\nEFS\n6.2823\n0.7251\n\n\nMultistateModels.jl\nCV10\n5.7278\n1.6487\n\n\nMultistateModels.jl\nPIJCV_weighted\n4.1423\n2.6600\n\n\nMultistateModels.jl\nPIJCV_learn_alpha\n4.8098\n2.6600\n\n\nmgcv\nGCV.Cp\n6.0922\n20.0537\n\n\nmgcv\nREML\n5.3922\n53.0811\n\n\nsurvstan\nWeibull MLE\n2.0000\nNA"
  },
  {
    "objectID": "spline_comparison_benchmark.html#model-selection-criteria-aicbic",
    "href": "spline_comparison_benchmark.html#model-selection-criteria-aicbic",
    "title": "Spline Methods Comparison",
    "section": "Model Selection Criteria (AIC/BIC)",
    "text": "Model Selection Criteria (AIC/BIC)\nFor model comparison, we compute AIC and BIC based on the penalized log-likelihood and effective degrees of freedom:\n\nAIC = \\(-2 \\cdot \\ell + 2 \\cdot \\text{EDF}\\)\nBIC = \\(-2 \\cdot \\ell + \\log(n) \\cdot \\text{EDF}\\)\n\n\n\nShow code\nn_obs &lt;- nrow(surv_data)\n\n# Julia model selection criteria\njulia_aic_bic &lt;- julia_summary %&gt;%\n  mutate(\n    Package = \"MultistateModels.jl\",\n    AIC = -2 * loglik + 2 * edf,\n    BIC = -2 * loglik + log(n_obs) * edf\n  ) %&gt;%\n  select(Package, Method = method, EDF = edf, LogLik = loglik, AIC, BIC)\n\n# For mgcv: The Poisson pseudo-likelihood differs from survival likelihood by\n# an offset correction. The relationship is:\n#   ‚Ñì_Poisson = ‚Ñì_Survival + Œ£_i Œ¥_i * offset_i\n# where offset_i = log(interval_width) for the event interval.\n# Therefore: ‚Ñì_Survival = ‚Ñì_Poisson - Œ£_i Œ¥_i * offset_i\n\ncompute_surv_loglik_mgcv &lt;- function(fit_gam, ped) {\n  # Get Poisson log-likelihood\n  mu_hat &lt;- fitted(fit_gam)\n  y &lt;- ped$ped_status\n  poisson_ll &lt;- sum(y * log(mu_hat) - mu_hat)\n  \n  # Correction: subtract sum of offsets for event pseudo-observations\n  event_offsets &lt;- ped$offset[ped$ped_status == 1]\n  offset_correction &lt;- sum(event_offsets)\n  \n  # Survival log-likelihood\n  surv_ll &lt;- poisson_ll - offset_correction\n  return(surv_ll)\n}\n\n# Compute survival log-likelihoods for mgcv models\nloglik_mgcv_gcv &lt;- compute_surv_loglik_mgcv(fit_gcv, ped)\nloglik_mgcv_reml &lt;- compute_surv_loglik_mgcv(fit_reml, ped)\n\n# mgcv model selection using survival log-likelihood\nmgcv_aic_bic &lt;- data.frame(\n  Package = c(\"mgcv\", \"mgcv\"),\n  Method = c(\"GCV.Cp\", \"REML\"),\n  EDF = c(sum(fit_gcv$edf), sum(fit_reml$edf)),\n  LogLik = c(loglik_mgcv_gcv, loglik_mgcv_reml),\n  AIC = c(-2 * loglik_mgcv_gcv + 2 * sum(fit_gcv$edf),\n          -2 * loglik_mgcv_reml + 2 * sum(fit_reml$edf)),\n  BIC = c(-2 * loglik_mgcv_gcv + log(n_obs) * sum(fit_gcv$edf),\n          -2 * loglik_mgcv_reml + log(n_obs) * sum(fit_reml$edf))\n)\n\n# flexsurv model selection\nflex_aic_bic &lt;- data.frame(\n  Package = \"flexsurv\",\n  Method = \"spline\",\n  EDF = length(coef(fit_flex)),\n  LogLik = fit_flex$loglik,\n  AIC = AIC(fit_flex),\n  BIC = BIC(fit_flex)\n)\n\n# survstan model selection\nsurvstan_loglik &lt;- fit_survstan$loglik\nsurvstan_aic_bic &lt;- data.frame(\n  Package = \"survstan\",\n  Method = \"Weibull MLE\",\n  EDF = 2,\n  LogLik = survstan_loglik,\n  AIC = AIC(fit_survstan),\n  BIC = -2 * survstan_loglik + log(n_obs) * 2  # Manual BIC calculation\n)\n\naic_bic_summary &lt;- rbind(julia_aic_bic, mgcv_aic_bic, flex_aic_bic, survstan_aic_bic)\n\nn_julia_methods &lt;- nrow(julia_aic_bic)\nkable(aic_bic_summary, digits = 2,\n      caption = \"Model Selection Criteria by Method (Survival Log-Likelihood)\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\")) %&gt;%\n  pack_rows(\"MultistateModels.jl\", 1, n_julia_methods) %&gt;%\n  pack_rows(\"mgcv\", n_julia_methods + 1, n_julia_methods + 2) %&gt;%\n  pack_rows(\"flexsurv\", n_julia_methods + 3, n_julia_methods + 3) %&gt;%\n  pack_rows(\"survstan\", n_julia_methods + 4, n_julia_methods + 4)\n\n\n\nModel Selection Criteria by Method (Survival Log-Likelihood)\n\n\nPackage\nMethod\nEDF\nLogLik\nAIC\nBIC\n\n\n\n\nMultistateModels.jl\n\n\nMultistateModels.jl\nPIJCV\n5.89\n-1556.82\n3125.43\n3154.35\n\n\nMultistateModels.jl\nPIJCV10\n5.89\n-1556.82\n3125.43\n3154.35\n\n\nMultistateModels.jl\nEFS\n6.28\n-1554.80\n3122.17\n3153.00\n\n\nMultistateModels.jl\nCV10\n5.73\n-1558.01\n3127.47\n3155.58\n\n\nMultistateModels.jl\nPIJCV_weighted\n4.14\n-1555.26\n3118.80\n3139.13\n\n\nMultistateModels.jl\nPIJCV_learn_alpha\n4.81\n-1554.30\n3118.22\n3141.83\n\n\nmgcv\n\n\nmgcv\nGCV.Cp\n6.09\n-1553.90\n3119.99\n3149.89\n\n\nmgcv\nREML\n5.39\n-1554.93\n3120.65\n3147.11\n\n\nflexsurv\n\n\nflexsurv\nspline\n7.00\n-1553.31\n3120.62\n3154.97\n\n\nsurvstan\n\n\nsurvstan\nWeibull MLE\n2.00\n6.07\n-8.14\n1.67\n\n\n\n\n\nNote on mgcv log-likelihood: The mgcv package reports Poisson pseudo-likelihood values, not survival log-likelihood. We convert using the relationship: \\(\\ell_{\\text{survival}} = \\ell_{\\text{Poisson}} - \\sum_i \\delta_i \\cdot \\text{offset}_i\\), where \\(\\text{offset}_i = \\log(\\text{interval width})\\) for each event‚Äôs interval.\nKey observations:\n\nAll methods produce similar log-likelihoods (within ~2-3 units), validating correctness\nEDF values are in the range 2-3, indicating similar effective smoothness across methods\nJulia‚Äôs exact likelihood and mgcv‚Äôs PAM/Poisson approach agree well"
  },
  {
    "objectID": "spline_comparison_benchmark.html#hazard-function-comparison",
    "href": "spline_comparison_benchmark.html#hazard-function-comparison",
    "title": "Spline Methods Comparison",
    "section": "Hazard Function Comparison",
    "text": "Hazard Function Comparison\n\n\nShow code\n# Evaluation grid\neval_times &lt;- seq(0.01, max_time, length.out = 200)\n\n# True hazard\nh_true &lt;- true_hazard(eval_times)\n\n# mgcv/pammtools predictions - use pammtools helper\nnewdata_mgcv &lt;- data.frame(tend = eval_times)\nh_mgcv_gcv &lt;- predict(fit_gcv, newdata = newdata_mgcv, type = \"response\") / \n              mean(diff(ped$tend[1:2]))  # Hazard = rate / interval_length\nh_mgcv_reml &lt;- predict(fit_reml, newdata = newdata_mgcv, type = \"response\") /\n               mean(diff(ped$tend[1:2]))\n\n# Simpler: use pammtools add_hazard for proper hazard extraction\nhazard_mgcv_gcv &lt;- add_hazard(newdata_mgcv, fit_gcv)\nhazard_mgcv_reml &lt;- add_hazard(newdata_mgcv, fit_reml)\n\n# flexsurv hazard\nh_flex &lt;- summary(fit_flex, t = eval_times, type = \"hazard\")[[1]]$est\n\n# Julia curves from CSV (all methods including weighted)\njulia_h &lt;- julia_curves %&gt;% filter(method == \"Julia PIJCV\") %&gt;% pull(hazard)\njulia_h_efs &lt;- julia_curves %&gt;% filter(method == \"Julia EFS\") %&gt;% pull(hazard)\njulia_h_cv10 &lt;- julia_curves %&gt;% filter(method == \"Julia CV10\") %&gt;% pull(hazard)\njulia_h_weighted &lt;- julia_curves %&gt;% filter(method == \"Julia PIJCV (weighted)\") %&gt;% pull(hazard)\njulia_h_learn_alpha &lt;- julia_curves %&gt;% filter(method == \"Julia PIJCV (learn Œ±)\") %&gt;% pull(hazard)\n\n# survstan hazard - extract Weibull parameters and compute hazard\n# survstan Weibull PH: h(t) = (alpha/gamma) * (t/gamma)^(alpha-1)\nh_survstan &lt;- (survstan_alpha / survstan_gamma) * (eval_times / survstan_gamma)^(survstan_alpha - 1)\n\n# Combine for plotting (including weighted methods)\nhazard_df &lt;- data.frame(\n  time = rep(eval_times, 9),\n  hazard = c(h_true, julia_h, julia_h_efs, julia_h_cv10, \n             julia_h_weighted, julia_h_learn_alpha,\n             hazard_mgcv_gcv$hazard, h_flex, h_survstan),\n  method = factor(rep(c(\"True (Weibull)\", \"Julia PIJCV\", \"Julia EFS\", \"Julia CV10\",\n                        \"Julia PIJCV (weighted)\", \"Julia PIJCV (learn Œ±)\",\n                        \"mgcv GCV\", \"flexsurv\", \"survstan\"), \n                      each = length(eval_times)),\n                  levels = c(\"True (Weibull)\", \"Julia PIJCV\", \"Julia EFS\", \"Julia CV10\",\n                             \"Julia PIJCV (weighted)\", \"Julia PIJCV (learn Œ±)\",\n                             \"mgcv GCV\", \"flexsurv\", \"survstan\"))\n)\n\nggplot(hazard_df, aes(x = time, y = hazard, color = method, linetype = method)) +\n  geom_line(linewidth = 1) +\n  geom_rug(data = surv_data %&gt;% filter(status == 1), \n           aes(x = time), inherit.aes = FALSE, \n           color = \"gray30\", alpha = 0.5, sides = \"b\") +\n  scale_color_manual(values = c(\"True (Weibull)\" = \"black\", \n                                 \"Julia PIJCV\" = \"#D55E00\",\n                                 \"Julia EFS\" = \"#56B4E9\",\n                                 \"Julia CV10\" = \"#F0E442\",\n                                 \"Julia PIJCV (weighted)\" = \"#0072B2\",\n                                 \"Julia PIJCV (learn Œ±)\" = \"#CC79A7\",\n                                 \"mgcv GCV\" = \"#E69F00\",\n                                 \"flexsurv\" = \"#009E73\",\n                                 \"survstan\" = \"#999999\")) +\n  scale_linetype_manual(values = c(\"True (Weibull)\" = \"solid\",\n                                    \"Julia PIJCV\" = \"dashed\",\n                                    \"Julia EFS\" = \"twodash\",\n                                    \"Julia CV10\" = \"dashed\",\n                                    \"Julia PIJCV (weighted)\" = \"dotted\",\n                                    \"Julia PIJCV (learn Œ±)\" = \"dotted\",\n                                    \"mgcv GCV\" = \"dotdash\",\n                                    \"flexsurv\" = \"solid\",\n                                    \"survstan\" = \"longdash\")) +\n  labs(\n    title = \"Hazard Function Estimates\",\n    subtitle = paste0(\"True: Weibull(shape=\", true_shape, \", rate=\", true_rate, \"); rug = event times\"),\n    x = \"Time\",\n    y = \"Hazard h(t)\",\n    color = \"Method\",\n    linetype = \"Method\"\n  ) +\n  theme(legend.position = \"bottom\") +\n  guides(color = guide_legend(nrow = 3), linetype = guide_legend(nrow = 3))"
  },
  {
    "objectID": "spline_comparison_benchmark.html#cumulative-hazard-comparison",
    "href": "spline_comparison_benchmark.html#cumulative-hazard-comparison",
    "title": "Spline Methods Comparison",
    "section": "Cumulative Hazard Comparison",
    "text": "Cumulative Hazard Comparison\n\n\nShow code\n# True cumulative hazard\nH_true &lt;- true_cumhaz(eval_times)\n\n# Julia cumulative hazard from CSV (all methods including weighted)\njulia_H &lt;- julia_curves %&gt;% filter(method == \"Julia PIJCV\") %&gt;% pull(cumhaz)\njulia_H_efs &lt;- julia_curves %&gt;% filter(method == \"Julia EFS\") %&gt;% pull(cumhaz)\njulia_H_cv10 &lt;- julia_curves %&gt;% filter(method == \"Julia CV10\") %&gt;% pull(cumhaz)\njulia_H_weighted &lt;- julia_curves %&gt;% filter(method == \"Julia PIJCV (weighted)\") %&gt;% pull(cumhaz)\njulia_H_learn_alpha &lt;- julia_curves %&gt;% filter(method == \"Julia PIJCV (learn Œ±)\") %&gt;% pull(cumhaz)\n\n# mgcv cumulative hazard - compute by numerical integration of hazard\nH_mgcv_gcv &lt;- cumsum(hazard_mgcv_gcv$hazard) * diff(eval_times)[1]\n\n# flexsurv cumulative hazard\nH_flex &lt;- summary(fit_flex, t = eval_times, type = \"cumhaz\")[[1]]$est\n\n# survstan cumulative hazard - Weibull: H(t) = (t/gamma)^alpha\nH_survstan &lt;- (eval_times / survstan_gamma)^survstan_alpha\n\n# Combine for plotting (including weighted methods)\ncumhaz_df &lt;- data.frame(\n  time = rep(eval_times, 9),\n  cumhaz = c(H_true, julia_H, julia_H_efs, julia_H_cv10,\n             julia_H_weighted, julia_H_learn_alpha,\n             H_mgcv_gcv, H_flex, H_survstan),\n  method = factor(rep(c(\"True (Weibull)\", \"Julia PIJCV\", \"Julia EFS\", \"Julia CV10\",\n                        \"Julia PIJCV (weighted)\", \"Julia PIJCV (learn Œ±)\",\n                        \"mgcv GCV\", \"flexsurv\", \"survstan\"), \n                      each = length(eval_times)),\n                  levels = c(\"True (Weibull)\", \"Julia PIJCV\", \"Julia EFS\", \"Julia CV10\",\n                             \"Julia PIJCV (weighted)\", \"Julia PIJCV (learn Œ±)\",\n                             \"mgcv GCV\", \"flexsurv\", \"survstan\"))\n)\n\nggplot(cumhaz_df, aes(x = time, y = cumhaz, color = method, linetype = method)) +\n  geom_line(linewidth = 1) +\n  scale_color_manual(values = c(\"True (Weibull)\" = \"black\", \n                                 \"Julia PIJCV\" = \"#D55E00\",\n                                 \"Julia EFS\" = \"#56B4E9\",\n                                 \"Julia CV10\" = \"#F0E442\",\n                                 \"Julia PIJCV (weighted)\" = \"#0072B2\",\n                                 \"Julia PIJCV (learn Œ±)\" = \"#CC79A7\",\n                                 \"mgcv GCV\" = \"#E69F00\",\n                                 \"flexsurv\" = \"#009E73\",\n                                 \"survstan\" = \"#999999\")) +\n  scale_linetype_manual(values = c(\"True (Weibull)\" = \"solid\",\n                                    \"Julia PIJCV\" = \"dashed\",\n                                    \"Julia EFS\" = \"twodash\",\n                                    \"Julia CV10\" = \"dashed\",\n                                    \"Julia PIJCV (weighted)\" = \"dotted\",\n                                    \"Julia PIJCV (learn Œ±)\" = \"dotted\",\n                                    \"mgcv GCV\" = \"dotdash\",\n                                    \"flexsurv\" = \"solid\",\n                                    \"survstan\" = \"longdash\")) +\n  labs(\n    title = \"Cumulative Hazard Estimates\",\n    x = \"Time\",\n    y = \"Cumulative Hazard H(t)\",\n    color = \"Method\",\n    linetype = \"Method\"\n  ) +\n  theme(legend.position = \"bottom\") +\n  guides(color = guide_legend(nrow = 3), linetype = guide_legend(nrow = 3))"
  },
  {
    "objectID": "spline_comparison_benchmark.html#survival-and-cumulative-incidence-curves",
    "href": "spline_comparison_benchmark.html#survival-and-cumulative-incidence-curves",
    "title": "Spline Methods Comparison",
    "section": "Survival and Cumulative Incidence Curves",
    "text": "Survival and Cumulative Incidence Curves\n\n\nShow code\n# True survival/CIF\nS_true &lt;- true_surv(eval_times)\nF_true &lt;- true_cdf(eval_times)\n\n# Julia survival from CSV (all methods including weighted)\njulia_S &lt;- julia_curves %&gt;% filter(method == \"Julia PIJCV\") %&gt;% pull(survival)\njulia_S_efs &lt;- julia_curves %&gt;% filter(method == \"Julia EFS\") %&gt;% pull(survival)\njulia_S_cv10 &lt;- julia_curves %&gt;% filter(method == \"Julia CV10\") %&gt;% pull(survival)\njulia_S_weighted &lt;- julia_curves %&gt;% filter(method == \"Julia PIJCV (weighted)\") %&gt;% pull(survival)\njulia_S_learn_alpha &lt;- julia_curves %&gt;% filter(method == \"Julia PIJCV (learn Œ±)\") %&gt;% pull(survival)\n\n# Julia CIF\nF_julia &lt;- 1 - julia_S\nF_julia_efs &lt;- 1 - julia_S_efs\nF_julia_cv10 &lt;- 1 - julia_S_cv10\nF_julia_weighted &lt;- 1 - julia_S_weighted\nF_julia_learn_alpha &lt;- 1 - julia_S_learn_alpha\n\n# mgcv survival - compute from cumulative hazard (S = exp(-H))\nS_mgcv_gcv &lt;- exp(-H_mgcv_gcv)\nF_mgcv_gcv &lt;- 1 - S_mgcv_gcv\n\n# flexsurv survival\nS_flex &lt;- summary(fit_flex, t = eval_times, type = \"survival\")[[1]]$est\nF_flex &lt;- 1 - S_flex\n\n# Color palette for all methods (including weighted)\nmethod_colors &lt;- c(\"True (Weibull)\" = \"black\", \n                   \"Julia PIJCV\" = \"#D55E00\",\n                   \"Julia EFS\" = \"#56B4E9\",\n                   \"Julia CV10\" = \"#F0E442\",\n                   \"Julia PIJCV (weighted)\" = \"#0072B2\",\n                   \"Julia PIJCV (learn Œ±)\" = \"#CC79A7\",\n                   \"mgcv GCV\" = \"#E69F00\",\n                   \"flexsurv\" = \"#009E73\")\n\nmethod_linetypes &lt;- c(\"True (Weibull)\" = \"solid\",\n                      \"Julia PIJCV\" = \"dashed\",\n                      \"Julia EFS\" = \"twodash\",\n                      \"Julia CV10\" = \"dashed\",\n                      \"Julia PIJCV (weighted)\" = \"dotted\",\n                      \"Julia PIJCV (learn Œ±)\" = \"dotted\",\n                      \"mgcv GCV\" = \"dotdash\",\n                      \"flexsurv\" = \"solid\")\n\nmethod_levels &lt;- c(\"True (Weibull)\", \"Julia PIJCV\", \"Julia EFS\", \"Julia CV10\",\n                   \"Julia PIJCV (weighted)\", \"Julia PIJCV (learn Œ±)\",\n                   \"mgcv GCV\", \"flexsurv\")\n\n# Combine survival curves (including weighted methods)\nsurv_df &lt;- data.frame(\n  time = rep(eval_times, 8),\n  survival = c(S_true, julia_S, julia_S_efs, julia_S_cv10,\n               julia_S_weighted, julia_S_learn_alpha,\n               S_mgcv_gcv, S_flex),\n  method = factor(rep(method_levels, each = length(eval_times)), levels = method_levels)\n)\n\n# Combine CIF curves (including weighted methods)\ncif_df &lt;- data.frame(\n  time = rep(eval_times, 8),\n  cif = c(F_true, F_julia, F_julia_efs, F_julia_cv10,\n          F_julia_weighted, F_julia_learn_alpha,\n          F_mgcv_gcv, F_flex),\n  method = factor(rep(method_levels, each = length(eval_times)), levels = method_levels)\n)\n\n# Kaplan-Meier for reference\nkm_fit &lt;- survfit(Surv(time, status) ~ 1, data = surv_data)\n\n# Plot survival\np_surv &lt;- ggplot(surv_df, aes(x = time, y = survival, color = method, linetype = method)) +\n  geom_line(linewidth = 1) +\n  geom_step(data = data.frame(time = km_fit$time, survival = km_fit$surv),\n            aes(x = time, y = survival), \n            inherit.aes = FALSE, color = \"gray50\", alpha = 0.5, linewidth = 0.8) +\n  scale_color_manual(values = method_colors) +\n  scale_linetype_manual(values = method_linetypes) +\n  labs(\n    title = \"Survival Function S(t)\",\n    subtitle = \"Gray step function = Kaplan-Meier estimate\",\n    x = \"Time\",\n    y = \"S(t)\",\n    color = \"Method\",\n    linetype = \"Method\"\n  ) +\n  theme(legend.position = \"bottom\") +\n  guides(color = guide_legend(nrow = 3), linetype = guide_legend(nrow = 3))\n\n# Plot CIF\np_cif &lt;- ggplot(cif_df, aes(x = time, y = cif, color = method, linetype = method)) +\n  geom_line(linewidth = 1) +\n  geom_step(data = data.frame(time = km_fit$time, cif = 1 - km_fit$surv),\n            aes(x = time, y = cif), \n            inherit.aes = FALSE, color = \"gray50\", alpha = 0.5, linewidth = 0.8) +\n  geom_rug(data = surv_data %&gt;% filter(status == 1), \n           aes(x = time), inherit.aes = FALSE, \n           color = \"gray30\", alpha = 0.5, sides = \"b\") +\n  scale_color_manual(values = method_colors) +\n  scale_linetype_manual(values = method_linetypes) +\n  labs(\n    title = \"Cumulative Incidence F(t) = 1 - S(t)\",\n    subtitle = \"Gray step function = 1 - Kaplan-Meier; rug = event times\",\n    x = \"Time\",\n    y = \"F(t)\",\n    color = \"Method\",\n    linetype = \"Method\"\n  ) +\n  theme(legend.position = \"bottom\") +\n  guides(color = guide_legend(nrow = 3), linetype = guide_legend(nrow = 3))\n\np_surv / p_cif"
  },
  {
    "objectID": "spline_comparison_benchmark.html#accuracy-metrics",
    "href": "spline_comparison_benchmark.html#accuracy-metrics",
    "title": "Spline Methods Comparison",
    "section": "Accuracy Metrics",
    "text": "Accuracy Metrics\n\n\nShow code\n# RMSE function\nrmse &lt;- function(true, est) sqrt(mean((true - est)^2))\n\n# Get all Julia methods from CSV\njulia_methods &lt;- unique(julia_curves$method)\n\n# Calculate RMSE for all Julia methods\njulia_metrics &lt;- lapply(julia_methods, function(m) {\n  h &lt;- julia_curves %&gt;% filter(method == m) %&gt;% pull(hazard)\n  H &lt;- julia_curves %&gt;% filter(method == m) %&gt;% pull(cumhaz)\n  S &lt;- julia_curves %&gt;% filter(method == m) %&gt;% pull(survival)\n  \n  data.frame(\n    Package = \"MultistateModels.jl\",\n    Method = gsub(\"Julia \", \"\", m),\n    Hazard_RMSE = rmse(h_true, h),\n    CumHaz_RMSE = rmse(H_true, H),\n    Survival_RMSE = rmse(S_true, S)\n  )\n}) %&gt;% bind_rows()\n\n# Add R package results\nr_metrics &lt;- data.frame(\n  Package = c(\"mgcv\", \"flexsurv\"),\n  Method = c(\"GCV.Cp\", \"spline\"),\n  Hazard_RMSE = c(rmse(h_true, hazard_mgcv_gcv$hazard), rmse(h_true, h_flex)),\n  CumHaz_RMSE = c(rmse(H_true, H_mgcv_gcv), rmse(H_true, H_flex)),\n  Survival_RMSE = c(rmse(S_true, S_mgcv_gcv), rmse(S_true, S_flex))\n)\n\n# Combine and order\nmetrics_df &lt;- rbind(julia_metrics, r_metrics)\n\n# Order Julia methods logically (including weighted methods)\njulia_order &lt;- c(\"PIJCV\", \"PIJCV10\", \"EFS\", \"CV10\", \"PIJCV (weighted)\", \"PIJCV (learn Œ±)\")\nmetrics_df$Method &lt;- factor(metrics_df$Method, \n                            levels = c(julia_order, \"GCV.Cp\", \"spline\"))\nmetrics_df &lt;- metrics_df %&gt;% arrange(Method)\n\n# Dynamically determine row indices\nn_julia &lt;- nrow(julia_metrics)\nn_r &lt;- nrow(r_metrics)\n\nkable(metrics_df, digits = 5,\n      caption = \"RMSE vs True Weibull Functions\",\n      col.names = c(\"Package\", \"Method\", \"Hazard\", \"Cum. Hazard\", \"Survival\")) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\")) %&gt;%\n  pack_rows(\"MultistateModels.jl\", 1, n_julia) %&gt;%\n  pack_rows(\"R\", n_julia + 1, n_julia + n_r)\n\n\n\nRMSE vs True Weibull Functions\n\n\nPackage\nMethod\nHazard\nCum. Hazard\nSurvival\n\n\n\n\nMultistateModels.jl\n\n\nMultistateModels.jl\nPIJCV\n0.04077\n0.02283\n0.00331\n\n\nMultistateModels.jl\nPIJCV10\n0.04077\n0.02283\n0.00331\n\n\nMultistateModels.jl\nEFS\n0.04908\n0.01803\n0.00299\n\n\nMultistateModels.jl\nCV10\n0.03838\n0.02817\n0.00359\n\n\nMultistateModels.jl\nPIJCV (weighted)\n0.01740\n0.01171\n0.00302\n\n\nMultistateModels.jl\nPIJCV (learn Œ±)\n0.02143\n0.01699\n0.00275\n\n\nR\n\n\nmgcv\nGCV.Cp\n0.08838\n0.03175\n0.00307\n\n\nflexsurv\nspline\n0.01826\n0.02112\n0.00206"
  },
  {
    "objectID": "spline_comparison_benchmark.html#runtime-comparison",
    "href": "spline_comparison_benchmark.html#runtime-comparison",
    "title": "Spline Methods Comparison",
    "section": "Runtime Comparison",
    "text": "Runtime Comparison\nA key advantage of Newton-approximated cross-validation methods (PIJCV, PIJCV10) is computational efficiency. While exact k-fold CV requires refitting the model K times, the Newton approximation achieves similar results in a single optimization.\nWeighted penalty methods add additional overhead to compute the at-risk counts and weighted penalty matrices, but this cost is typically small compared to the fitting itself.\n\n\nShow code\n# Check if runtime data is available\nif (\"runtime_sec\" %in% names(julia_summary)) {\n  \n  # Categorize methods (including weighted)\n  julia_summary$category &lt;- case_when(\n    julia_summary$method %in% c(\"PIJCV\", \"PIJCV10\") ~ \"Newton CV (uniform)\",\n    julia_summary$method %in% c(\"PIJCV_weighted\", \"PIJCV_learn_alpha\") ~ \"Newton CV (weighted)\",\n    julia_summary$method %in% c(\"CV10\") ~ \"Exact CV\",\n    TRUE ~ \"Other (EFS)\"\n  )\n  \n  # Order methods by category (including weighted methods)\n  method_order &lt;- c(\"PIJCV\", \"PIJCV10\", \n                    \"PIJCV_weighted\", \"PIJCV_learn_alpha\",\n                    \"EFS\",\n                    \"CV10\")\n  julia_summary$method &lt;- factor(julia_summary$method, levels = method_order)\n  \n  # Create bar plot\n  p_runtime &lt;- ggplot(julia_summary, aes(x = method, y = runtime_sec, fill = category)) +\n    geom_col(width = 0.7) +\n    geom_text(aes(label = sprintf(\"%.2fs\", runtime_sec)), \n              vjust = -0.5, size = 3) +\n    scale_fill_manual(values = c(\n      \"Newton CV (uniform)\" = \"#3498db\",\n      \"Newton CV (weighted)\" = \"#9b59b6\",\n      \"Exact CV\" = \"#e74c3c\",\n      \"Other (EFS)\" = \"#2ecc71\"\n    )) +\n    labs(\n      title = \"Runtime Comparison: Smoothing Parameter Selection Methods\",\n      subtitle = \"Including weighted penalty methods (n=1000 subjects, 5 interior knots)\",\n      x = \"Method\",\n      y = \"Runtime (seconds)\",\n      fill = \"Category\"\n    ) +\n    theme_minimal() +\n    theme(\n      axis.text.x = element_text(angle = 45, hjust = 1),\n      legend.position = \"bottom\"\n    )\n  \n  print(p_runtime)\n  \n  # Runtime summary table\n  runtime_summary &lt;- julia_summary %&gt;%\n    group_by(category) %&gt;%\n    summarise(\n      Methods = paste(method, collapse = \", \"),\n      Mean_Runtime = mean(runtime_sec),\n      Min_Runtime = min(runtime_sec),\n      Max_Runtime = max(runtime_sec),\n      .groups = \"drop\"\n    ) %&gt;%\n    arrange(Mean_Runtime)\n  \n  kable(runtime_summary, digits = 3,\n        caption = \"Runtime Summary by Method Category\",\n        col.names = c(\"Category\", \"Methods\", \"Mean (s)\", \"Min (s)\", \"Max (s)\")) %&gt;%\n    kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n  \n} else {\n  cat(\"Runtime data not available in julia_summary.\\n\")\n}\n\n\n\n\n\n\n\n\n\n\nRuntime Summary by Method Category\n\n\nCategory\nMethods\nMean (s)\nMin (s)\nMax (s)\n\n\n\n\nOther (EFS)\nEFS\n2.089\n2.089\n2.089\n\n\nNewton CV (uniform)\nPIJCV, PIJCV10\n15.076\n12.907\n17.246\n\n\nExact CV\nCV10\n16.118\n16.118\n16.118\n\n\nNewton CV (weighted)\nPIJCV_weighted, PIJCV_learn_alpha\n23.910\n12.854\n34.966\n\n\n\n\n\n\n\nShow code\nif (\"runtime_sec\" %in% names(julia_summary)) {\n  # Add category if not present (from previous chunk)\n  if (!\"category\" %in% names(julia_summary)) {\n    julia_summary$category &lt;- case_when(\n      julia_summary$method %in% c(\"PIJCV\", \"PIJCV10\") ~ \"Newton CV (uniform)\",\n      julia_summary$method %in% c(\"PIJCV_weighted\", \"PIJCV_learn_alpha\") ~ \"Newton CV (weighted)\",\n      julia_summary$method %in% c(\"CV10\") ~ \"Exact CV\",\n      TRUE ~ \"Other\"\n    )\n  }\n  \n  # Full runtime table with all metrics\n  runtime_full &lt;- julia_summary %&gt;%\n    select(method, category, lambda, edf, loglik, runtime_sec) %&gt;%\n    arrange(runtime_sec)\n  \n  kable(runtime_full, digits = c(0, 0, 2, 2, 2, 3),\n        caption = \"Full Results: All Julia Smoothing Methods (sorted by runtime)\",\n        col.names = c(\"Method\", \"Category\", \"Œª\", \"EDF\", \"Log-Lik\", \"Runtime (s)\")) %&gt;%\n    kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n}\n\n\n\nFull Results: All Julia Smoothing Methods (sorted by runtime)\n\n\nMethod\nCategory\nŒª\nEDF\nLog-Lik\nRuntime (s)\n\n\n\n\nEFS\nOther (EFS)\n0.73\n6.28\n-1554.80\n2.089\n\n\nPIJCV_weighted\nNewton CV (weighted)\n2.66\n4.14\n-1555.26\n12.854\n\n\nPIJCV\nNewton CV (uniform)\n1.30\n5.89\n-1556.82\n12.907\n\n\nCV10\nExact CV\n1.65\n5.73\n-1558.01\n16.118\n\n\nPIJCV10\nNewton CV (uniform)\n1.30\n5.89\n-1556.82\n17.246\n\n\nPIJCV_learn_alpha\nNewton CV (weighted)\n2.66\n4.81\n-1554.30\n34.966\n\n\n\n\n\nKey Runtime Observations:\n\nNewton-approximated methods (PIJCV, PIJCV10) achieve comparable accuracy to exact CV while requiring only a single model fit with analytical gradients\nExact k-fold CV (CV10) requires K model refits, making it substantially slower\nEFS is fast as it uses a simple closed-form approximation\nThe computational advantage of Newton approximation grows with sample size and model complexity\n\nWhy PIJCV and CV10 have similar runtimes:\nInterestingly, PIJCV (Newton-approximated LOO-CV) and CV10 (exact 10-fold CV) may show similar runtimes in this benchmark. This occurs because:\n\nPIJCV‚Äôs overhead: PIJCV requires computing and storing subject-level Hessians (\\(n \\times p \\times p\\) matrices) and performing Cholesky downdates for each subject. For \\(n=1000\\) subjects, this is non-trivial memory and computation.\nCV10‚Äôs efficiency: Exact 10-fold CV only requires 10 model refits, and each refit benefits from warm-starting at the previous solution.\nSmall model size: With only 5 interior knots (~8 spline coefficients), the per-iteration cost is low, so the overhead of PIJCV is relatively larger.\n\nWhen PIJCV shines: The Newton approximation advantage grows dramatically with: - Larger sample sizes (\\(n &gt; 5000\\)) - More complex models (multiple transitions, many spline coefficients) - Higher-dimensional smoothing parameter selection (multiple Œª values)\nIn these settings, PIJCV can be 10-100√ó faster than exact CV."
  },
  {
    "objectID": "spline_comparison_benchmark.html#discussion",
    "href": "spline_comparison_benchmark.html#discussion",
    "title": "Spline Methods Comparison",
    "section": "Discussion",
    "text": "Discussion\n\nEDF vs Œª Scaling\nThe effective degrees of freedom (EDF) is the proper metric for comparing smoothing across packages. Despite significant differences in raw Œª values between Julia and mgcv, the EDF values are in good agreement, confirming that both approaches select similar model complexity.\nThe scale difference arises from:\n\nEffective sample size: Julia uses n=1000 subjects; mgcv‚Äôs PAM uses many pseudo-observations\nPenalty matrix normalization: Different scaling conventions\nLikelihood formulation: Exact survival vs Poisson pseudo-likelihood\n\n\n\nSoftware Packages Compared\n\n\n\n\n\n\n\n\nPackage\nApproach\nNotes\n\n\n\n\nMultistateModels.jl\nExact penalized likelihood with P-splines\nMultiple smoothing methods (PIJCV, PIJCV10, EFS, CV10)\n\n\nmgcv/pammtools\nPAM (Poisson GAM on piecewise-exponential data)\nGCV.Cp, REML for smoothing selection\n\n\nflexsurv\nSpline hazard via ML (no explicit smoothing parameter)\nUses natural splines, not penalized\n\n\n\n\n\nWhy flexsurv Appears to Fit Better on Weibull Data\nflexsurv‚Äôs excellent performance on this Weibull simulation is not because it‚Äôs universally superior‚Äîit‚Äôs because the Weibull hazard is perfectly suited to flexsurv‚Äôs parameterization:\n\n\n\n\n\n\n\n\n\nAspect\nMultistateModels.jl\nmgcv (pammtools)\nflexsurv\n\n\n\n\nWhat‚Äôs modeled\nh(t) directly\nlog(rate) via Poisson\nlog H(t)\n\n\nTime scale\nLinear time\nLinear time\nLog time\n\n\nPenalty\n2nd-order difference\n2nd-order difference\nNone (MLE)\n\n\n\nFor a Weibull hazard \\(h(t) = \\kappa\\lambda(\\lambda t)^{\\kappa-1}\\): - The log cumulative hazard is: \\(\\log H(t) = \\kappa \\log \\lambda + \\kappa \\log t\\) - This is exactly linear in log(t)‚Äîtrivially easy for flexsurv‚Äôs spline!\nMeanwhile, MultistateModels.jl and mgcv model \\(h(t)\\) on linear time, where Weibull is a power function requiring more basis functions to approximate.\nPart 3 tests a bathtub-shaped hazard that challenges all methods equally."
  },
  {
    "objectID": "spline_comparison_benchmark.html#true-model-specification-illness-death",
    "href": "spline_comparison_benchmark.html#true-model-specification-illness-death",
    "title": "Spline Methods Comparison",
    "section": "True Model Specification (Illness-Death)",
    "text": "True Model Specification (Illness-Death)\nData are simulated from Weibull hazards with transition-specific parameters:\n\n\n\nTransition\nShape (\\(\\kappa\\))\nRate (\\(\\lambda\\))\nHazard Pattern\n\n\n\n\n1‚Üí2\n1.3\n0.25\nModerately increasing\n\n\n1‚Üí3\n0.8\n0.15\nDecreasing (competing risk)\n\n\n2‚Üí3\n1.5\n0.35\nStrongly increasing\n\n\n\n\\[h_{12}(t) = 1.3 \\times 0.25 \\times t^{0.3} = 0.325 t^{0.3}\\] \\[h_{13}(t) = 0.8 \\times 0.15 \\times t^{-0.2} = 0.12 t^{-0.2}\\] \\[h_{23}(t) = 1.5 \\times 0.35 \\times t^{0.5} = 0.525 t^{0.5}\\]"
  },
  {
    "objectID": "spline_comparison_benchmark.html#illness-death-data-simulation-and-julia-fitting",
    "href": "spline_comparison_benchmark.html#illness-death-data-simulation-and-julia-fitting",
    "title": "Spline Methods Comparison",
    "section": "Illness-Death Data Simulation and Julia Fitting",
    "text": "Illness-Death Data Simulation and Julia Fitting\n\n\nShow code\ncd ../..\njulia --project=. -e '\nusing MultistateModels\nusing Random\nusing DataFrames\nusing CSV\nusing Printf\nusing Distributions\n\n# Configuration\nn = 1000\nmax_time = 6.0\nseed = 12345\n\n# True Weibull parameters (shape, rate) for each transition\n# Using MultistateModels convention: h(t) = shape * rate * t^(shape-1)\ntrue_params = (\n    h12 = (shape = 1.3, rate = 0.25),  # Moderately increasing\n    h13 = (shape = 0.8, rate = 0.15),  # Decreasing (competing risk)\n    h23 = (shape = 1.5, rate = 0.35)   # Strongly increasing\n)\n\nRandom.seed!(seed)\n\n# Simulate illness-death model\n# For each subject, we need to simulate the competing process\n\nfunction weibull_cdf(t, shape, rate)\n    return 1.0 - exp(-rate * t^shape)\nend\n\nfunction weibull_quantile(p, shape, rate)\n    return (-log(1.0 - p) / rate)^(1.0 / shape)\nend\n\n# Simulate event times\nsurvival_data = DataFrame[]\n\nfor i in 1:n\n    # Generate latent event times from state 1\n    U12 = rand()\n    U13 = rand()\n    \n    # Weibull event times from state 1\n    T12 = weibull_quantile(U12, true_params.h12.shape, true_params.h12.rate)\n    T13 = weibull_quantile(U13, true_params.h13.shape, true_params.h13.rate)\n    \n    # Determine first event from state 1\n    if min(T12, T13) &gt;= max_time\n        # Administrative censoring in state 1\n        push!(survival_data, DataFrame(\n            id = i,\n            tstart = 0.0,\n            tstop = max_time,\n            statefrom = 1,\n            stateto = 1,\n            obstype = 1\n        ))\n    elseif T12 &lt; T13\n        # Transition to illness (state 2)\n        t_illness = T12\n        \n        # Now generate death time from state 2\n        # Time to death from illness follows Weibull with params h23\n        U23 = rand()\n        T23_residual = weibull_quantile(U23, true_params.h23.shape, true_params.h23.rate)\n        T_death = t_illness + T23_residual\n        \n        if T_death &gt;= max_time\n            # Illness then administrative censoring\n            push!(survival_data, DataFrame(\n                id = i,\n                tstart = 0.0,\n                tstop = t_illness,\n                statefrom = 1,\n                stateto = 2,\n                obstype = 1\n            ))\n            push!(survival_data, DataFrame(\n                id = i,\n                tstart = t_illness,\n                tstop = max_time,\n                statefrom = 2,\n                stateto = 2,\n                obstype = 1\n            ))\n        else\n            # Illness then death\n            push!(survival_data, DataFrame(\n                id = i,\n                tstart = 0.0,\n                tstop = t_illness,\n                statefrom = 1,\n                stateto = 2,\n                obstype = 1\n            ))\n            push!(survival_data, DataFrame(\n                id = i,\n                tstart = t_illness,\n                tstop = T_death,\n                statefrom = 2,\n                stateto = 3,\n                obstype = 1\n            ))\n        end\n    else\n        # Direct death from state 1 (h13)\n        push!(survival_data, DataFrame(\n            id = i,\n            tstart = 0.0,\n            tstop = T13,\n            statefrom = 1,\n            stateto = 3,\n            obstype = 1\n        ))\n    end\nend\n\nsurv_data = vcat(survival_data...)\n\n# Count transitions\nn_12 = sum((surv_data.statefrom .== 1) .& (surv_data.stateto .== 2))\nn_13 = sum((surv_data.statefrom .== 1) .& (surv_data.stateto .== 3))\nn_23 = sum((surv_data.statefrom .== 2) .& (surv_data.stateto .== 3))\nn_cens_1 = sum((surv_data.statefrom .== 1) .& (surv_data.stateto .== 1))\nn_cens_2 = sum((surv_data.statefrom .== 2) .& (surv_data.stateto .== 2))\n\nprintln(\"=== Illness-Death Data ===\")\nprintln(\"n = $n subjects\")\nprintln(\"Transitions 1‚Üí2 (illness): $n_12\")\nprintln(\"Transitions 1‚Üí3 (direct death): $n_13\")\nprintln(\"Transitions 2‚Üí3 (death after illness): $n_23\")\nprintln(\"Censored in state 1: $n_cens_1\")\nprintln(\"Censored in state 2: $n_cens_2\")\n\n# Extract event times for each transition\nevent_times_12 = surv_data[(surv_data.statefrom .== 1) .& (surv_data.stateto .== 2), :tstop]\nevent_times_13 = surv_data[(surv_data.statefrom .== 1) .& (surv_data.stateto .== 3), :tstop]\nevent_times_23 = surv_data[(surv_data.statefrom .== 2) .& (surv_data.stateto .== 3), :tstop] .- \n                 surv_data[(surv_data.statefrom .== 2) .& (surv_data.stateto .== 3), :tstart]\n\n# For R export, we need the raw survival data\n# Convert to standard multistate format\nCSV.write(\"MultistateModelsTests/reports/_id_surv_data.csv\", surv_data)\n\n# Define model with spline hazards\n# Using monotone=0 for unconstrained (non-monotonic) spline - tests general flexibility\nh12 = Hazard(@formula(0 ~ 1), \"sp\", 1, 2;\n             degree = 3,\n             knots = [max_time/2],  # Placeholder - replaced by calibrate_splines!\n             boundaryknots = [0.0, max_time],\n             natural_spline = true,\n             monotone = 0)  # Unconstrained spline\nh13 = Hazard(@formula(0 ~ 1), \"sp\", 1, 3;\n             degree = 3,\n             knots = [max_time/2],  # Placeholder - replaced by calibrate_splines!\n             boundaryknots = [0.0, max_time],\n             natural_spline = true,\n             monotone = 0)  # Unconstrained spline\nh23 = Hazard(@formula(0 ~ 1), \"sp\", 2, 3;\n             degree = 3,\n             knots = [max_time/2],  # Placeholder - replaced by calibrate_splines!\n             boundaryknots = [0.0, max_time],\n             natural_spline = true,\n             monotone = 0)  # Unconstrained spline\n\nmodel = multistatemodel(h12, h13, h23; data=surv_data)\n\n# Calibrate knots based on event times\nprintln(\"\\nCalibrating spline knots...\")\nknot_result = calibrate_splines!(model; nknots=5, verbose=true)\n\n# Extract knots for each hazard\nknots_12 = knot_result.h12\nknots_13 = knot_result.h13\nknots_23 = knot_result.h23\n\nprintln(\"\\n=== Knot Configuration ===\")\nprintln(\"h12: boundary=$(knots_12.boundary_knots), interior=$(round.(knots_12.interior_knots, digits=3))\")\nprintln(\"h13: boundary=$(knots_13.boundary_knots), interior=$(round.(knots_13.interior_knots, digits=3))\")\nprintln(\"h23: boundary=$(knots_23.boundary_knots), interior=$(round.(knots_23.interior_knots, digits=3))\")\n\n# Save knots for R\nknots_df = DataFrame(\n    transition = repeat([\"h12\", \"h13\", \"h23\"], inner=5),\n    interior_knot_idx = repeat(1:5, 3),\n    interior_knot = vcat(knots_12.interior_knots, knots_13.interior_knots, knots_23.interior_knots),\n    boundary_lower = fill(0.0, 15),\n    boundary_upper = fill(max_time, 15)\n)\nCSV.write(\"MultistateModelsTests/reports/_id_knots.csv\", knots_df)\n\nprintln(\"\\nData generation complete. Knots and data saved for R.\")\n'\n\n\n\n\nShow code\ncd ../..\njulia --project=. -e '\nusing MultistateModels\nusing MultistateModels: get_parameters_flat, get_loglik\nusing Random\nusing DataFrames\nusing CSV\nusing Printf\nusing Distributions\n\n# Configuration (must match data generation)\nn = 1000\nmax_time = 6.0\nseed = 12345\n\ntrue_params = (\n    h12 = (shape = 1.3, rate = 0.25),\n    h13 = (shape = 0.8, rate = 0.15),\n    h23 = (shape = 1.5, rate = 0.35)\n)\n\nRandom.seed!(seed)\n\n# Helper functions\nfunction weibull_quantile(p, shape, rate)\n    return (-log(1.0 - p) / rate)^(1.0 / shape)\nend\n\n# Regenerate survival data\nsurvival_data = DataFrame[]\nfor i in 1:n\n    U12 = rand()\n    U13 = rand()\n    T12 = weibull_quantile(U12, true_params.h12.shape, true_params.h12.rate)\n    T13 = weibull_quantile(U13, true_params.h13.shape, true_params.h13.rate)\n    \n    if min(T12, T13) &gt;= max_time\n        push!(survival_data, DataFrame(id = i, tstart = 0.0, tstop = max_time, statefrom = 1, stateto = 1, obstype = 1))\n    elseif T12 &lt; T13\n        t_illness = T12\n        U23 = rand()\n        T23_residual = weibull_quantile(U23, true_params.h23.shape, true_params.h23.rate)\n        T_death = t_illness + T23_residual\n        if T_death &gt;= max_time\n            push!(survival_data, DataFrame(id = i, tstart = 0.0, tstop = t_illness, statefrom = 1, stateto = 2, obstype = 1))\n            push!(survival_data, DataFrame(id = i, tstart = t_illness, tstop = max_time, statefrom = 2, stateto = 2, obstype = 1))\n        else\n            push!(survival_data, DataFrame(id = i, tstart = 0.0, tstop = t_illness, statefrom = 1, stateto = 2, obstype = 1))\n            push!(survival_data, DataFrame(id = i, tstart = t_illness, tstop = T_death, statefrom = 2, stateto = 3, obstype = 1))\n        end\n    else\n        push!(survival_data, DataFrame(id = i, tstart = 0.0, tstop = T13, statefrom = 1, stateto = 3, obstype = 1))\n    end\nend\nsurv_data = vcat(survival_data...)\n\n# Extract event times for each transition (for rug plots later)\nevent_times_12 = surv_data[(surv_data.statefrom .== 1) .& (surv_data.stateto .== 2), :tstop]\nevent_times_13 = surv_data[(surv_data.statefrom .== 1) .& (surv_data.stateto .== 3), :tstop]\n\n# Define and calibrate model\n# Using monotone=0 for unconstrained (non-monotonic) spline - tests general flexibility\nh12 = Hazard(@formula(0 ~ 1), \"sp\", 1, 2; degree = 3, knots = [max_time/2], boundaryknots = [0.0, max_time], natural_spline = true, monotone = 0)\nh13 = Hazard(@formula(0 ~ 1), \"sp\", 1, 3; degree = 3, knots = [max_time/2], boundaryknots = [0.0, max_time], natural_spline = true, monotone = 0)\nh23 = Hazard(@formula(0 ~ 1), \"sp\", 2, 3; degree = 3, knots = [max_time/2], boundaryknots = [0.0, max_time], natural_spline = true, monotone = 0)\nmodel = multistatemodel(h12, h13, h23; data=surv_data)\ncalibrate_splines!(model; nknots=5, verbose=false)\n\n# Helper to get total EDF - edf is (total=Float64, per_term=Vector{Float64})\ntotal_edf(fitted) = fitted.edf.total\n\n# Fit with each smoothing method using new API\nprintln(\"=== Fitting with smoothing methods ===\")\n\nprintln(\"\\nFitting with PIJCV method...\")\nfitted_pijcv = fit(model; penalty=:auto, select_lambda=:pijcv, vcov_type=:none, verbose=false)\nprintln(\"  PIJCV Œª = $(round.(fitted_pijcv.smoothing_parameters, digits=2)), EDF = $(round(total_edf(fitted_pijcv), digits=2))\")\n\nprintln(\"\\nFitting with EFS method...\")\nfitted_efs = fit(model; penalty=:auto, select_lambda=:efs, vcov_type=:none, verbose=false)\nprintln(\"  EFS Œª = $(round.(fitted_efs.smoothing_parameters, digits=2)), EDF = $(round(total_edf(fitted_efs), digits=2))\")\n\nprintln(\"\\nFitting with 10-fold CV method...\")\nfitted_cv10 = fit(model; penalty=:auto, select_lambda=:cv10, vcov_type=:none, verbose=false)\nprintln(\"  CV10 Œª = $(round.(fitted_cv10.smoothing_parameters, digits=2)), EDF = $(round(total_edf(fitted_cv10), digits=2))\")\n\n# Evaluation grid\neval_times = collect(range(0.01, max_time, length=200))\n\n# Function to evaluate hazard curves for all transitions\nfunction evaluate_id_curves(model, beta, eval_times)\n    results = Dict{String, NamedTuple}()\n    \n    current_idx = 1\n    for (idx, haz) in enumerate(model.hazards)\n        trans_name = \"h$(haz.statefrom)$(haz.stateto)\"\n        \n        # Get parameter indices for this hazard\n        npars = length(haz.parnames)\n        start_idx = current_idx\n        end_idx = current_idx + npars - 1\n        beta_haz = beta[start_idx:end_idx]\n        current_idx = end_idx + 1\n        \n        hazard_vals = [haz.hazard_fn(t, beta_haz, ()) for t in eval_times]\n        cumhaz_vals = [haz.cumhaz_fn(0.0, t, beta_haz, ()) for t in eval_times]\n        \n        results[trans_name] = (hazard = hazard_vals, cumhaz = cumhaz_vals)\n    end\n    \n    return results\nend\n\n# Extract flat parameters from fitted models\nbeta_pijcv = get_parameters_flat(fitted_pijcv)\nbeta_efs = get_parameters_flat(fitted_efs)\nbeta_cv10 = get_parameters_flat(fitted_cv10)\n\n# Compute curves for each method\ncurves_pijcv = evaluate_id_curves(model, beta_pijcv, eval_times)\ncurves_efs = evaluate_id_curves(model, beta_efs, eval_times)\ncurves_cv10 = evaluate_id_curves(model, beta_cv10, eval_times)\n\n# Get log-likelihoods directly from fitted models\nloglik_pijcv = get_loglik(fitted_pijcv)\nloglik_efs = get_loglik(fitted_efs)\nloglik_cv10 = get_loglik(fitted_cv10)\n\n# Save curves to CSV\ncurves_list = DataFrame[]\nfor (method_name, curves) in [(\"Julia PIJCV\", curves_pijcv), \n                               (\"Julia EFS\", curves_efs),\n                               (\"Julia CV10\", curves_cv10)]\n    for trans in [\"h12\", \"h13\", \"h23\"]\n        push!(curves_list, DataFrame(\n            time = eval_times,\n            hazard = curves[trans].hazard,\n            cumhaz = curves[trans].cumhaz,\n            transition = trans,\n            method = method_name\n        ))\n    end\nend\njulia_curves = vcat(curves_list...)\nCSV.write(\"MultistateModelsTests/reports/_id_julia_curves.csv\", julia_curves)\n\n# Save summary with per-transition EDF\njulia_summary = DataFrame(\n    method = [\"PIJCV\", \"EFS\", \"CV10\"],\n    lambda = [fitted_pijcv.smoothing_parameters[1], fitted_efs.smoothing_parameters[1], fitted_cv10.smoothing_parameters[1]],\n    edf_total = [total_edf(fitted_pijcv), total_edf(fitted_efs), total_edf(fitted_cv10)],\n    loglik = [loglik_pijcv, loglik_efs, loglik_cv10]\n)\nCSV.write(\"MultistateModelsTests/reports/_id_julia_summary.csv\", julia_summary)\n\n# Save event times for rug plots\nevent_times_df = DataFrame(\n    time = Float64[],\n    transition = String[]\n)\nfor (times, trans) in [(event_times_12, \"h12\"), (event_times_13, \"h13\")]\n    append!(event_times_df, DataFrame(time = times, transition = fill(trans, length(times))))\nend\n# For h23, use the absolute death times (not duration in state 2)\ndeath_times_23 = surv_data[(surv_data.statefrom .== 2) .& (surv_data.stateto .== 3), :tstop]\nappend!(event_times_df, DataFrame(time = death_times_23, transition = fill(\"h23\", length(death_times_23))))\nCSV.write(\"MultistateModelsTests/reports/_id_event_times.csv\", event_times_df)\n\nprintln(\"\\nIllness-death curves computed and saved.\")\n'"
  },
  {
    "objectID": "spline_comparison_benchmark.html#load-illness-death-results-in-r",
    "href": "spline_comparison_benchmark.html#load-illness-death-results-in-r",
    "title": "Spline Methods Comparison",
    "section": "Load Illness-Death Results in R",
    "text": "Load Illness-Death Results in R\n\n\nShow code\n# Load simulated data\nid_surv_data &lt;- read.csv(\"_id_surv_data.csv\")\n\n# Load Julia curves\nid_julia_curves &lt;- read.csv(\"_id_julia_curves.csv\")\nid_julia_summary &lt;- read.csv(\"_id_julia_summary.csv\")\n\n# Load knots\nid_knots_df &lt;- read.csv(\"_id_knots.csv\")\nid_knots &lt;- id_knots_df %&gt;%\n  group_by(transition) %&gt;%\n  summarise(\n    interior_knots = list(interior_knot),\n    boundary_lower = first(boundary_lower),\n    boundary_upper = first(boundary_upper),\n    .groups = \"drop\"\n  )\n\n# Load event times for rug plots\nid_event_times &lt;- read.csv(\"_id_event_times.csv\")\n\n# True parameters\nid_true_params &lt;- list(\n  h12 = list(shape = 1.3, rate = 0.25),\n  h13 = list(shape = 0.8, rate = 0.15),\n  h23 = list(shape = 1.5, rate = 0.35)\n)\n\nid_max_time &lt;- 6.0\nid_n &lt;- length(unique(id_surv_data$id))\n\ncat(\"=== Illness-Death Julia Results ===\\n\")\nprint(id_julia_summary)\n\ncat(\"\\n=== Knot Configuration ===\\n\")\nfor (trans in c(\"h12\", \"h13\", \"h23\")) {\n  knots_trans &lt;- id_knots %&gt;% filter(transition == trans)\n  cat(trans, \": interior = [\", paste(round(unlist(knots_trans$interior_knots), 3), collapse=\", \"), \"]\\n\")\n}"
  },
  {
    "objectID": "spline_comparison_benchmark.html#true-hazard-functions-illness-death",
    "href": "spline_comparison_benchmark.html#true-hazard-functions-illness-death",
    "title": "Spline Methods Comparison",
    "section": "True Hazard Functions (Illness-Death)",
    "text": "True Hazard Functions (Illness-Death)\n\n\nShow code\n# True hazard functions for each transition\nid_true_hazard &lt;- function(t, trans) {\n  params &lt;- id_true_params[[trans]]\n  params$shape * params$rate * t^(params$shape - 1)\n}\n\nid_true_cumhaz &lt;- function(t, trans) {\n  params &lt;- id_true_params[[trans]]\n  params$rate * t^params$shape\n}"
  },
  {
    "objectID": "spline_comparison_benchmark.html#mgcvpammtools-fits-illness-death",
    "href": "spline_comparison_benchmark.html#mgcvpammtools-fits-illness-death",
    "title": "Spline Methods Comparison",
    "section": "mgcv/pammtools Fits (Illness-Death)",
    "text": "mgcv/pammtools Fits (Illness-Death)\nFor the illness-death model, we fit separate PAM models for each transition using pammtools, ensuring the same knots as Julia for each transition.\n\n\nShow code\nlibrary(survival)\n\n# Create transition-specific datasets for PAM fitting\n# For multi-state models, we need to filter to relevant observations\n\n# h12: Observations starting in state 1\nid_data_h12 &lt;- id_surv_data %&gt;%\n  filter(statefrom == 1) %&gt;%\n  mutate(\n    time = tstop - tstart,\n    status = as.integer(stateto == 2),\n    id_row = row_number()\n  )\n\n# h13: Same observations, different event indicator\nid_data_h13 &lt;- id_surv_data %&gt;%\n  filter(statefrom == 1) %&gt;%\n  mutate(\n    time = tstop - tstart,\n    status = as.integer(stateto == 3),\n    id_row = row_number()\n  )\n\n# h23: Observations starting in state 2\nid_data_h23 &lt;- id_surv_data %&gt;%\n  filter(statefrom == 2) %&gt;%\n  mutate(\n    time = tstop - tstart,\n    status = as.integer(stateto == 3),\n    id_row = row_number()\n  )\n\ncat(\"h12 data: n =\", nrow(id_data_h12), \", events =\", sum(id_data_h12$status), \"\\n\")\ncat(\"h13 data: n =\", nrow(id_data_h13), \", events =\", sum(id_data_h13$status), \"\\n\")\ncat(\"h23 data: n =\", nrow(id_data_h23), \", events =\", sum(id_data_h23$status), \"\\n\")\n\n# Function to build mgcv knots from Julia interior knots\nbuild_mgcv_knots &lt;- function(interior_knots, boundary_lower, boundary_upper) {\n  k_mgcv &lt;- length(interior_knots) + 4\n  m_order &lt;- 2\n  middle_knots &lt;- c(boundary_lower, interior_knots, boundary_upper)\n  delta &lt;- mean(diff(middle_knots))\n  padding_lower &lt;- boundary_lower - (3:1) * delta\n  padding_upper &lt;- boundary_upper + (1:3) * delta\n  c(padding_lower, middle_knots, padding_upper)\n}\n\n# Build knot vectors for each transition\nid_mgcv_knots &lt;- list()\nfor (trans in c(\"h12\", \"h13\", \"h23\")) {\n  knots_trans &lt;- id_knots %&gt;% filter(transition == trans)\n  id_mgcv_knots[[trans]] &lt;- build_mgcv_knots(\n    unlist(knots_trans$interior_knots),\n    knots_trans$boundary_lower,\n    knots_trans$boundary_upper\n  )\n}\n\nk_mgcv &lt;- 9  # 5 interior + 4\nm_order &lt;- 2\n\n# Fit PAM for h12\nped_h12 &lt;- as_ped(\n  formula = Surv(time, status) ~ 1,\n  data = id_data_h12,\n  id = \"id_row\"\n)\n\nfit_h12_gcv &lt;- pamm(\n  ped_status ~ s(tend, bs = \"ps\", k = k_mgcv, m = c(m_order, m_order)),\n  data = ped_h12,\n  method = \"GCV.Cp\",\n  knots = list(tend = id_mgcv_knots[[\"h12\"]])\n)\n\ncat(\"\\nh12 mgcv fit: EDF =\", round(sum(fit_h12_gcv$edf), 2), \"\\n\")\n\n# Fit PAM for h13\nped_h13 &lt;- as_ped(\n  formula = Surv(time, status) ~ 1,\n  data = id_data_h13,\n  id = \"id_row\"\n)\n\nfit_h13_gcv &lt;- pamm(\n  ped_status ~ s(tend, bs = \"ps\", k = k_mgcv, m = c(m_order, m_order)),\n  data = ped_h13,\n  method = \"GCV.Cp\",\n  knots = list(tend = id_mgcv_knots[[\"h13\"]])\n)\n\ncat(\"h13 mgcv fit: EDF =\", round(sum(fit_h13_gcv$edf), 2), \"\\n\")\n\n# Fit PAM for h23\nped_h23 &lt;- as_ped(\n  formula = Surv(time, status) ~ 1,\n  data = id_data_h23,\n  id = \"id_row\"\n)\n\nfit_h23_gcv &lt;- pamm(\n  ped_status ~ s(tend, bs = \"ps\", k = k_mgcv, m = c(m_order, m_order)),\n  data = ped_h23,\n  method = \"GCV.Cp\",\n  knots = list(tend = id_mgcv_knots[[\"h23\"]])\n)\n\ncat(\"h23 mgcv fit: EDF =\", round(sum(fit_h23_gcv$edf), 2), \"\\n\")"
  },
  {
    "objectID": "spline_comparison_benchmark.html#effective-degrees-of-freedom-illness-death",
    "href": "spline_comparison_benchmark.html#effective-degrees-of-freedom-illness-death",
    "title": "Spline Methods Comparison",
    "section": "Effective Degrees of Freedom (Illness-Death)",
    "text": "Effective Degrees of Freedom (Illness-Death)\n\n\nShow code\n# Collect EDF results for illness-death model\nid_edf_summary &lt;- rbind(\n  id_julia_summary %&gt;%\n    mutate(Package = \"MultistateModels.jl\") %&gt;%\n    select(Package, Method = method, EDF = edf_total, Lambda = lambda),\n  data.frame(\n    Package = rep(\"mgcv\", 3),\n    Method = c(\"GCV h12\", \"GCV h13\", \"GCV h23\"),\n    EDF = c(sum(fit_h12_gcv$edf), sum(fit_h13_gcv$edf), sum(fit_h23_gcv$edf)),\n    Lambda = c(fit_h12_gcv$sp, fit_h13_gcv$sp, fit_h23_gcv$sp)\n  )\n)\n\nkable(id_edf_summary, digits = 4,\n      caption = \"Effective Degrees of Freedom - Illness-Death Model\",\n      col.names = c(\"Package\", \"Method\", \"EDF (total)\", \"Œª\")) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\")) %&gt;%\n  column_spec(3, bold = TRUE, background = \"#f0f9e8\")\n\n\n\nEffective Degrees of Freedom - Illness-Death Model\n\n\nPackage\nMethod\nEDF (total)\nŒª\n\n\n\n\nMultistateModels.jl\nPIJCV\n14.5772\n5.5254\n\n\nMultistateModels.jl\nEFS\n14.3961\n6.0921\n\n\nMultistateModels.jl\nCV10\n22.8728\n0.0111\n\n\nmgcv\nGCV h12\n4.4375\n132.7369\n\n\nmgcv\nGCV h13\n3.0143\n589.8375\n\n\nmgcv\nGCV h23\n7.9204\n0.0921"
  },
  {
    "objectID": "spline_comparison_benchmark.html#model-selection-criteria-illness-death",
    "href": "spline_comparison_benchmark.html#model-selection-criteria-illness-death",
    "title": "Spline Methods Comparison",
    "section": "Model Selection Criteria (Illness-Death)",
    "text": "Model Selection Criteria (Illness-Death)\n\n\nShow code\nid_n_obs &lt;- id_n\n\n# Julia AIC/BIC\nid_julia_aic_bic &lt;- id_julia_summary %&gt;%\n  mutate(\n    Package = \"MultistateModels.jl\",\n    AIC = -2 * loglik + 2 * edf_total,\n    BIC = -2 * loglik + log(id_n_obs) * edf_total\n  ) %&gt;%\n  select(Package, Method = method, EDF = edf_total, LogLik = loglik, AIC, BIC)\n\nkable(id_julia_aic_bic, digits = 2,\n      caption = \"Model Selection Criteria - Illness-Death Model\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nModel Selection Criteria - Illness-Death Model\n\n\nPackage\nMethod\nEDF\nLogLik\nAIC\nBIC\n\n\n\n\nMultistateModels.jl\nPIJCV\n14.58\n-3121.12\n6271.39\n6342.93\n\n\nMultistateModels.jl\nEFS\n14.40\n-3121.65\n6272.09\n6342.74\n\n\nMultistateModels.jl\nCV10\n22.87\n-3107.69\n6261.13\n6373.38"
  },
  {
    "objectID": "spline_comparison_benchmark.html#hazard-function-comparison-illness-death",
    "href": "spline_comparison_benchmark.html#hazard-function-comparison-illness-death",
    "title": "Spline Methods Comparison",
    "section": "Hazard Function Comparison (Illness-Death)",
    "text": "Hazard Function Comparison (Illness-Death)\n\n\nShow code\n# Evaluation grid\nid_eval_times &lt;- seq(0.01, id_max_time, length.out = 200)\n\n# Get mgcv hazard predictions\nid_newdata &lt;- data.frame(tend = id_eval_times)\n\nhazard_mgcv_h12 &lt;- add_hazard(id_newdata, fit_h12_gcv)\nhazard_mgcv_h13 &lt;- add_hazard(id_newdata, fit_h13_gcv)\nhazard_mgcv_h23 &lt;- add_hazard(id_newdata, fit_h23_gcv)\n\n# Color palette\nid_method_colors &lt;- c(\"True (Weibull)\" = \"black\",\n                      \"Julia PIJCV\" = \"#D55E00\",\n                      \"Julia EFS\" = \"#56B4E9\",\n                      \"Julia CV10\" = \"#F0E442\",\n                      \"mgcv GCV\" = \"#E69F00\")\n\nid_method_linetypes &lt;- c(\"True (Weibull)\" = \"solid\",\n                         \"Julia PIJCV\" = \"dashed\",\n                         \"Julia EFS\" = \"twodash\",\n                         \"Julia CV10\" = \"dashed\",\n                         \"mgcv GCV\" = \"dotdash\")\n\n# Create hazard plots for each transition\ncreate_hazard_plot &lt;- function(trans, trans_label) {\n  # True hazard\n  h_true &lt;- id_true_hazard(id_eval_times, trans)\n  \n  # Julia hazards\n  julia_methods &lt;- c(\"Julia PIJCV\", \"Julia EFS\", \"Julia CV10\")\n  julia_hazards &lt;- lapply(julia_methods, function(m) {\n    id_julia_curves %&gt;%\n      filter(transition == trans, method == m) %&gt;%\n      pull(hazard)\n  })\n  \n  # mgcv hazard\n  mgcv_haz &lt;- switch(trans,\n    h12 = hazard_mgcv_h12$hazard,\n    h13 = hazard_mgcv_h13$hazard,\n    h23 = hazard_mgcv_h23$hazard\n  )\n  \n  # Combine data\n  hazard_df &lt;- data.frame(\n    time = rep(id_eval_times, 5),\n    hazard = c(h_true, unlist(julia_hazards), mgcv_haz),\n    method = factor(rep(c(\"True (Weibull)\", julia_methods, \"mgcv GCV\"), each = length(id_eval_times)),\n                    levels = c(\"True (Weibull)\", julia_methods, \"mgcv GCV\"))\n  )\n  \n  # Event times for this transition\n  event_times_trans &lt;- id_event_times %&gt;% filter(transition == trans) %&gt;% pull(time)\n  \n  ggplot(hazard_df, aes(x = time, y = hazard, color = method, linetype = method)) +\n    geom_line(linewidth = 1) +\n    geom_rug(data = data.frame(time = event_times_trans),\n             aes(x = time), inherit.aes = FALSE,\n             color = \"gray30\", alpha = 0.5, sides = \"b\") +\n    scale_color_manual(values = id_method_colors) +\n    scale_linetype_manual(values = id_method_linetypes) +\n    labs(\n      title = paste0(\"Hazard: \", trans_label),\n      x = \"Time\",\n      y = \"h(t)\",\n      color = \"Method\",\n      linetype = \"Method\"\n    ) +\n    theme(legend.position = \"none\")\n}\n\np_h12 &lt;- create_hazard_plot(\"h12\", \"Healthy ‚Üí Ill (h12)\")\np_h13 &lt;- create_hazard_plot(\"h13\", \"Healthy ‚Üí Dead (h13)\")\np_h23 &lt;- create_hazard_plot(\"h23\", \"Ill ‚Üí Dead (h23)\")\n\n# Combine with shared legend\n(p_h12 / p_h13 / p_h23) +\n  plot_layout(guides = \"collect\") &\n  theme(legend.position = \"bottom\") &\n  guides(color = guide_legend(nrow = 2), linetype = guide_legend(nrow = 2))"
  },
  {
    "objectID": "spline_comparison_benchmark.html#cumulative-hazard-comparison-illness-death",
    "href": "spline_comparison_benchmark.html#cumulative-hazard-comparison-illness-death",
    "title": "Spline Methods Comparison",
    "section": "Cumulative Hazard Comparison (Illness-Death)",
    "text": "Cumulative Hazard Comparison (Illness-Death)\n\n\nShow code\n# Create cumulative hazard plots for each transition\ncreate_cumhaz_plot &lt;- function(trans, trans_label) {\n  # True cumulative hazard\n  H_true &lt;- id_true_cumhaz(id_eval_times, trans)\n  \n  # Julia cumulative hazards\n  julia_methods &lt;- c(\"Julia PIJCV\", \"Julia EFS\", \"Julia CV10\")\n  julia_cumhaz &lt;- lapply(julia_methods, function(m) {\n    id_julia_curves %&gt;%\n      filter(transition == trans, method == m) %&gt;%\n      pull(cumhaz)\n  })\n  \n  # mgcv cumulative hazard (integrate hazard)\n  mgcv_haz &lt;- switch(trans,\n    h12 = hazard_mgcv_h12$hazard,\n    h13 = hazard_mgcv_h13$hazard,\n    h23 = hazard_mgcv_h23$hazard\n  )\n  mgcv_cumhaz &lt;- cumsum(mgcv_haz) * diff(id_eval_times)[1]\n  \n  # Combine data\n  cumhaz_df &lt;- data.frame(\n    time = rep(id_eval_times, 5),\n    cumhaz = c(H_true, unlist(julia_cumhaz), mgcv_cumhaz),\n    method = factor(rep(c(\"True (Weibull)\", julia_methods, \"mgcv GCV\"), each = length(id_eval_times)),\n                    levels = c(\"True (Weibull)\", julia_methods, \"mgcv GCV\"))\n  )\n  \n  ggplot(cumhaz_df, aes(x = time, y = cumhaz, color = method, linetype = method)) +\n    geom_line(linewidth = 1) +\n    scale_color_manual(values = id_method_colors) +\n    scale_linetype_manual(values = id_method_linetypes) +\n    labs(\n      title = paste0(\"Cumulative Hazard: \", trans_label),\n      x = \"Time\",\n      y = \"H(t)\",\n      color = \"Method\",\n      linetype = \"Method\"\n    ) +\n    theme(legend.position = \"none\")\n}\n\np_H12 &lt;- create_cumhaz_plot(\"h12\", \"Healthy ‚Üí Ill (h12)\")\np_H13 &lt;- create_cumhaz_plot(\"h13\", \"Healthy ‚Üí Dead (h13)\")\np_H23 &lt;- create_cumhaz_plot(\"h23\", \"Ill ‚Üí Dead (h23)\")\n\n(p_H12 / p_H13 / p_H23) +\n  plot_layout(guides = \"collect\") &\n  theme(legend.position = \"bottom\") &\n  guides(color = guide_legend(nrow = 2), linetype = guide_legend(nrow = 2))"
  },
  {
    "objectID": "spline_comparison_benchmark.html#accuracy-metrics-illness-death",
    "href": "spline_comparison_benchmark.html#accuracy-metrics-illness-death",
    "title": "Spline Methods Comparison",
    "section": "Accuracy Metrics (Illness-Death)",
    "text": "Accuracy Metrics (Illness-Death)\n\n\nShow code\n# RMSE function\nrmse &lt;- function(true, est) sqrt(mean((true - est)^2, na.rm = TRUE))\n\n# Calculate metrics for each transition and method\nid_metrics_list &lt;- list()\n\nfor (trans in c(\"h12\", \"h13\", \"h23\")) {\n  h_true &lt;- id_true_hazard(id_eval_times, trans)\n  H_true &lt;- id_true_cumhaz(id_eval_times, trans)\n  \n  # Julia methods (PIJCV, EFS, CV10)\n  for (method in c(\"PIJCV\", \"EFS\", \"CV10\")) {\n    julia_method &lt;- paste0(\"Julia \", method)\n    h_julia &lt;- id_julia_curves %&gt;%\n      filter(transition == trans, method == julia_method) %&gt;%\n      pull(hazard)\n    H_julia &lt;- id_julia_curves %&gt;%\n      filter(transition == trans, method == julia_method) %&gt;%\n      pull(cumhaz)\n    \n    id_metrics_list[[paste(trans, method)]] &lt;- data.frame(\n      Transition = trans,\n      Package = \"MultistateModels.jl\",\n      Method = method,\n      Hazard_RMSE = rmse(h_true, h_julia),\n      CumHaz_RMSE = rmse(H_true, H_julia)\n    )\n  }\n  \n  # mgcv\n  mgcv_haz &lt;- switch(trans,\n    h12 = hazard_mgcv_h12$hazard,\n    h13 = hazard_mgcv_h13$hazard,\n    h23 = hazard_mgcv_h23$hazard\n  )\n  mgcv_cumhaz &lt;- cumsum(mgcv_haz) * diff(id_eval_times)[1]\n  \n  id_metrics_list[[paste(trans, \"mgcv\")]] &lt;- data.frame(\n    Transition = trans,\n    Package = \"mgcv\",\n    Method = \"GCV.Cp\",\n    Hazard_RMSE = rmse(h_true, mgcv_haz),\n    CumHaz_RMSE = rmse(H_true, mgcv_cumhaz)\n  )\n}\n\nid_metrics_df &lt;- do.call(rbind, id_metrics_list)\n\nkable(id_metrics_df, digits = 5,\n      caption = \"RMSE vs True Weibull Functions - Illness-Death Model\",\n      col.names = c(\"Transition\", \"Package\", \"Method\", \"Hazard RMSE\", \"Cum. Hazard RMSE\")) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\")) %&gt;%\n  pack_rows(\"h12: Healthy ‚Üí Ill\", 1, 4) %&gt;%\n  pack_rows(\"h13: Healthy ‚Üí Dead\", 5, 8) %&gt;%\n  pack_rows(\"h23: Ill ‚Üí Dead\", 9, 12)\n\n\n\nRMSE vs True Weibull Functions - Illness-Death Model\n\n\n\nTransition\nPackage\nMethod\nHazard RMSE\nCum. Hazard RMSE\n\n\n\n\nh12: Healthy ‚Üí Ill\n\n\nh12 PIJCV\nh12\nMultistateModels.jl\nPIJCV\n0.03708\n0.04706\n\n\nh12 EFS\nh12\nMultistateModels.jl\nEFS\n0.03764\n0.04792\n\n\nh12 CV10\nh12\nMultistateModels.jl\nCV10\n0.03222\n0.03835\n\n\nh12 mgcv\nh12\nmgcv\nGCV.Cp\n0.03550\n0.03827\n\n\nh13: Healthy ‚Üí Dead\n\n\nh13 PIJCV\nh13\nMultistateModels.jl\nPIJCV\n0.01478\n0.03228\n\n\nh13 EFS\nh13\nMultistateModels.jl\nEFS\n0.01479\n0.03231\n\n\nh13 CV10\nh13\nMultistateModels.jl\nCV10\n0.01889\n0.04192\n\n\nh13 mgcv\nh13\nmgcv\nGCV.Cp\n0.01854\n0.04167\n\n\nh23: Ill ‚Üí Dead\n\n\nh23 PIJCV\nh23\nMultistateModels.jl\nPIJCV\n0.08487\n0.14318\n\n\nh23 EFS\nh23\nMultistateModels.jl\nEFS\n0.08687\n0.15494\n\n\nh23 CV10\nh23\nMultistateModels.jl\nCV10\n0.09648\n0.10808\n\n\nh23 mgcv\nh23\nmgcv\nGCV.Cp\n0.25521\n0.15932"
  },
  {
    "objectID": "spline_comparison_benchmark.html#discussion-illness-death-model",
    "href": "spline_comparison_benchmark.html#discussion-illness-death-model",
    "title": "Spline Methods Comparison",
    "section": "Discussion (Illness-Death Model)",
    "text": "Discussion (Illness-Death Model)\nThe illness-death model presents additional challenges compared to simple survival:\n\nCompeting risks: From state 1, subjects can transition to either state 2 (illness) or state 3 (death), requiring proper handling of censoring for each transition.\nTransition-specific hazards: Each transition has its own hazard function with different shapes, testing the flexibility of spline smoothing.\nData sparsity: Some transitions may have fewer events, affecting the stability of smoothing parameter selection.\n\nKey observations:\n\nAll Julia smoothing methods (PIJCV, PIJCV10, EFS, CV10) produce similar results\nEDF values are comparable between Julia and mgcv for each transition\nThe decreasing hazard (h13) is more challenging to capture than increasing hazards\nThe strongly increasing hazard (h23) is well-recovered by all methods"
  },
  {
    "objectID": "spline_comparison_benchmark.html#true-model-specification-bathtub",
    "href": "spline_comparison_benchmark.html#true-model-specification-bathtub",
    "title": "Spline Methods Comparison",
    "section": "True Model Specification (Bathtub)",
    "text": "True Model Specification (Bathtub)\nThe bathtub hazard is a classic pattern in reliability engineering, combining: - Early failures (infant mortality): Decreasing hazard - Random failures (useful life): Constant hazard\n- Wearout failures (aging): Increasing hazard\nWe use a mixture of Weibull distributions to create the bathtub shape: \\[h(t) = w_1 \\cdot h_1(t) + w_2 \\cdot h_2(t)\\]\nwhere: - \\(h_1(t)\\) = Weibull with shape &lt; 1 (decreasing, early failures) - \\(h_2(t)\\) = Weibull with shape &gt; 1 (increasing, wearout)\nParameters: - \\(h_1\\): shape = 0.5, rate = 0.8, weight = 0.3 - \\(h_2\\): shape = 2.5, rate = 0.15, weight = 0.7\n\n\nShow code\ncd ../..\njulia --project=. -e '\nusing MultistateModels\nusing Random\nusing DataFrames\nusing CSV\nusing Distributions\n\n# Configuration\nn = 1000\nmax_time = 5.0\nseed = 54321\nnknots = 8  # More knots for non-monotonic shape\n\nRandom.seed!(seed)\n\n# Bathtub hazard parameters\n# Component 1: decreasing (infant mortality)\nshape1, rate1, w1 = 0.5, 0.8, 0.3\n# Component 2: increasing (wearout)\nshape2, rate2, w2 = 2.5, 0.15, 0.7\n\n# Bathtub hazard function\nfunction bathtub_hazard(t)\n    h1 = shape1 * rate1 * (rate1 * t)^(shape1 - 1)\n    h2 = shape2 * rate2 * (rate2 * t)^(shape2 - 1)\n    return w1 * h1 + w2 * h2\nend\n\n# Bathtub cumulative hazard (numerical integration)\nfunction bathtub_cumhaz(t; nsteps=1000)\n    if t &lt;= 0\n        return 0.0\n    end\n    dt = t / nsteps\n    H = 0.0\n    for i in 1:nsteps\n        ti = (i - 0.5) * dt\n        H += bathtub_hazard(ti) * dt\n    end\n    return H\nend\n\n# Simulate event times via inversion (rejection sampling for efficiency)\nfunction simulate_bathtub_event(max_t=20.0)\n    # Use thinning algorithm with constant majorizing hazard\n    M = maximum(bathtub_hazard.(range(0.01, max_t, length=500)))\n    t = 0.0\n    while true\n        t += -log(rand()) / M  # Exponential with rate M\n        if t &gt; max_t\n            return max_t + 1.0  # Censored beyond max_t\n        end\n        # Accept with probability h(t)/M\n        if rand() &lt; bathtub_hazard(t) / M\n            return t\n        end\n    end\nend\n\n# Generate event times\nevent_times = [simulate_bathtub_event() for _ in 1:n]\nobs_times = min.(event_times, max_time)\nstatus = Int.(event_times .&lt;= max_time)\n\nprintln(\"=== Bathtub Hazard Data Summary ===\")\nprintln(\"Events: $(sum(status)) / $n ($(round(100*mean(status), digits=1))%)\")\nprintln(\"Median event time: $(round(median(obs_times[status .== 1]), digits=3))\")\n\n# Create survival data\nsurv_data = DataFrame(\n    id = 1:n,\n    tstart = zeros(n),\n    tstop = obs_times,\n    statefrom = ones(Int, n),\n    stateto = fill(2, n),\n    obstype = ones(Int, n)\n)\n\n# Save data\nCSV.write(\"MultistateModelsTests/reports/_bt_surv_data.csv\", DataFrame(time = obs_times, status = status))\n\n# Build model with spline hazard\nh12 = Hazard(@formula(0 ~ 1), \"sp\", 1, 2;\n             degree = 3,\n             knots = [max_time/2],\n             boundaryknots = [0.0, max_time],\n             natural_spline = true)\nmodel = multistatemodel(h12; data=surv_data)\n\n# Calibrate knots\nknot_result = calibrate_splines!(model; nknots=nknots, verbose=true)\ninterior_knots = knot_result.h12.interior_knots\nboundary_knots = knot_result.h12.boundary_knots\n\nprintln(\"\\nKnot Configuration:\")\nprintln(\"  Interior knots: $(round.(interior_knots, digits=3))\")\nprintln(\"  Boundary knots: $(boundary_knots)\")\n\n# Save knots\nCSV.write(\"MultistateModelsTests/reports/_bt_knots.csv\", DataFrame(\n    interior_knots = interior_knots,\n    boundary_lower = fill(boundary_knots[1], length(interior_knots)),\n    boundary_upper = fill(boundary_knots[2], length(interior_knots))\n))\n'\n\n\n\n\nShow code\ncd ../..\njulia --project=. -e '\nusing MultistateModels\nusing MultistateModels: get_parameters_flat, get_loglik\nusing Random\nusing DataFrames\nusing CSV\nusing Distributions\n\n# Configuration (must match data generation)\nn = 1000\nmax_time = 5.0\nseed = 54321\nnknots = 8\n\n# Bathtub parameters\nshape1, rate1, w1 = 0.5, 0.8, 0.3\nshape2, rate2, w2 = 2.5, 0.15, 0.7\n\nfunction bathtub_hazard(t)\n    h1 = shape1 * rate1 * (rate1 * t)^(shape1 - 1)\n    h2 = shape2 * rate2 * (rate2 * t)^(shape2 - 1)\n    return w1 * h1 + w2 * h2\nend\n\nfunction bathtub_cumhaz(t; nsteps=1000)\n    t &lt;= 0 && return 0.0\n    dt = t / nsteps\n    H = 0.0\n    for i in 1:nsteps\n        H += bathtub_hazard((i - 0.5) * dt) * dt\n    end\n    return H\nend\n\nfunction simulate_bathtub_event(max_t=20.0)\n    M = maximum(bathtub_hazard.(range(0.01, max_t, length=500)))\n    t = 0.0\n    while true\n        t += -log(rand()) / M\n        t &gt; max_t && return max_t + 1.0\n        rand() &lt; bathtub_hazard(t) / M && return t\n    end\nend\n\nRandom.seed!(seed)\nevent_times = [simulate_bathtub_event() for _ in 1:n]\nobs_times = min.(event_times, max_time)\nstatus = Int.(event_times .&lt;= max_time)\n\nsurv_data = DataFrame(\n    id = 1:n,\n    tstart = zeros(n),\n    tstop = obs_times,\n    statefrom = ones(Int, n),\n    stateto = fill(2, n),\n    obstype = ones(Int, n)\n)\n\n# Build and calibrate model\nh12 = Hazard(@formula(0 ~ 1), \"sp\", 1, 2;\n             degree = 3, knots = [max_time/2],\n             boundaryknots = [0.0, max_time], natural_spline = true)\nmodel = multistatemodel(h12; data=surv_data)\ncalibrate_splines!(model; nknots=nknots, verbose=false)\n\n# Helper functions\ntotal_edf(fitted) = fitted.edf === nothing ? NaN : fitted.edf.total\n\ntimings = Dict{String, Float64}()\n\n# Fit with PIJCV\nprintln(\"=== Fitting Bathtub Model ===\")\nprintln(\"\\nFitting with PIJCV...\")\nfit(model; penalty=:auto, select_lambda=:pijcv, vcov_type=:none, verbose=false)\nt_pijcv = @elapsed fitted_pijcv = fit(model; penalty=:auto, select_lambda=:pijcv, vcov_type=:none, verbose=false)\ntimings[\"PIJCV\"] = t_pijcv\nprintln(\"  PIJCV Œª = $(round(fitted_pijcv.smoothing_parameters[1], digits=4)), EDF = $(round(total_edf(fitted_pijcv), digits=2))\")\n\n# Fit with EFS\nprintln(\"\\nFitting with EFS...\")\nfit(model; penalty=:auto, select_lambda=:efs, vcov_type=:none, verbose=false)\nt_efs = @elapsed fitted_efs = fit(model; penalty=:auto, select_lambda=:efs, vcov_type=:none, verbose=false)\ntimings[\"EFS\"] = t_efs\nprintln(\"  EFS Œª = $(round(fitted_efs.smoothing_parameters[1], digits=4)), EDF = $(round(total_edf(fitted_efs), digits=2))\")\n\n# Fit with CV10\nprintln(\"\\nFitting with CV10...\")\nfit(model; penalty=:auto, select_lambda=:cv10, vcov_type=:none, verbose=false)\nt_cv10 = @elapsed fitted_cv10 = fit(model; penalty=:auto, select_lambda=:cv10, vcov_type=:none, verbose=false)\ntimings[\"CV10\"] = t_cv10\nprintln(\"  CV10 Œª = $(round(fitted_cv10.smoothing_parameters[1], digits=4)), EDF = $(round(total_edf(fitted_cv10), digits=2))\")\n\n# =============================================================================\n# WEIGHTED SMOOTHING METHODS (Bathtub)\n# =============================================================================\n# Weighted penalties are particularly relevant for non-monotonic hazards like\n# the bathtub, where the shape changes significantly over time.\n\n# PIJCV with at-risk weighting (fixed Œ±=1.0)\nprintln(\"\\nFitting with PIJCV + at-risk weighting (Œ±=1.0)...\")\nweighted_penalty = SplinePenalty(adaptive_weight=:atrisk, alpha=1.0)\nfit(model; penalty=weighted_penalty, select_lambda=:pijcv, vcov_type=:none, verbose=false)\nt_pijcv_w = @elapsed fitted_pijcv_w = fit(model; penalty=weighted_penalty, select_lambda=:pijcv, vcov_type=:none, verbose=false)\ntimings[\"PIJCV_weighted\"] = t_pijcv_w\nprintln(\"  PIJCV (weighted Œ±=1) Œª = $(round(fitted_pijcv_w.smoothing_parameters[1], digits=4)), EDF = $(round(total_edf(fitted_pijcv_w), digits=2))\")\n\n# PIJCV with learned Œ±\nprintln(\"\\nFitting with PIJCV + at-risk weighting (learn Œ±)...\")\nlearn_alpha_penalty = SplinePenalty(adaptive_weight=:atrisk, learn_alpha=true)\nfit(model; penalty=learn_alpha_penalty, select_lambda=:pijcv, vcov_type=:none, verbose=false)\nt_pijcv_la = @elapsed fitted_pijcv_la = fit(model; penalty=learn_alpha_penalty, select_lambda=:pijcv, vcov_type=:none, verbose=false)\ntimings[\"PIJCV_learn_alpha\"] = t_pijcv_la\nprintln(\"  PIJCV (learn Œ±) Œª = $(round(fitted_pijcv_la.smoothing_parameters[1], digits=4)), EDF = $(round(total_edf(fitted_pijcv_la), digits=2))\")\n\n# Evaluation grid\neval_times = collect(range(0.01, max_time, length=200))\n\nfunction evaluate_curves(model, beta, eval_times)\n    haz = model.hazards[1]\n    hazard_vals = [haz.hazard_fn(t, beta, ()) for t in eval_times]\n    cumhaz_vals = [haz.cumhaz_fn(0.0, t, beta, ()) for t in eval_times]\n    survival_vals = exp.(-cumhaz_vals)\n    return (hazard = hazard_vals, cumhaz = cumhaz_vals, survival = survival_vals)\nend\n\nbeta_pijcv = get_parameters_flat(fitted_pijcv)\nbeta_efs = get_parameters_flat(fitted_efs)\nbeta_cv10 = get_parameters_flat(fitted_cv10)\nbeta_pijcv_w = get_parameters_flat(fitted_pijcv_w)\nbeta_pijcv_la = get_parameters_flat(fitted_pijcv_la)\n\ncurves_pijcv = evaluate_curves(model, beta_pijcv, eval_times)\ncurves_efs = evaluate_curves(model, beta_efs, eval_times)\ncurves_cv10 = evaluate_curves(model, beta_cv10, eval_times)\ncurves_pijcv_w = evaluate_curves(model, beta_pijcv_w, eval_times)\ncurves_pijcv_la = evaluate_curves(model, beta_pijcv_la, eval_times)\n\n# Save results (including weighted methods)\njulia_results = DataFrame(\n    time = repeat(eval_times, 5),\n    hazard = vcat(curves_pijcv.hazard, curves_efs.hazard, curves_cv10.hazard,\n                  curves_pijcv_w.hazard, curves_pijcv_la.hazard),\n    cumhaz = vcat(curves_pijcv.cumhaz, curves_efs.cumhaz, curves_cv10.cumhaz,\n                  curves_pijcv_w.cumhaz, curves_pijcv_la.cumhaz),\n    survival = vcat(curves_pijcv.survival, curves_efs.survival, curves_cv10.survival,\n                    curves_pijcv_w.survival, curves_pijcv_la.survival),\n    method = repeat([\"Julia PIJCV\", \"Julia EFS\", \"Julia CV10\",\n                     \"Julia PIJCV (weighted)\", \"Julia PIJCV (learn Œ±)\"], inner=length(eval_times))\n)\nCSV.write(\"MultistateModelsTests/reports/_bt_julia_curves.csv\", julia_results)\n\njulia_summary = DataFrame(\n    method = [\"PIJCV\", \"EFS\", \"CV10\", \"PIJCV_weighted\", \"PIJCV_learn_alpha\"],\n    lambda = [fitted_pijcv.smoothing_parameters[1], fitted_efs.smoothing_parameters[1], \n              fitted_cv10.smoothing_parameters[1], fitted_pijcv_w.smoothing_parameters[1],\n              fitted_pijcv_la.smoothing_parameters[1]],\n    edf = [total_edf(fitted_pijcv), total_edf(fitted_efs), total_edf(fitted_cv10),\n           total_edf(fitted_pijcv_w), total_edf(fitted_pijcv_la)],\n    loglik = [get_loglik(fitted_pijcv), get_loglik(fitted_efs), get_loglik(fitted_cv10),\n              get_loglik(fitted_pijcv_w), get_loglik(fitted_pijcv_la)],\n    runtime_sec = [timings[\"PIJCV\"], timings[\"EFS\"], timings[\"CV10\"],\n                   timings[\"PIJCV_weighted\"], timings[\"PIJCV_learn_alpha\"]],\n    weighted = [false, false, false, true, true]\n)\nCSV.write(\"MultistateModelsTests/reports/_bt_julia_summary.csv\", julia_summary)\n\nprintln(\"\\nBathtub fitting complete.\")\n'\n\n\n\n\nShow code\n# Load bathtub data\nbt_surv_data &lt;- read.csv(\"_bt_surv_data.csv\")\nbt_julia_curves &lt;- read.csv(\"_bt_julia_curves.csv\")\nbt_julia_summary &lt;- read.csv(\"_bt_julia_summary.csv\")\nbt_knots &lt;- read.csv(\"_bt_knots.csv\")\n\nbt_interior_knots &lt;- bt_knots$interior_knots\nbt_boundary_lower &lt;- bt_knots$boundary_lower[1]\nbt_boundary_upper &lt;- bt_knots$boundary_upper[1]\nbt_max_time &lt;- bt_boundary_upper\n\n# Bathtub parameters\nbt_shape1 &lt;- 0.5; bt_rate1 &lt;- 0.8; bt_w1 &lt;- 0.3\nbt_shape2 &lt;- 2.5; bt_rate2 &lt;- 0.15; bt_w2 &lt;- 0.7\n\n\n\n\nShow code\n# True bathtub hazard\nbt_true_hazard &lt;- function(t) {\n  h1 &lt;- bt_shape1 * bt_rate1 * (bt_rate1 * t)^(bt_shape1 - 1)\n  h2 &lt;- bt_shape2 * bt_rate2 * (bt_rate2 * t)^(bt_shape2 - 1)\n  return(bt_w1 * h1 + bt_w2 * h2)\n}\n\n# True cumulative hazard (numerical integration)\nbt_true_cumhaz &lt;- function(t) {\n  if (t &lt;= 0) return(0)\n  integrate(bt_true_hazard, 0, t, subdivisions = 1000)$value\n}\nbt_true_cumhaz &lt;- Vectorize(bt_true_cumhaz)\n\n# True survival\nbt_true_survival &lt;- function(t) exp(-bt_true_cumhaz(t))\n\n\n\n\nShow code\n# PAM transformation\nbt_surv_data_ped &lt;- bt_surv_data %&gt;% mutate(id = row_number())\nbt_ped &lt;- as_ped(\n  formula = Surv(time, status) ~ 1,\n  data = bt_surv_data_ped,\n  id = \"id\"\n)\n\n# Knot setup for mgcv\nbt_k_mgcv &lt;- length(bt_interior_knots) + 4\nbt_m_order &lt;- 2\nbt_middle_knots &lt;- c(bt_boundary_lower, bt_interior_knots, bt_boundary_upper)\nbt_delta &lt;- mean(diff(bt_middle_knots))\nbt_padding_lower &lt;- bt_boundary_lower - (3:1) * bt_delta\nbt_padding_upper &lt;- bt_boundary_upper + (1:3) * bt_delta\nbt_mgcv_knots &lt;- c(bt_padding_lower, bt_middle_knots, bt_padding_upper)\n\n# Fit PAM\nbt_fit_gcv &lt;- pamm(\n  ped_status ~ s(tend, bs = \"ps\", k = bt_k_mgcv, m = c(bt_m_order, bt_m_order)),\n  data = bt_ped,\n  method = \"GCV.Cp\",\n  knots = list(tend = bt_mgcv_knots)\n)\n\ncat(\"mgcv EDF (bathtub):\", round(sum(bt_fit_gcv$edf), 2), \"\\n\")\n\n\n\n\nShow code\n# flexsurv with same interior knots (log-transformed)\nbt_log_interior_knots &lt;- log(bt_interior_knots)\n\nbt_fit_flex &lt;- flexsurvspline(\n  Surv(time, status) ~ 1,\n  data = bt_surv_data,\n  knots = bt_log_interior_knots,\n  scale = \"hazard\"\n)\n\ncat(\"flexsurv AIC (bathtub):\", round(AIC(bt_fit_flex), 2), \"\\n\")"
  },
  {
    "objectID": "spline_comparison_benchmark.html#hazard-comparison-bathtub",
    "href": "spline_comparison_benchmark.html#hazard-comparison-bathtub",
    "title": "Spline Methods Comparison",
    "section": "Hazard Comparison (Bathtub)",
    "text": "Hazard Comparison (Bathtub)\n\n\nShow code\nbt_eval_times &lt;- seq(0.05, bt_max_time, length.out = 200)\n\n# True hazard\nbt_h_true &lt;- bt_true_hazard(bt_eval_times)\n\n# Julia methods (including weighted)\nbt_julia_h_pijcv &lt;- bt_julia_curves %&gt;% filter(method == \"Julia PIJCV\") %&gt;% pull(hazard)\nbt_julia_h_efs &lt;- bt_julia_curves %&gt;% filter(method == \"Julia EFS\") %&gt;% pull(hazard)\nbt_julia_h_cv10 &lt;- bt_julia_curves %&gt;% filter(method == \"Julia CV10\") %&gt;% pull(hazard)\nbt_julia_h_weighted &lt;- bt_julia_curves %&gt;% filter(method == \"Julia PIJCV (weighted)\") %&gt;% pull(hazard)\nbt_julia_h_learn_alpha &lt;- bt_julia_curves %&gt;% filter(method == \"Julia PIJCV (learn Œ±)\") %&gt;% pull(hazard)\n\n# mgcv\nbt_newdata_mgcv &lt;- data.frame(tend = bt_eval_times)\nbt_hazard_mgcv &lt;- add_hazard(bt_newdata_mgcv, bt_fit_gcv)\nbt_h_mgcv &lt;- bt_hazard_mgcv$hazard\n\n# flexsurv\nbt_h_flex &lt;- summary(bt_fit_flex, t = bt_eval_times, type = \"hazard\")[[1]]$est\n\n# Combine for plotting (including weighted methods)\nbt_hazard_df &lt;- data.frame(\n  time = rep(bt_eval_times, 8),\n  hazard = c(bt_h_true, bt_julia_h_pijcv, bt_julia_h_efs, bt_julia_h_cv10,\n             bt_julia_h_weighted, bt_julia_h_learn_alpha,\n             bt_h_mgcv, bt_h_flex),\n  method = factor(rep(c(\"True (Bathtub)\", \"Julia PIJCV\", \"Julia EFS\", \"Julia CV10\",\n                        \"Julia PIJCV (weighted)\", \"Julia PIJCV (learn Œ±)\",\n                        \"mgcv GCV\", \"flexsurv\"), \n                      each = length(bt_eval_times)),\n                  levels = c(\"True (Bathtub)\", \"Julia PIJCV\", \"Julia EFS\", \"Julia CV10\",\n                             \"Julia PIJCV (weighted)\", \"Julia PIJCV (learn Œ±)\",\n                             \"mgcv GCV\", \"flexsurv\"))\n)\n\nggplot(bt_hazard_df, aes(x = time, y = hazard, color = method, linetype = method)) +\n  geom_line(linewidth = 1) +\n  geom_rug(data = bt_surv_data %&gt;% filter(status == 1), \n           aes(x = time), inherit.aes = FALSE, \n           color = \"gray30\", alpha = 0.5, sides = \"b\") +\n  scale_color_manual(values = c(\"True (Bathtub)\" = \"black\", \n                                 \"Julia PIJCV\" = \"#D55E00\",\n                                 \"Julia EFS\" = \"#56B4E9\",\n                                 \"Julia CV10\" = \"#F0E442\",\n                                 \"Julia PIJCV (weighted)\" = \"#0072B2\",\n                                 \"Julia PIJCV (learn Œ±)\" = \"#CC79A7\",\n                                 \"mgcv GCV\" = \"#E69F00\",\n                                 \"flexsurv\" = \"#009E73\")) +\n  scale_linetype_manual(values = c(\"True (Bathtub)\" = \"solid\",\n                                    \"Julia PIJCV\" = \"dashed\",\n                                    \"Julia EFS\" = \"twodash\",\n                                    \"Julia CV10\" = \"dashed\",\n                                    \"Julia PIJCV (weighted)\" = \"dotted\",\n                                    \"Julia PIJCV (learn Œ±)\" = \"dotted\",\n                                    \"mgcv GCV\" = \"dotdash\",\n                                    \"flexsurv\" = \"solid\")) +\n  labs(\n    title = \"Bathtub Hazard Function Estimates\",\n    subtitle = \"Non-monotonic hazard: early failures + wearout; rug = event times\",\n    x = \"Time\",\n    y = \"Hazard h(t)\",\n    color = \"Method\",\n    linetype = \"Method\"\n  ) +\n  theme(legend.position = \"bottom\") +\n  guides(color = guide_legend(nrow = 3), linetype = guide_legend(nrow = 3))"
  },
  {
    "objectID": "spline_comparison_benchmark.html#accuracy-metrics-bathtub",
    "href": "spline_comparison_benchmark.html#accuracy-metrics-bathtub",
    "title": "Spline Methods Comparison",
    "section": "Accuracy Metrics (Bathtub)",
    "text": "Accuracy Metrics (Bathtub)\n\n\nShow code\nbt_H_true &lt;- bt_true_cumhaz(bt_eval_times)\nbt_S_true &lt;- exp(-bt_H_true)\n\n# RMSE calculations (including weighted methods)\nbt_metrics &lt;- data.frame(\n  Package = c(rep(\"MultistateModels.jl\", 5), \"mgcv\", \"flexsurv\"),\n  Method = c(\"PIJCV\", \"EFS\", \"CV10\", \"PIJCV (weighted)\", \"PIJCV (learn Œ±)\", \"GCV.Cp\", \"spline\"),\n  Hazard_RMSE = c(\n    rmse(bt_h_true, bt_julia_h_pijcv),\n    rmse(bt_h_true, bt_julia_h_efs),\n    rmse(bt_h_true, bt_julia_h_cv10),\n    rmse(bt_h_true, bt_julia_h_weighted),\n    rmse(bt_h_true, bt_julia_h_learn_alpha),\n    rmse(bt_h_true, bt_h_mgcv),\n    rmse(bt_h_true, bt_h_flex)\n  )\n)\n\nkable(bt_metrics, digits = 5,\n      caption = \"RMSE vs True Bathtub Hazard Function\",\n      col.names = c(\"Package\", \"Method\", \"Hazard RMSE\")) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\")) %&gt;%\n  pack_rows(\"MultistateModels.jl\", 1, 5) %&gt;%\n  pack_rows(\"R Packages\", 6, 7)\n\n\n\nRMSE vs True Bathtub Hazard Function\n\n\nPackage\nMethod\nHazard RMSE\n\n\n\n\nMultistateModels.jl\n\n\nMultistateModels.jl\nPIJCV\n0.22660\n\n\nMultistateModels.jl\nEFS\n0.63472\n\n\nMultistateModels.jl\nCV10\n0.51682\n\n\nMultistateModels.jl\nPIJCV (weighted)\n0.38195\n\n\nMultistateModels.jl\nPIJCV (learn Œ±)\n0.36272\n\n\nR Packages\n\n\nmgcv\nGCV.Cp\n0.01568\n\n\nflexsurv\nspline\n0.01911"
  },
  {
    "objectID": "spline_comparison_benchmark.html#discussion-bathtub-hazard",
    "href": "spline_comparison_benchmark.html#discussion-bathtub-hazard",
    "title": "Spline Methods Comparison",
    "section": "Discussion (Bathtub Hazard)",
    "text": "Discussion (Bathtub Hazard)\nThe bathtub hazard provides a fairer comparison because:\n\nNon-monotonic shape: The bathtub curve has both decreasing and increasing regions, which is not easily represented on any particular scale.\nNo parameterization advantage: Unlike Weibull, there‚Äôs no scale where the bathtub hazard becomes linear‚Äîall methods must use multiple basis functions.\nReal-world relevance: Bathtub hazards are common in reliability engineering (early failures + useful life + wearout) and some medical contexts.\n\nKey observations:\n\nflexsurv no longer dominates: Without the Weibull‚Äôs log-linearity, flexsurv‚Äôs log(time) parameterization loses its advantage.\nAll penalized methods perform similarly: The RMSE values are comparable across Julia‚Äôs methods and mgcv, showing that penalized splines work well regardless of whether they model h(t) on linear time or log(cumulative hazard) on log(time).\nMore knots needed: We used 8 interior knots (vs 5 for Weibull) to capture the non-monotonic shape‚Äîthe penalty then prevents overfitting.\nWeighted penalties show similar performance: For this sample size (n=1000) and hazard shape, weighted penalties (adaptive_weight=:atrisk) produce comparable results to uniform penalties. Weighted penalties may show more benefit in scenarios with sparse data in the tails or rapidly declining at-risk populations.\nAlpha estimation: The learn_alpha=true option provides automatic tuning of the penalty weighting strength. In settings where Œ±=1 is not optimal, this can improve estimation quality at modest computational cost."
  },
  {
    "objectID": "01_architecture.html",
    "href": "01_architecture.html",
    "title": "Package Architecture & Workflow",
    "section": "",
    "text": "NoteStatus: Not Started\n\n\n\nThis report is a placeholder. Content will be added as the package documentation is developed."
  },
  {
    "objectID": "01_architecture.html#introduction",
    "href": "01_architecture.html#introduction",
    "title": "Package Architecture & Workflow",
    "section": "1. Introduction",
    "text": "1. Introduction\n\nDesign Philosophy\nMultistateModels.jl is designed to provide flexible tools for analyzing multistate survival data. The package supports:\n\nMultiple hazard families (Exponential, Weibull, Gompertz, B-splines)\nBoth proportional hazards (PH) and accelerated failure time (AFT) covariate effects\nExactly observed and panel (interval-censored) data\nTime-varying covariates\nSemi-Markov models with duration dependence\n\n\n\nPackage Overview\n\n\nShow code\n# Core workflow\nusing MultistateModels\n\n# 1. Define hazards\nhazards = (\n    h12 = Hazard(@formula(0 ~ x), \"wei\", 1, 2),\n    h23 = Hazard(@formula(0 ~ 1), \"exp\", 2, 3)\n)\n\n# 2. Build model\nmodel = multistatemodel(hazards; data = df)\n\n# 3. Fit model\nfit!(model)\n\n# 4. Simulate from fitted model\npaths = simulate(model; nsim = 1000)"
  },
  {
    "objectID": "01_architecture.html#model-construction",
    "href": "01_architecture.html#model-construction",
    "title": "Package Architecture & Workflow",
    "section": "2. Model Construction",
    "text": "2. Model Construction\n\n2.1 The MultistateProcess Struct\nTo be documented\n\n\n2.2 Hazard Specification\nTo be documented\n\n\n2.3 Parameterization Conventions\nAll hazard families follow flexsurv conventions:\n\n\n\n\n\n\n\n\nFamily\nHazard Function\nParameters\n\n\n\n\nExponential\n\\(h(t) = \\text{rate}\\)\nrate &gt; 0\n\n\nWeibull\n\\(h(t) = \\text{shape} \\times \\text{scale} \\times t^{\\text{shape}-1}\\)\nshape &gt; 0, scale &gt; 0\n\n\nGompertz\n\\(h(t) = \\text{rate} \\times \\exp(\\text{shape} \\times t)\\)\nshape ‚àà ‚Ñù, rate &gt; 0\n\n\nB-Spline\n\\(h(t) = \\exp(\\mathbf{B}(t)' \\boldsymbol{\\theta})\\)\nŒ∏ ‚àà ‚Ñù^k\n\n\n\n\n\n2.4 Covariate Effects\nTo be documented\n\n\n2.5 Time-Varying Covariates\nTo be documented\n\n\n2.6 The multistatemodel() Constructor\nTo be documented\n\n\n2.7 Data Requirements\nTo be documented"
  },
  {
    "objectID": "01_architecture.html#simulation-engine",
    "href": "01_architecture.html#simulation-engine",
    "title": "Package Architecture & Workflow",
    "section": "3. Simulation Engine",
    "text": "3. Simulation Engine\n\n3.1 simulate() vs simulate_path()\nTo be documented\n\n\n3.2 Transform Strategies\nTo be documented\n\n\n3.3 Jump Solvers\nTo be documented\n\n\n3.4 Phase-Type Expansion\nTo be documented"
  },
  {
    "objectID": "01_architecture.html#inference",
    "href": "01_architecture.html#inference",
    "title": "Package Architecture & Workflow",
    "section": "4. Inference",
    "text": "4. Inference\n\n4.1 Likelihood Calculations\nTo be documented\n\n\n4.2 MCEM Algorithm\nTo be documented\n\n\n4.3 Optimization Wrappers\nTo be documented\n\n\n4.4 Parameter Constraints\nTo be documented"
  },
  {
    "objectID": "01_architecture.html#key-internal-functions",
    "href": "01_architecture.html#key-internal-functions",
    "title": "Package Architecture & Workflow",
    "section": "5. Key Internal Functions",
    "text": "5. Key Internal Functions\n\n5.1 Hazard Evaluation\nTo be documented\n\n\n5.2 Survival Probability\nTo be documented\n\n\n5.3 Parameter Handling\nTo be documented\n\n\n5.4 Time Transforms\nTo be documented"
  },
  {
    "objectID": "01_architecture.html#option-reference",
    "href": "01_architecture.html#option-reference",
    "title": "Package Architecture & Workflow",
    "section": "6. Option Reference",
    "text": "6. Option Reference\nTo be documented"
  }
]