---
title: "Spline Methods Comparison"
subtitle: "Comparing MultistateModels.jl Splines vs mgcv, flexsurv, and survstan"
date: last-modified
format:
  html:
    toc: true
    toc-depth: 3
    code-fold: true
    fig-width: 10
    fig-height: 6
execute:
  warning: false
  message: false
  freeze: auto
engine: knitr
---

```{r}
#| label: r-setup
#| output: false

library(tidyverse)
library(mgcv)
library(pammtools)
library(flexsurv)
library(survival)
library(patchwork)
library(knitr)
library(kableExtra)
library(survstan)

# Set theme
theme_set(theme_bw(base_size = 12))
```

```{bash}
#| label: julia-data-generation
#| output: false

cd ../..
julia --project=. -e '
using MultistateModels
using Random
using DataFrames
using CSV
using Printf

# Configuration
n = 1000
true_shape = 1.5
true_rate = 0.3
max_time = 5.0
seed = 12345
nknots = 5  # Number of interior knots

Random.seed!(seed)

# Simulate Weibull survival data
E = -log.(rand(n))
event_times = (E ./ true_rate) .^ (1 / true_shape)
obs_times = min.(event_times, max_time)
status = Int.(event_times .<= max_time)

println("=== Simple Survival Data ===")
println("n = $n")
println("Events: $(sum(status))")
println("Censored: $(n - sum(status))")
println("Time range: [$(round(minimum(obs_times), digits=3)), $(round(maximum(obs_times), digits=3))]")

# Create multistate model data
surv_data = DataFrame(
    id = 1:n,
    tstart = zeros(n),
    tstop = obs_times,
    statefrom = ones(Int, n),
    stateto = ifelse.(status .== 1, 2, 1),
    obstype = ones(Int, n)
)

# Save to CSV for R to read
CSV.write("MultistateModelsTests/reports/_surv_data.csv", DataFrame(time = obs_times, status = status))

# Define model with spline hazard (placeholder knots will be replaced by calibration)
# Using monotone=0 for unconstrained (non-monotonic) spline - tests general flexibility
h12 = Hazard(@formula(0 ~ 1), "sp", 1, 2;
             degree = 3,
             knots = [max_time/2],  # Placeholder - will be replaced by calibrate_splines!
             boundaryknots = [0.0, max_time],
             natural_spline = true,
             monotone = 0)  # Unconstrained spline
model = multistatemodel(h12; data=surv_data)

# Calibrate knots based on data - places knots at quantiles of event times
println("\nCalibrating spline knots...")
knot_result = calibrate_splines!(model; nknots=nknots, verbose=true)

# Extract the calibrated knots
interior_knots = knot_result.h12.interior_knots
boundary_knots = knot_result.h12.boundary_knots
all_knots = model.hazards[1].knots

println("\n=== Knot Configuration ===")
println("Boundary knots: $(boundary_knots)")
println("Interior knots: $(round.(interior_knots, digits=3))")
println("Number of interior knots: $(length(interior_knots))")
println("Total knots: $(length(all_knots))")
println("Number of basis functions: $(length(model.hazards[1].parnames) - length(model.hazards[1].covar_names))")

# Save knots for R to use
CSV.write("MultistateModelsTests/reports/_knots.csv", DataFrame(
    interior_knots = interior_knots,
    boundary_lower = fill(boundary_knots[1], length(interior_knots)),
    boundary_upper = fill(boundary_knots[2], length(interior_knots))
))

println("\nData generation complete. Knots and data saved for R.")
'
```

```{bash}
#| label: julia-fitting
#| output: false

cd ../..
julia --project=. -e '
using MultistateModels
using MultistateModels: get_parameters_flat, get_loglik
using Random
using DataFrames
using CSV
using Printf

# Configuration (must match data generation)
n = 1000
true_shape = 1.5
true_rate = 0.3
max_time = 5.0
seed = 12345
nknots = 5

Random.seed!(seed)

# Regenerate the same data
E = -log.(rand(n))
event_times = (E ./ true_rate) .^ (1 / true_shape)
obs_times = min.(event_times, max_time)
status = Int.(event_times .<= max_time)

surv_data = DataFrame(
    id = 1:n,
    tstart = zeros(n),
    tstop = obs_times,
    statefrom = ones(Int, n),
    stateto = ifelse.(status .== 1, 2, 1),
    obstype = ones(Int, n)
)

# Define and calibrate model
# Using monotone=0 for unconstrained (non-monotonic) spline - tests general flexibility
h12 = Hazard(@formula(0 ~ 1), "sp", 1, 2;
             degree = 3,
             knots = [max_time/2],  # Placeholder - replaced by calibrate_splines!
             boundaryknots = [0.0, max_time],
             natural_spline = true,
             monotone = 0)  # Unconstrained spline
model = multistatemodel(h12; data=surv_data)
calibrate_splines!(model; nknots=nknots, verbose=false)

# Fit with each smoothing method and measure runtime
# NEW API: fit(model; penalty=:auto, select_lambda=:method, vcov_type=:none)
timings = Dict{String, Float64}()

println("=== Fitting Smoothing Methods with Timing ===")

# Helper to get total EDF - edf is (total=Float64, per_term=Vector{Float64})
total_edf(fitted) = fitted.edf.total

# PIJCV (LOO) - Newton-approximated
println("\nFitting with PIJCV (LOO) method...")
fit(model; penalty=:auto, select_lambda=:pijcv, vcov_type=:none, verbose=false)  # warmup
t_pijcv = @elapsed fitted_pijcv = fit(model; penalty=:auto, select_lambda=:pijcv, vcov_type=:none, verbose=false)
timings["PIJCV"] = t_pijcv
println("  PIJCV λ = $(round(fitted_pijcv.smoothing_parameters[1], digits=2)), EDF = $(round(total_edf(fitted_pijcv), digits=2)), time = $(round(t_pijcv, digits=3))s")

# PIJCV10 (10-fold) - Newton-approximated
println("\nFitting with PIJCV10 (10-fold) method...")
fit(model; penalty=:auto, select_lambda=:pijcv10, vcov_type=:none, verbose=false)  # warmup
t_pijcv10 = @elapsed fitted_pijcv10 = fit(model; penalty=:auto, select_lambda=:pijcv10, vcov_type=:none, verbose=false)
timings["PIJCV10"] = t_pijcv10
println("  PIJCV10 λ = $(round(fitted_pijcv10.smoothing_parameters[1], digits=2)), EDF = $(round(total_edf(fitted_pijcv10), digits=2)), time = $(round(t_pijcv10, digits=3))s")

# EFS
println("\nFitting with EFS method...")
fit(model; penalty=:auto, select_lambda=:efs, vcov_type=:none, verbose=false)  # warmup
t_efs = @elapsed fitted_efs = fit(model; penalty=:auto, select_lambda=:efs, vcov_type=:none, verbose=false)
timings["EFS"] = t_efs
println("  EFS λ = $(round(fitted_efs.smoothing_parameters[1], digits=2)), EDF = $(round(total_edf(fitted_efs), digits=2)), time = $(round(t_efs, digits=3))s")

# CV10 (exact 10-fold)
println("\nFitting with 10-fold CV (exact) method...")
fit(model; penalty=:auto, select_lambda=:cv10, vcov_type=:none, verbose=false)  # warmup
t_cv10 = @elapsed fitted_cv10 = fit(model; penalty=:auto, select_lambda=:cv10, vcov_type=:none, verbose=false)
timings["CV10"] = t_cv10
println("  CV10 λ = $(round(fitted_cv10.smoothing_parameters[1], digits=2)), EDF = $(round(total_edf(fitted_cv10), digits=2)), time = $(round(t_cv10, digits=3))s")

# =============================================================================
# WEIGHTED SMOOTHING METHODS
# =============================================================================
# Weighted penalties use w(t) = Y(t)^(-α) to penalize more where fewer subjects
# are at risk. This can improve estimation in regions with sparse data.

# PIJCV with at-risk weighting (fixed α=1.0)
println("\nFitting with PIJCV + at-risk weighting (α=1.0)...")
weighted_penalty = SplinePenalty(adaptive_weight=:atrisk, alpha=1.0)
fit(model; penalty=weighted_penalty, select_lambda=:pijcv, vcov_type=:none, verbose=false)  # warmup
t_pijcv_w = @elapsed fitted_pijcv_w = fit(model; penalty=weighted_penalty, select_lambda=:pijcv, vcov_type=:none, verbose=false)
timings["PIJCV_weighted"] = t_pijcv_w
println("  PIJCV (weighted α=1) λ = $(round(fitted_pijcv_w.smoothing_parameters[1], digits=2)), EDF = $(round(total_edf(fitted_pijcv_w), digits=2)), time = $(round(t_pijcv_w, digits=3))s")

# PIJCV with learned α (estimate optimal α from data)
println("\nFitting with PIJCV + at-risk weighting (learn α)...")
learn_alpha_penalty = SplinePenalty(adaptive_weight=:atrisk, learn_alpha=true)
fit(model; penalty=learn_alpha_penalty, select_lambda=:pijcv, vcov_type=:none, verbose=false)  # warmup
t_pijcv_la = @elapsed fitted_pijcv_la = fit(model; penalty=learn_alpha_penalty, select_lambda=:pijcv, vcov_type=:none, verbose=false)
timings["PIJCV_learn_alpha"] = t_pijcv_la
# Extract learned alpha from the penalty configuration if available
learned_alpha = hasfield(typeof(fitted_pijcv_la), :penalty_config) && 
                fitted_pijcv_la.penalty_config !== nothing ? 
                "α=$(round(fitted_pijcv_la.penalty_config.alpha, digits=2))" : "α=learned"
println("  PIJCV (learn α) λ = $(round(fitted_pijcv_la.smoothing_parameters[1], digits=2)), EDF = $(round(total_edf(fitted_pijcv_la), digits=2)), $learned_alpha, time = $(round(t_pijcv_la, digits=3))s")

# Evaluation grid
eval_times = collect(range(0.01, max_time, length=200))

# Function to evaluate hazard at a grid of times
function evaluate_curves(model, beta, eval_times)
    haz = model.hazards[1]
    hazard_vals = [haz.hazard_fn(t, beta, ()) for t in eval_times]
    cumhaz_vals = [haz.cumhaz_fn(0.0, t, beta, ()) for t in eval_times]
    survival_vals = exp.(-cumhaz_vals)
    return (hazard = hazard_vals, cumhaz = cumhaz_vals, survival = survival_vals)
end

# Extract flat parameters from fitted models
beta_pijcv = get_parameters_flat(fitted_pijcv)
beta_pijcv10 = get_parameters_flat(fitted_pijcv10)
beta_efs = get_parameters_flat(fitted_efs)
beta_cv10 = get_parameters_flat(fitted_cv10)
beta_pijcv_w = get_parameters_flat(fitted_pijcv_w)
beta_pijcv_la = get_parameters_flat(fitted_pijcv_la)

# Compute curves for each method
curves_pijcv = evaluate_curves(model, beta_pijcv, eval_times)
curves_pijcv10 = evaluate_curves(model, beta_pijcv10, eval_times)
curves_efs = evaluate_curves(model, beta_efs, eval_times)
curves_cv10 = evaluate_curves(model, beta_cv10, eval_times)
curves_pijcv_w = evaluate_curves(model, beta_pijcv_w, eval_times)
curves_pijcv_la = evaluate_curves(model, beta_pijcv_la, eval_times)

# Get log-likelihoods directly from fitted models
loglik_pijcv = get_loglik(fitted_pijcv)
loglik_pijcv10 = get_loglik(fitted_pijcv10)
loglik_efs = get_loglik(fitted_efs)
loglik_cv10 = get_loglik(fitted_cv10)
loglik_pijcv_w = get_loglik(fitted_pijcv_w)
loglik_pijcv_la = get_loglik(fitted_pijcv_la)

# Save Julia results for R plotting (include weighted methods)
julia_results = DataFrame(
    time = repeat(eval_times, 6),
    hazard = vcat(curves_pijcv.hazard, curves_pijcv10.hazard,
                  curves_efs.hazard, curves_cv10.hazard,
                  curves_pijcv_w.hazard, curves_pijcv_la.hazard),
    cumhaz = vcat(curves_pijcv.cumhaz, curves_pijcv10.cumhaz,
                  curves_efs.cumhaz, curves_cv10.cumhaz,
                  curves_pijcv_w.cumhaz, curves_pijcv_la.cumhaz),
    survival = vcat(curves_pijcv.survival, curves_pijcv10.survival,
                    curves_efs.survival, curves_cv10.survival,
                    curves_pijcv_w.survival, curves_pijcv_la.survival),
    method = repeat(["Julia PIJCV", "Julia PIJCV10",
                     "Julia EFS", "Julia CV10",
                     "Julia PIJCV (weighted)", "Julia PIJCV (learn α)"], inner=length(eval_times))
)
CSV.write("MultistateModelsTests/reports/_julia_curves.csv", julia_results)

# Save summary stats with log-likelihoods and timing
julia_summary = DataFrame(
    method = ["PIJCV", "PIJCV10", "EFS", "CV10", "PIJCV_weighted", "PIJCV_learn_alpha"],
    lambda = [fitted_pijcv.smoothing_parameters[1], fitted_pijcv10.smoothing_parameters[1],
              fitted_efs.smoothing_parameters[1], fitted_cv10.smoothing_parameters[1],
              fitted_pijcv_w.smoothing_parameters[1], fitted_pijcv_la.smoothing_parameters[1]],
    edf = [total_edf(fitted_pijcv), total_edf(fitted_pijcv10),
           total_edf(fitted_efs), total_edf(fitted_cv10),
           total_edf(fitted_pijcv_w), total_edf(fitted_pijcv_la)],
    loglik = [loglik_pijcv, loglik_pijcv10,
              loglik_efs, loglik_cv10,
              loglik_pijcv_w, loglik_pijcv_la],
    runtime_sec = [timings["PIJCV"], timings["PIJCV10"],
                   timings["EFS"], timings["CV10"],
                   timings["PIJCV_weighted"], timings["PIJCV_learn_alpha"]],
    weighted = [false, false, false, false, true, true]
)
CSV.write("MultistateModelsTests/reports/_julia_summary.csv", julia_summary)

println("\n=== Runtime Summary ===")
println("Newton-approximated CV methods (unweighted):")
println("  PIJCV (LOO): $(round(timings["PIJCV"], digits=3))s")
println("  PIJCV10:     $(round(timings["PIJCV10"], digits=3))s")
println("Other analytical methods:")
println("  EFS:         $(round(timings["EFS"], digits=3))s")
println("Exact CV methods:")
println("  CV10:        $(round(timings["CV10"], digits=3))s")
println("Weighted penalty methods:")
println("  PIJCV (α=1): $(round(timings["PIJCV_weighted"], digits=3))s")
println("  PIJCV (learn α): $(round(timings["PIJCV_learn_alpha"], digits=3))s")

println("\nJulia curves computed and saved.")
'
```

# Introduction

This report presents a comprehensive benchmark comparing spline-based survival models 
across different software packages and smoothing parameter selection methods.

## Comparison Overview

**Packages:**

1. **MultistateModels.jl** (Julia): Penalized likelihood with P-splines using the 
   **General P-Spline (GPS)** penalty from Li & Cao (2022), which correctly handles 
   non-uniform knot spacing via weighted differences
2. **mgcv** (R): Generalized additive models via piecewise-exponential/Poisson
3. **flexsurv** (R): Flexible parametric survival models with spline hazards on log-time scale
4. **survstan** (R): Parametric survival models via Stan optimization (Weibull comparison)

**Smoothing Parameter Selection Methods:**

| Package | Method | Description | Reference |
|---------|--------|-------------|-----------|
| MultistateModels.jl | PIJCV | Newton-approx LOO CV | Wood (2024) |
| MultistateModels.jl | PIJCV10 | Newton-approx 10-fold CV | Extended from Wood (2024) |
| MultistateModels.jl | EFS | Extended Fellner-Schall | Wood & Fasiolo (2017) |
| MultistateModels.jl | CV10 | Exact 10-Fold CV | (refits 10 times) |
| MultistateModels.jl | PIJCV (weighted) | PIJCV with w(t)=Y(t)^(-1) penalty | At-risk weighting |
| MultistateModels.jl | PIJCV (learn α) | PIJCV with estimated α | Adaptive weighting |
| mgcv | GCV.Cp | Generalized CV via Poisson GAM | Wood (2017) |
| mgcv | REML | Restricted ML | Wood (2011) |
| survstan | MLE | MLE via Stan (Weibull PH) | Demarqui & Mayrink (2021) |

**Weighted Penalty Methods:**

Weighted penalties use $w(t) = Y(t)^{-\alpha}$ where $Y(t)$ is the number of subjects 
at risk at time $t$. This penalizes more strongly in regions with sparse data (fewer 
subjects at risk), helping prevent overfitting in the tails. Two variants are tested:

- **Fixed α=1**: Standard inverse at-risk weighting
- **Learned α**: Estimate optimal α from data via marginal likelihood optimization

# Part 1: Simple Survival Model

We start with the simplest case: a two-state survival model with a single transition 
from alive (state 1) to dead (state 2).

## True Model Specification

Data are simulated from a Weibull hazard with:

- **Shape ($\kappa$)**: 1.5 (increasing hazard)
- **Rate ($\lambda$)**: 0.3
- **Sample size**: 100

$$h(t) = \kappa \cdot \lambda \cdot t^{\kappa - 1} = 1.5 \times 0.3 \times t^{0.5} = 0.45 \sqrt{t}$$

$$H(t) = \lambda \cdot t^{\kappa} = 0.3 \cdot t^{1.5}$$

$$S(t) = \exp(-H(t)) = \exp(-0.3 \cdot t^{1.5})$$

## Load Julia Results in R

```{r}
#| label: load-julia-results
#| output: false

# Load simulated data
surv_data <- read.csv("_surv_data.csv")
n <- nrow(surv_data)
true_shape <- 1.5
true_rate <- 0.3
max_time <- 5.0

# Load Julia curves
julia_curves <- read.csv("_julia_curves.csv")
julia_summary <- read.csv("_julia_summary.csv")

# Load knots calibrated by Julia (from event time quantiles)
knots_df <- read.csv("_knots.csv")
interior_knots <- knots_df$interior_knots
boundary_lower <- knots_df$boundary_lower[1]
boundary_upper <- knots_df$boundary_upper[1]

cat("=== Julia MultistateModels.jl Results ===\n")
print(julia_summary)

cat("\n=== Knot Configuration (shared across all methods) ===\n")
cat("Boundary knots: [", boundary_lower, ",", boundary_upper, "]\n")
cat("Interior knots:", round(interior_knots, 3), "\n")
cat("Number of interior knots:", length(interior_knots), "\n")
```

## True Hazard and Survival Functions

```{r}
#| label: true-functions
#| output: false

# True hazard function (Weibull with rate parameterization)
true_hazard <- function(t, shape = true_shape, rate = true_rate) {
  shape * rate * t^(shape - 1)
}

# True cumulative hazard
true_cumhaz <- function(t, shape = true_shape, rate = true_rate) {
  rate * t^shape
}

# True survival function
true_surv <- function(t, shape = true_shape, rate = true_rate) {
  exp(-true_cumhaz(t, shape, rate))
}

# True CDF (cumulative incidence for single transition)
true_cdf <- function(t, shape = true_shape, rate = true_rate) {
  1 - true_surv(t, shape, rate)
}
```

## mgcv PAM Fit

We fit a piecewise-exponential additive model (PAM) using `mgcv::gam` with Poisson 
likelihood. This transforms the survival problem into a Poisson regression.
**mgcv uses the exact same knots as Julia** via manual knot specification for P-splines.

```{r}
#| label: mgcv-fit
#| output: false

# Create Surv object
surv_obj <- Surv(surv_data$time, surv_data$status)

# Add id column if not present
surv_data_ped <- surv_data %>% mutate(id = row_number())

# Use pammtools to create piecewise-exponential data (PED)
# This handles the data transformation properly
ped <- as_ped(
  formula = Surv(time, status) ~ 1,
  data = surv_data_ped,
  id = "id"
)

cat("PED data structure (via pammtools):\n")
cat("  Total pseudo-observations:", nrow(ped), "\n")
cat("  Events in PED:", sum(ped$ped_status), "\n")

# For mgcv P-splines with manual knots:
# - k is the basis dimension
# - m is the spline order (m=2 for cubic, with 2nd order difference penalty)
# - Need k + m + 2 knots total
# - The middle k - m knots must span the data range
# - P-splines require EVENLY SPACED knots for proper penalty behavior

# Julia uses 5 interior knots + 2 boundary = 7 middle knots
# So k - m = 7, and with m = 2, we have k = 9
k_mgcv <- length(interior_knots) + 4  # 5 + 4 = 9
m_order <- 2

# Build the full knot sequence for P-splines
# Middle knots: [boundary_lower, interior_knots, boundary_upper]
middle_knots <- c(boundary_lower, interior_knots, boundary_upper)

# Need (k + m + 2) - (k - m) = 2m + 2 = 6 padding knots (3 on each side)
# P-splines require evenly spaced knots - use average spacing for padding
# Note: Padding knots extend below 0 but this is just for basis construction;
# the hazard is never evaluated at negative times
delta <- mean(diff(middle_knots))
padding_lower <- boundary_lower - (3:1) * delta
padding_upper <- boundary_upper + (1:3) * delta

# Full knot sequence
mgcv_knots <- c(padding_lower, middle_knots, padding_upper)

cat("\n=== Knot Configuration ===\n")
cat("Julia interior knots:", round(interior_knots, 3), "\n")
cat("Middle knots (k-m =", length(middle_knots), "):", round(middle_knots, 3), "\n")
cat("Full P-spline knots (k+m+2 =", length(mgcv_knots), "):", round(mgcv_knots, 3), "\n")
cat("Average knot spacing (delta):", round(delta, 3), "\n")
cat("mgcv k =", k_mgcv, ", m =", m_order, "\n")

# Fit PAM with GCV using P-splines and manual knots
fit_gcv <- pamm(
  ped_status ~ s(tend, bs = "ps", k = k_mgcv, m = c(m_order, m_order)),
  data = ped,
  method = "GCV.Cp",
  knots = list(tend = mgcv_knots)
)

# Fit PAM with REML
fit_reml <- pamm(
  ped_status ~ s(tend, bs = "ps", k = k_mgcv, m = c(m_order, m_order)),
  data = ped,
  method = "REML",
  knots = list(tend = mgcv_knots)
)

cat("\n=== mgcv/pammtools Results (P-splines with Julia knots) ===\n")
cat("k =", k_mgcv, ", m =", m_order, "\n")
cat("Manual knots used:", round(mgcv_knots, 3), "\n")

cat("\nGCV.Cp:\n")
cat("  sp =", round(fit_gcv$sp, 6), "\n")
cat("  EDF =", round(sum(fit_gcv$edf), 4), "\n")

cat("\nREML:\n")
cat("  sp =", round(fit_reml$sp, 6), "\n")
cat("  EDF =", round(sum(fit_reml$edf), 4), "\n")

```

## flexsurv Fit

```{r}
#| label: flexsurv-fit
#| output: false

# flexsurv uses knots on log(time) scale, so we need to transform
# But we can specify explicit knots via the knots argument
# flexsurv's "knots" argument takes the interior knots on the log scale

# Transform interior knots to log scale for flexsurv
# Note: flexsurv interprets knots as on log(time) scale
log_interior_knots <- log(interior_knots)

cat("Using same interior knots as Julia (transformed to log scale for flexsurv):\n")
cat("  Original knots:", round(interior_knots, 3), "\n")
cat("  Log-scale knots:", round(log_interior_knots, 3), "\n")

# Fit flexsurv spline model with the same knots
fit_flex <- flexsurvspline(
  Surv(time, status) ~ 1,
  data = surv_data,
  knots = log_interior_knots,  # Interior knots on log scale
  scale = "hazard"
)

# Also fit true Weibull for reference
fit_weibull <- flexsurvreg(
  Surv(time, status) ~ 1,
  data = surv_data,
  dist = "weibullPH"
)

cat("\n=== flexsurv Results ===\n")
cat("\nSpline model (", length(interior_knots), " interior knots):\n", sep="")
cat("  AIC =", round(AIC(fit_flex), 2), "\n")

cat("\nWeibull fit (for reference):\n")
cat("  Estimated shape:", round(exp(fit_weibull$res["shape", "est"]), 3), "\n")
cat("  Estimated scale:", round(exp(fit_weibull$res["scale", "est"]), 3), "\n")
cat("  True shape:", true_shape, "\n")
cat("  True rate:", true_rate, "\n")
```

## survstan Fit (Weibull PH via Stan)

We fit a Weibull proportional hazards model using `survstan` which uses Stan for 
optimization. This provides a parametric comparison point against the flexible spline methods.

```{r}
#| label: survstan-fit
#| output: false
#| cache: true

cat("=== Fitting survstan (Weibull PH via Stan) ===\n")

# Fit Weibull PH model via Stan
# survstan uses phreg() for proportional hazards with MLE via Stan
fit_survstan <- survstan::phreg(
  Surv(time, status) ~ 1,
  data = surv_data,
  baseline = "weibull"
)

cat("\n=== survstan Results ===\n")
cat("Weibull PH model (MLE via Stan)\n")

# Extract parameter estimates - estimates() returns a named vector
survstan_est <- survstan::estimates(fit_survstan)
cat("\nEstimated parameters:\n")
print(survstan_est)

# Extract alpha (shape) and gamma (scale) for hazard computation
survstan_alpha <- survstan_est["alpha"]
survstan_gamma <- survstan_est["gamma"]
cat("\nWeibull shape (alpha):", survstan_alpha, "\n")
cat("Weibull scale (gamma):", survstan_gamma, "\n")
```

## Effective Degrees of Freedom Comparison

The **effective degrees of freedom (EDF)** is the proper metric for comparing smoothing
across different software packages. EDF measures how many parameters the smooth effectively
uses, independent of λ scaling conventions.

```{r}
#| label: edf-comparison

# Collect EDF results - this is the key comparison metric
edf_summary <- rbind(
  julia_summary %>% 
    mutate(Package = "MultistateModels.jl") %>%
    select(Package, Method = method, EDF = edf, Lambda_or_sp = lambda),
  data.frame(
    Package = c("mgcv", "mgcv"),
    Method = c("GCV.Cp", "REML"),
    EDF = c(sum(fit_gcv$edf), sum(fit_reml$edf)),
    Lambda_or_sp = c(fit_gcv$sp, fit_reml$sp)
  ),
  data.frame(
    Package = "survstan",
    Method = "Weibull MLE",
    EDF = 2,  # Weibull has 2 parameters (shape, scale)
    Lambda_or_sp = NA
  )
)

kable(edf_summary, digits = 4,
      caption = "Effective Degrees of Freedom by Method (Key Comparison)",
      col.names = c("Package", "Method", "EDF", "λ or sp")) %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  column_spec(3, bold = TRUE, background = "#f0f9e8")
```

## Model Selection Criteria (AIC/BIC)

For model comparison, we compute AIC and BIC based on the penalized log-likelihood
and effective degrees of freedom:

- **AIC** = $-2 \cdot \ell + 2 \cdot \text{EDF}$
- **BIC** = $-2 \cdot \ell + \log(n) \cdot \text{EDF}$

```{r}
#| label: aic-bic-comparison

n_obs <- nrow(surv_data)

# Julia model selection criteria
julia_aic_bic <- julia_summary %>%
  mutate(
    Package = "MultistateModels.jl",
    AIC = -2 * loglik + 2 * edf,
    BIC = -2 * loglik + log(n_obs) * edf
  ) %>%
  select(Package, Method = method, EDF = edf, LogLik = loglik, AIC, BIC)

# For mgcv: The Poisson pseudo-likelihood differs from survival likelihood by
# an offset correction. The relationship is:
#   ℓ_Poisson = ℓ_Survival + Σ_i δ_i * offset_i
# where offset_i = log(interval_width) for the event interval.
# Therefore: ℓ_Survival = ℓ_Poisson - Σ_i δ_i * offset_i

compute_surv_loglik_mgcv <- function(fit_gam, ped) {
  # Get Poisson log-likelihood
  mu_hat <- fitted(fit_gam)
  y <- ped$ped_status
  poisson_ll <- sum(y * log(mu_hat) - mu_hat)
  
  # Correction: subtract sum of offsets for event pseudo-observations
  event_offsets <- ped$offset[ped$ped_status == 1]
  offset_correction <- sum(event_offsets)
  
  # Survival log-likelihood
  surv_ll <- poisson_ll - offset_correction
  return(surv_ll)
}

# Compute survival log-likelihoods for mgcv models
loglik_mgcv_gcv <- compute_surv_loglik_mgcv(fit_gcv, ped)
loglik_mgcv_reml <- compute_surv_loglik_mgcv(fit_reml, ped)

# mgcv model selection using survival log-likelihood
mgcv_aic_bic <- data.frame(
  Package = c("mgcv", "mgcv"),
  Method = c("GCV.Cp", "REML"),
  EDF = c(sum(fit_gcv$edf), sum(fit_reml$edf)),
  LogLik = c(loglik_mgcv_gcv, loglik_mgcv_reml),
  AIC = c(-2 * loglik_mgcv_gcv + 2 * sum(fit_gcv$edf),
          -2 * loglik_mgcv_reml + 2 * sum(fit_reml$edf)),
  BIC = c(-2 * loglik_mgcv_gcv + log(n_obs) * sum(fit_gcv$edf),
          -2 * loglik_mgcv_reml + log(n_obs) * sum(fit_reml$edf))
)

# flexsurv model selection
flex_aic_bic <- data.frame(
  Package = "flexsurv",
  Method = "spline",
  EDF = length(coef(fit_flex)),
  LogLik = fit_flex$loglik,
  AIC = AIC(fit_flex),
  BIC = BIC(fit_flex)
)

# survstan model selection
survstan_loglik <- fit_survstan$loglik
survstan_aic_bic <- data.frame(
  Package = "survstan",
  Method = "Weibull MLE",
  EDF = 2,
  LogLik = survstan_loglik,
  AIC = AIC(fit_survstan),
  BIC = -2 * survstan_loglik + log(n_obs) * 2  # Manual BIC calculation
)

aic_bic_summary <- rbind(julia_aic_bic, mgcv_aic_bic, flex_aic_bic, survstan_aic_bic)

n_julia_methods <- nrow(julia_aic_bic)
kable(aic_bic_summary, digits = 2,
      caption = "Model Selection Criteria by Method (Survival Log-Likelihood)") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  pack_rows("MultistateModels.jl", 1, n_julia_methods) %>%
  pack_rows("mgcv", n_julia_methods + 1, n_julia_methods + 2) %>%
  pack_rows("flexsurv", n_julia_methods + 3, n_julia_methods + 3) %>%
  pack_rows("survstan", n_julia_methods + 4, n_julia_methods + 4)
```

**Note on mgcv log-likelihood:** The mgcv package reports Poisson pseudo-likelihood values, 
not survival log-likelihood. We convert using the relationship: 
$\ell_{\text{survival}} = \ell_{\text{Poisson}} - \sum_i \delta_i \cdot \text{offset}_i$,
where $\text{offset}_i = \log(\text{interval width})$ for each event's interval.

**Key observations:**

- All methods produce similar log-likelihoods (within ~2-3 units), validating correctness
- EDF values are in the range 2-3, indicating similar effective smoothness across methods
- Julia's exact likelihood and mgcv's PAM/Poisson approach agree well

## Hazard Function Comparison

```{r}
#| label: hazard-comparison
#| fig-height: 7

# Evaluation grid
eval_times <- seq(0.01, max_time, length.out = 200)

# True hazard
h_true <- true_hazard(eval_times)

# mgcv/pammtools predictions - use pammtools helper
newdata_mgcv <- data.frame(tend = eval_times)
h_mgcv_gcv <- predict(fit_gcv, newdata = newdata_mgcv, type = "response") / 
              mean(diff(ped$tend[1:2]))  # Hazard = rate / interval_length
h_mgcv_reml <- predict(fit_reml, newdata = newdata_mgcv, type = "response") /
               mean(diff(ped$tend[1:2]))

# Simpler: use pammtools add_hazard for proper hazard extraction
hazard_mgcv_gcv <- add_hazard(newdata_mgcv, fit_gcv)
hazard_mgcv_reml <- add_hazard(newdata_mgcv, fit_reml)

# flexsurv hazard
h_flex <- summary(fit_flex, t = eval_times, type = "hazard")[[1]]$est

# Julia curves from CSV (all methods including weighted)
julia_h <- julia_curves %>% filter(method == "Julia PIJCV") %>% pull(hazard)
julia_h_efs <- julia_curves %>% filter(method == "Julia EFS") %>% pull(hazard)
julia_h_cv10 <- julia_curves %>% filter(method == "Julia CV10") %>% pull(hazard)
julia_h_weighted <- julia_curves %>% filter(method == "Julia PIJCV (weighted)") %>% pull(hazard)
julia_h_learn_alpha <- julia_curves %>% filter(method == "Julia PIJCV (learn α)") %>% pull(hazard)

# survstan hazard - extract Weibull parameters and compute hazard
# survstan Weibull PH: h(t) = (alpha/gamma) * (t/gamma)^(alpha-1)
h_survstan <- (survstan_alpha / survstan_gamma) * (eval_times / survstan_gamma)^(survstan_alpha - 1)

# Combine for plotting (including weighted methods)
hazard_df <- data.frame(
  time = rep(eval_times, 9),
  hazard = c(h_true, julia_h, julia_h_efs, julia_h_cv10, 
             julia_h_weighted, julia_h_learn_alpha,
             hazard_mgcv_gcv$hazard, h_flex, h_survstan),
  method = factor(rep(c("True (Weibull)", "Julia PIJCV", "Julia EFS", "Julia CV10",
                        "Julia PIJCV (weighted)", "Julia PIJCV (learn α)",
                        "mgcv GCV", "flexsurv", "survstan"), 
                      each = length(eval_times)),
                  levels = c("True (Weibull)", "Julia PIJCV", "Julia EFS", "Julia CV10",
                             "Julia PIJCV (weighted)", "Julia PIJCV (learn α)",
                             "mgcv GCV", "flexsurv", "survstan"))
)

ggplot(hazard_df, aes(x = time, y = hazard, color = method, linetype = method)) +
  geom_line(linewidth = 1) +
  geom_rug(data = surv_data %>% filter(status == 1), 
           aes(x = time), inherit.aes = FALSE, 
           color = "gray30", alpha = 0.5, sides = "b") +
  scale_color_manual(values = c("True (Weibull)" = "black", 
                                 "Julia PIJCV" = "#D55E00",
                                 "Julia EFS" = "#56B4E9",
                                 "Julia CV10" = "#F0E442",
                                 "Julia PIJCV (weighted)" = "#0072B2",
                                 "Julia PIJCV (learn α)" = "#CC79A7",
                                 "mgcv GCV" = "#E69F00",
                                 "flexsurv" = "#009E73",
                                 "survstan" = "#999999")) +
  scale_linetype_manual(values = c("True (Weibull)" = "solid",
                                    "Julia PIJCV" = "dashed",
                                    "Julia EFS" = "twodash",
                                    "Julia CV10" = "dashed",
                                    "Julia PIJCV (weighted)" = "dotted",
                                    "Julia PIJCV (learn α)" = "dotted",
                                    "mgcv GCV" = "dotdash",
                                    "flexsurv" = "solid",
                                    "survstan" = "longdash")) +
  labs(
    title = "Hazard Function Estimates",
    subtitle = paste0("True: Weibull(shape=", true_shape, ", rate=", true_rate, "); rug = event times"),
    x = "Time",
    y = "Hazard h(t)",
    color = "Method",
    linetype = "Method"
  ) +
  theme(legend.position = "bottom") +
  guides(color = guide_legend(nrow = 3), linetype = guide_legend(nrow = 3))
```

## Cumulative Hazard Comparison

```{r}
#| label: cumhaz-comparison
#| fig-height: 6

# True cumulative hazard
H_true <- true_cumhaz(eval_times)

# Julia cumulative hazard from CSV (all methods including weighted)
julia_H <- julia_curves %>% filter(method == "Julia PIJCV") %>% pull(cumhaz)
julia_H_efs <- julia_curves %>% filter(method == "Julia EFS") %>% pull(cumhaz)
julia_H_cv10 <- julia_curves %>% filter(method == "Julia CV10") %>% pull(cumhaz)
julia_H_weighted <- julia_curves %>% filter(method == "Julia PIJCV (weighted)") %>% pull(cumhaz)
julia_H_learn_alpha <- julia_curves %>% filter(method == "Julia PIJCV (learn α)") %>% pull(cumhaz)

# mgcv cumulative hazard - compute by numerical integration of hazard
H_mgcv_gcv <- cumsum(hazard_mgcv_gcv$hazard) * diff(eval_times)[1]

# flexsurv cumulative hazard
H_flex <- summary(fit_flex, t = eval_times, type = "cumhaz")[[1]]$est

# survstan cumulative hazard - Weibull: H(t) = (t/gamma)^alpha
H_survstan <- (eval_times / survstan_gamma)^survstan_alpha

# Combine for plotting (including weighted methods)
cumhaz_df <- data.frame(
  time = rep(eval_times, 9),
  cumhaz = c(H_true, julia_H, julia_H_efs, julia_H_cv10,
             julia_H_weighted, julia_H_learn_alpha,
             H_mgcv_gcv, H_flex, H_survstan),
  method = factor(rep(c("True (Weibull)", "Julia PIJCV", "Julia EFS", "Julia CV10",
                        "Julia PIJCV (weighted)", "Julia PIJCV (learn α)",
                        "mgcv GCV", "flexsurv", "survstan"), 
                      each = length(eval_times)),
                  levels = c("True (Weibull)", "Julia PIJCV", "Julia EFS", "Julia CV10",
                             "Julia PIJCV (weighted)", "Julia PIJCV (learn α)",
                             "mgcv GCV", "flexsurv", "survstan"))
)

ggplot(cumhaz_df, aes(x = time, y = cumhaz, color = method, linetype = method)) +
  geom_line(linewidth = 1) +
  scale_color_manual(values = c("True (Weibull)" = "black", 
                                 "Julia PIJCV" = "#D55E00",
                                 "Julia EFS" = "#56B4E9",
                                 "Julia CV10" = "#F0E442",
                                 "Julia PIJCV (weighted)" = "#0072B2",
                                 "Julia PIJCV (learn α)" = "#CC79A7",
                                 "mgcv GCV" = "#E69F00",
                                 "flexsurv" = "#009E73",
                                 "survstan" = "#999999")) +
  scale_linetype_manual(values = c("True (Weibull)" = "solid",
                                    "Julia PIJCV" = "dashed",
                                    "Julia EFS" = "twodash",
                                    "Julia CV10" = "dashed",
                                    "Julia PIJCV (weighted)" = "dotted",
                                    "Julia PIJCV (learn α)" = "dotted",
                                    "mgcv GCV" = "dotdash",
                                    "flexsurv" = "solid",
                                    "survstan" = "longdash")) +
  labs(
    title = "Cumulative Hazard Estimates",
    x = "Time",
    y = "Cumulative Hazard H(t)",
    color = "Method",
    linetype = "Method"
  ) +
  theme(legend.position = "bottom") +
  guides(color = guide_legend(nrow = 3), linetype = guide_legend(nrow = 3))
```

## Survival and Cumulative Incidence Curves

```{r}
#| label: survival-comparison
#| fig-height: 10

# True survival/CIF
S_true <- true_surv(eval_times)
F_true <- true_cdf(eval_times)

# Julia survival from CSV (all methods including weighted)
julia_S <- julia_curves %>% filter(method == "Julia PIJCV") %>% pull(survival)
julia_S_efs <- julia_curves %>% filter(method == "Julia EFS") %>% pull(survival)
julia_S_cv10 <- julia_curves %>% filter(method == "Julia CV10") %>% pull(survival)
julia_S_weighted <- julia_curves %>% filter(method == "Julia PIJCV (weighted)") %>% pull(survival)
julia_S_learn_alpha <- julia_curves %>% filter(method == "Julia PIJCV (learn α)") %>% pull(survival)

# Julia CIF
F_julia <- 1 - julia_S
F_julia_efs <- 1 - julia_S_efs
F_julia_cv10 <- 1 - julia_S_cv10
F_julia_weighted <- 1 - julia_S_weighted
F_julia_learn_alpha <- 1 - julia_S_learn_alpha

# mgcv survival - compute from cumulative hazard (S = exp(-H))
S_mgcv_gcv <- exp(-H_mgcv_gcv)
F_mgcv_gcv <- 1 - S_mgcv_gcv

# flexsurv survival
S_flex <- summary(fit_flex, t = eval_times, type = "survival")[[1]]$est
F_flex <- 1 - S_flex

# Color palette for all methods (including weighted)
method_colors <- c("True (Weibull)" = "black", 
                   "Julia PIJCV" = "#D55E00",
                   "Julia EFS" = "#56B4E9",
                   "Julia CV10" = "#F0E442",
                   "Julia PIJCV (weighted)" = "#0072B2",
                   "Julia PIJCV (learn α)" = "#CC79A7",
                   "mgcv GCV" = "#E69F00",
                   "flexsurv" = "#009E73")

method_linetypes <- c("True (Weibull)" = "solid",
                      "Julia PIJCV" = "dashed",
                      "Julia EFS" = "twodash",
                      "Julia CV10" = "dashed",
                      "Julia PIJCV (weighted)" = "dotted",
                      "Julia PIJCV (learn α)" = "dotted",
                      "mgcv GCV" = "dotdash",
                      "flexsurv" = "solid")

method_levels <- c("True (Weibull)", "Julia PIJCV", "Julia EFS", "Julia CV10",
                   "Julia PIJCV (weighted)", "Julia PIJCV (learn α)",
                   "mgcv GCV", "flexsurv")

# Combine survival curves (including weighted methods)
surv_df <- data.frame(
  time = rep(eval_times, 8),
  survival = c(S_true, julia_S, julia_S_efs, julia_S_cv10,
               julia_S_weighted, julia_S_learn_alpha,
               S_mgcv_gcv, S_flex),
  method = factor(rep(method_levels, each = length(eval_times)), levels = method_levels)
)

# Combine CIF curves (including weighted methods)
cif_df <- data.frame(
  time = rep(eval_times, 8),
  cif = c(F_true, F_julia, F_julia_efs, F_julia_cv10,
          F_julia_weighted, F_julia_learn_alpha,
          F_mgcv_gcv, F_flex),
  method = factor(rep(method_levels, each = length(eval_times)), levels = method_levels)
)

# Kaplan-Meier for reference
km_fit <- survfit(Surv(time, status) ~ 1, data = surv_data)

# Plot survival
p_surv <- ggplot(surv_df, aes(x = time, y = survival, color = method, linetype = method)) +
  geom_line(linewidth = 1) +
  geom_step(data = data.frame(time = km_fit$time, survival = km_fit$surv),
            aes(x = time, y = survival), 
            inherit.aes = FALSE, color = "gray50", alpha = 0.5, linewidth = 0.8) +
  scale_color_manual(values = method_colors) +
  scale_linetype_manual(values = method_linetypes) +
  labs(
    title = "Survival Function S(t)",
    subtitle = "Gray step function = Kaplan-Meier estimate",
    x = "Time",
    y = "S(t)",
    color = "Method",
    linetype = "Method"
  ) +
  theme(legend.position = "bottom") +
  guides(color = guide_legend(nrow = 3), linetype = guide_legend(nrow = 3))

# Plot CIF
p_cif <- ggplot(cif_df, aes(x = time, y = cif, color = method, linetype = method)) +
  geom_line(linewidth = 1) +
  geom_step(data = data.frame(time = km_fit$time, cif = 1 - km_fit$surv),
            aes(x = time, y = cif), 
            inherit.aes = FALSE, color = "gray50", alpha = 0.5, linewidth = 0.8) +
  geom_rug(data = surv_data %>% filter(status == 1), 
           aes(x = time), inherit.aes = FALSE, 
           color = "gray30", alpha = 0.5, sides = "b") +
  scale_color_manual(values = method_colors) +
  scale_linetype_manual(values = method_linetypes) +
  labs(
    title = "Cumulative Incidence F(t) = 1 - S(t)",
    subtitle = "Gray step function = 1 - Kaplan-Meier; rug = event times",
    x = "Time",
    y = "F(t)",
    color = "Method",
    linetype = "Method"
  ) +
  theme(legend.position = "bottom") +
  guides(color = guide_legend(nrow = 3), linetype = guide_legend(nrow = 3))

p_surv / p_cif
```

## Accuracy Metrics

```{r}
#| label: accuracy-metrics

# RMSE function
rmse <- function(true, est) sqrt(mean((true - est)^2))

# Get all Julia methods from CSV
julia_methods <- unique(julia_curves$method)

# Calculate RMSE for all Julia methods
julia_metrics <- lapply(julia_methods, function(m) {
  h <- julia_curves %>% filter(method == m) %>% pull(hazard)
  H <- julia_curves %>% filter(method == m) %>% pull(cumhaz)
  S <- julia_curves %>% filter(method == m) %>% pull(survival)
  
  data.frame(
    Package = "MultistateModels.jl",
    Method = gsub("Julia ", "", m),
    Hazard_RMSE = rmse(h_true, h),
    CumHaz_RMSE = rmse(H_true, H),
    Survival_RMSE = rmse(S_true, S)
  )
}) %>% bind_rows()

# Add R package results
r_metrics <- data.frame(
  Package = c("mgcv", "flexsurv"),
  Method = c("GCV.Cp", "spline"),
  Hazard_RMSE = c(rmse(h_true, hazard_mgcv_gcv$hazard), rmse(h_true, h_flex)),
  CumHaz_RMSE = c(rmse(H_true, H_mgcv_gcv), rmse(H_true, H_flex)),
  Survival_RMSE = c(rmse(S_true, S_mgcv_gcv), rmse(S_true, S_flex))
)

# Combine and order
metrics_df <- rbind(julia_metrics, r_metrics)

# Order Julia methods logically (including weighted methods)
julia_order <- c("PIJCV", "PIJCV10", "EFS", "CV10", "PIJCV (weighted)", "PIJCV (learn α)")
metrics_df$Method <- factor(metrics_df$Method, 
                            levels = c(julia_order, "GCV.Cp", "spline"))
metrics_df <- metrics_df %>% arrange(Method)

# Dynamically determine row indices
n_julia <- nrow(julia_metrics)
n_r <- nrow(r_metrics)

kable(metrics_df, digits = 5,
      caption = "RMSE vs True Weibull Functions",
      col.names = c("Package", "Method", "Hazard", "Cum. Hazard", "Survival")) %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  pack_rows("MultistateModels.jl", 1, n_julia) %>%
  pack_rows("R", n_julia + 1, n_julia + n_r)
```

## Runtime Comparison

A key advantage of Newton-approximated cross-validation methods (PIJCV, PIJCV10)
is computational efficiency. While exact k-fold CV requires refitting the model K times,
the Newton approximation achieves similar results in a single optimization.

**Weighted penalty methods** add additional overhead to compute the at-risk counts and
weighted penalty matrices, but this cost is typically small compared to the fitting itself.

```{r}
#| label: runtime-comparison
#| fig-height: 6

# Check if runtime data is available
if ("runtime_sec" %in% names(julia_summary)) {
  
  # Categorize methods (including weighted)
  julia_summary$category <- case_when(
    julia_summary$method %in% c("PIJCV", "PIJCV10") ~ "Newton CV (uniform)",
    julia_summary$method %in% c("PIJCV_weighted", "PIJCV_learn_alpha") ~ "Newton CV (weighted)",
    julia_summary$method %in% c("CV10") ~ "Exact CV",
    TRUE ~ "Other (EFS)"
  )
  
  # Order methods by category (including weighted methods)
  method_order <- c("PIJCV", "PIJCV10", 
                    "PIJCV_weighted", "PIJCV_learn_alpha",
                    "EFS",
                    "CV10")
  julia_summary$method <- factor(julia_summary$method, levels = method_order)
  
  # Create bar plot
  p_runtime <- ggplot(julia_summary, aes(x = method, y = runtime_sec, fill = category)) +
    geom_col(width = 0.7) +
    geom_text(aes(label = sprintf("%.2fs", runtime_sec)), 
              vjust = -0.5, size = 3) +
    scale_fill_manual(values = c(
      "Newton CV (uniform)" = "#3498db",
      "Newton CV (weighted)" = "#9b59b6",
      "Exact CV" = "#e74c3c",
      "Other (EFS)" = "#2ecc71"
    )) +
    labs(
      title = "Runtime Comparison: Smoothing Parameter Selection Methods",
      subtitle = "Including weighted penalty methods (n=1000 subjects, 5 interior knots)",
      x = "Method",
      y = "Runtime (seconds)",
      fill = "Category"
    ) +
    theme_minimal() +
    theme(
      axis.text.x = element_text(angle = 45, hjust = 1),
      legend.position = "bottom"
    )
  
  print(p_runtime)
  
  # Runtime summary table
  runtime_summary <- julia_summary %>%
    group_by(category) %>%
    summarise(
      Methods = paste(method, collapse = ", "),
      Mean_Runtime = mean(runtime_sec),
      Min_Runtime = min(runtime_sec),
      Max_Runtime = max(runtime_sec),
      .groups = "drop"
    ) %>%
    arrange(Mean_Runtime)
  
  kable(runtime_summary, digits = 3,
        caption = "Runtime Summary by Method Category",
        col.names = c("Category", "Methods", "Mean (s)", "Min (s)", "Max (s)")) %>%
    kable_styling(bootstrap_options = c("striped", "hover"))
  
} else {
  cat("Runtime data not available in julia_summary.\n")
}
```

```{r}
#| label: runtime-table

if ("runtime_sec" %in% names(julia_summary)) {
  # Add category if not present (from previous chunk)
  if (!"category" %in% names(julia_summary)) {
    julia_summary$category <- case_when(
      julia_summary$method %in% c("PIJCV", "PIJCV10") ~ "Newton CV (uniform)",
      julia_summary$method %in% c("PIJCV_weighted", "PIJCV_learn_alpha") ~ "Newton CV (weighted)",
      julia_summary$method %in% c("CV10") ~ "Exact CV",
      TRUE ~ "Other"
    )
  }
  
  # Full runtime table with all metrics
  runtime_full <- julia_summary %>%
    select(method, category, lambda, edf, loglik, runtime_sec) %>%
    arrange(runtime_sec)
  
  kable(runtime_full, digits = c(0, 0, 2, 2, 2, 3),
        caption = "Full Results: All Julia Smoothing Methods (sorted by runtime)",
        col.names = c("Method", "Category", "λ", "EDF", "Log-Lik", "Runtime (s)")) %>%
    kable_styling(bootstrap_options = c("striped", "hover"))
}
```

**Key Runtime Observations:**

- **Newton-approximated methods** (PIJCV, PIJCV10) achieve comparable 
  accuracy to exact CV while requiring only **a single model fit** with analytical gradients
- **Exact k-fold CV** (CV10) requires K model refits, 
  making it substantially slower
- **EFS** is fast as it uses a simple closed-form approximation
- The computational advantage of Newton approximation grows with sample size and model complexity

**Why PIJCV and CV10 have similar runtimes:**

Interestingly, PIJCV (Newton-approximated LOO-CV) and CV10 (exact 10-fold CV) may show 
similar runtimes in this benchmark. This occurs because:

1. **PIJCV's overhead**: PIJCV requires computing and storing subject-level Hessians 
   ($n \times p \times p$ matrices) and performing Cholesky downdates for each subject.
   For $n=1000$ subjects, this is non-trivial memory and computation.

2. **CV10's efficiency**: Exact 10-fold CV only requires 10 model refits, and each 
   refit benefits from warm-starting at the previous solution.

3. **Small model size**: With only 5 interior knots (~8 spline coefficients), the 
   per-iteration cost is low, so the overhead of PIJCV is relatively larger.

**When PIJCV shines**: The Newton approximation advantage grows dramatically with:
- Larger sample sizes ($n > 5000$)
- More complex models (multiple transitions, many spline coefficients)
- Higher-dimensional smoothing parameter selection (multiple λ values)

In these settings, PIJCV can be 10-100× faster than exact CV.

## Discussion

### EDF vs λ Scaling

The **effective degrees of freedom (EDF)** is the proper metric for comparing smoothing
across packages. Despite significant differences in raw λ values between Julia and mgcv,
the EDF values are in good agreement, confirming that both approaches select similar 
model complexity.

The scale difference arises from:

1. **Effective sample size**: Julia uses n=1000 subjects; mgcv's PAM uses many pseudo-observations
2. **Penalty matrix normalization**: Different scaling conventions
3. **Likelihood formulation**: Exact survival vs Poisson pseudo-likelihood

### Software Packages Compared

| Package | Approach | Notes |
|---------|----------|-------|
| **MultistateModels.jl** | Exact penalized likelihood with P-splines | Multiple smoothing methods (PIJCV, PIJCV10, EFS, CV10) |
| **mgcv/pammtools** | PAM (Poisson GAM on piecewise-exponential data) | GCV.Cp, REML for smoothing selection |
| **flexsurv** | Spline hazard via ML (no explicit smoothing parameter) | Uses natural splines, not penalized |

### Why flexsurv Appears to Fit Better on Weibull Data

flexsurv's excellent performance on this Weibull simulation is **not** because it's universally 
superior—it's because the **Weibull hazard is perfectly suited to flexsurv's parameterization**:

| Aspect | MultistateModels.jl | mgcv (pammtools) | flexsurv |
|--------|---------------------|------------------|----------|
| **What's modeled** | h(t) directly | log(rate) via Poisson | **log H(t)** |
| **Time scale** | Linear time | Linear time | **Log time** |
| **Penalty** | 2nd-order difference | 2nd-order difference | **None (MLE)** |

For a Weibull hazard $h(t) = \kappa\lambda(\lambda t)^{\kappa-1}$:
- The **log cumulative hazard** is: $\log H(t) = \kappa \log \lambda + \kappa \log t$
- This is **exactly linear in log(t)**—trivially easy for flexsurv's spline!

Meanwhile, MultistateModels.jl and mgcv model $h(t)$ on linear time, where Weibull 
is a power function requiring more basis functions to approximate.

**Part 3** tests a **bathtub-shaped hazard** that challenges all methods equally.

---

# Part 2: Illness-Death Model

We now extend the analysis to a three-state illness-death model, which is more complex
due to competing risks and transition-specific hazards:

```
State 1 (Healthy) → State 2 (Ill) → State 3 (Dead)
                  ↘ State 3 (Dead) ↗
```

Three distinct transitions with different hazard functions:

- **h12** (Healthy → Ill): Moderately increasing hazard (disease onset)
- **h13** (Healthy → Dead): Decreasing hazard (early competing risk)
- **h23** (Ill → Dead): Strongly increasing hazard (death after illness)

## True Model Specification (Illness-Death)

Data are simulated from Weibull hazards with transition-specific parameters:

| Transition | Shape ($\kappa$) | Rate ($\lambda$) | Hazard Pattern |
|------------|------------------|------------------|----------------|
| 1→2 | 1.3 | 0.25 | Moderately increasing |
| 1→3 | 0.8 | 0.15 | Decreasing (competing risk) |
| 2→3 | 1.5 | 0.35 | Strongly increasing |

$$h_{12}(t) = 1.3 \times 0.25 \times t^{0.3} = 0.325 t^{0.3}$$
$$h_{13}(t) = 0.8 \times 0.15 \times t^{-0.2} = 0.12 t^{-0.2}$$
$$h_{23}(t) = 1.5 \times 0.35 \times t^{0.5} = 0.525 t^{0.5}$$

## Illness-Death Data Simulation and Julia Fitting

```{bash}
#| label: id-julia-data-generation
#| output: false

cd ../..
julia --project=. -e '
using MultistateModels
using Random
using DataFrames
using CSV
using Printf
using Distributions

# Configuration
n = 1000
max_time = 6.0
seed = 12345

# True Weibull parameters (shape, rate) for each transition
# Using MultistateModels convention: h(t) = shape * rate * t^(shape-1)
true_params = (
    h12 = (shape = 1.3, rate = 0.25),  # Moderately increasing
    h13 = (shape = 0.8, rate = 0.15),  # Decreasing (competing risk)
    h23 = (shape = 1.5, rate = 0.35)   # Strongly increasing
)

Random.seed!(seed)

# Simulate illness-death model
# For each subject, we need to simulate the competing process

function weibull_cdf(t, shape, rate)
    return 1.0 - exp(-rate * t^shape)
end

function weibull_quantile(p, shape, rate)
    return (-log(1.0 - p) / rate)^(1.0 / shape)
end

# Simulate event times
survival_data = DataFrame[]

for i in 1:n
    # Generate latent event times from state 1
    U12 = rand()
    U13 = rand()
    
    # Weibull event times from state 1
    T12 = weibull_quantile(U12, true_params.h12.shape, true_params.h12.rate)
    T13 = weibull_quantile(U13, true_params.h13.shape, true_params.h13.rate)
    
    # Determine first event from state 1
    if min(T12, T13) >= max_time
        # Administrative censoring in state 1
        push!(survival_data, DataFrame(
            id = i,
            tstart = 0.0,
            tstop = max_time,
            statefrom = 1,
            stateto = 1,
            obstype = 1
        ))
    elseif T12 < T13
        # Transition to illness (state 2)
        t_illness = T12
        
        # Now generate death time from state 2
        # Time to death from illness follows Weibull with params h23
        U23 = rand()
        T23_residual = weibull_quantile(U23, true_params.h23.shape, true_params.h23.rate)
        T_death = t_illness + T23_residual
        
        if T_death >= max_time
            # Illness then administrative censoring
            push!(survival_data, DataFrame(
                id = i,
                tstart = 0.0,
                tstop = t_illness,
                statefrom = 1,
                stateto = 2,
                obstype = 1
            ))
            push!(survival_data, DataFrame(
                id = i,
                tstart = t_illness,
                tstop = max_time,
                statefrom = 2,
                stateto = 2,
                obstype = 1
            ))
        else
            # Illness then death
            push!(survival_data, DataFrame(
                id = i,
                tstart = 0.0,
                tstop = t_illness,
                statefrom = 1,
                stateto = 2,
                obstype = 1
            ))
            push!(survival_data, DataFrame(
                id = i,
                tstart = t_illness,
                tstop = T_death,
                statefrom = 2,
                stateto = 3,
                obstype = 1
            ))
        end
    else
        # Direct death from state 1 (h13)
        push!(survival_data, DataFrame(
            id = i,
            tstart = 0.0,
            tstop = T13,
            statefrom = 1,
            stateto = 3,
            obstype = 1
        ))
    end
end

surv_data = vcat(survival_data...)

# Count transitions
n_12 = sum((surv_data.statefrom .== 1) .& (surv_data.stateto .== 2))
n_13 = sum((surv_data.statefrom .== 1) .& (surv_data.stateto .== 3))
n_23 = sum((surv_data.statefrom .== 2) .& (surv_data.stateto .== 3))
n_cens_1 = sum((surv_data.statefrom .== 1) .& (surv_data.stateto .== 1))
n_cens_2 = sum((surv_data.statefrom .== 2) .& (surv_data.stateto .== 2))

println("=== Illness-Death Data ===")
println("n = $n subjects")
println("Transitions 1→2 (illness): $n_12")
println("Transitions 1→3 (direct death): $n_13")
println("Transitions 2→3 (death after illness): $n_23")
println("Censored in state 1: $n_cens_1")
println("Censored in state 2: $n_cens_2")

# Extract event times for each transition
event_times_12 = surv_data[(surv_data.statefrom .== 1) .& (surv_data.stateto .== 2), :tstop]
event_times_13 = surv_data[(surv_data.statefrom .== 1) .& (surv_data.stateto .== 3), :tstop]
event_times_23 = surv_data[(surv_data.statefrom .== 2) .& (surv_data.stateto .== 3), :tstop] .- 
                 surv_data[(surv_data.statefrom .== 2) .& (surv_data.stateto .== 3), :tstart]

# For R export, we need the raw survival data
# Convert to standard multistate format
CSV.write("MultistateModelsTests/reports/_id_surv_data.csv", surv_data)

# Define model with spline hazards
# Using monotone=0 for unconstrained (non-monotonic) spline - tests general flexibility
h12 = Hazard(@formula(0 ~ 1), "sp", 1, 2;
             degree = 3,
             knots = [max_time/2],  # Placeholder - replaced by calibrate_splines!
             boundaryknots = [0.0, max_time],
             natural_spline = true,
             monotone = 0)  # Unconstrained spline
h13 = Hazard(@formula(0 ~ 1), "sp", 1, 3;
             degree = 3,
             knots = [max_time/2],  # Placeholder - replaced by calibrate_splines!
             boundaryknots = [0.0, max_time],
             natural_spline = true,
             monotone = 0)  # Unconstrained spline
h23 = Hazard(@formula(0 ~ 1), "sp", 2, 3;
             degree = 3,
             knots = [max_time/2],  # Placeholder - replaced by calibrate_splines!
             boundaryknots = [0.0, max_time],
             natural_spline = true,
             monotone = 0)  # Unconstrained spline

model = multistatemodel(h12, h13, h23; data=surv_data)

# Calibrate knots based on event times
println("\nCalibrating spline knots...")
knot_result = calibrate_splines!(model; nknots=5, verbose=true)

# Extract knots for each hazard
knots_12 = knot_result.h12
knots_13 = knot_result.h13
knots_23 = knot_result.h23

println("\n=== Knot Configuration ===")
println("h12: boundary=$(knots_12.boundary_knots), interior=$(round.(knots_12.interior_knots, digits=3))")
println("h13: boundary=$(knots_13.boundary_knots), interior=$(round.(knots_13.interior_knots, digits=3))")
println("h23: boundary=$(knots_23.boundary_knots), interior=$(round.(knots_23.interior_knots, digits=3))")

# Save knots for R
knots_df = DataFrame(
    transition = repeat(["h12", "h13", "h23"], inner=5),
    interior_knot_idx = repeat(1:5, 3),
    interior_knot = vcat(knots_12.interior_knots, knots_13.interior_knots, knots_23.interior_knots),
    boundary_lower = fill(0.0, 15),
    boundary_upper = fill(max_time, 15)
)
CSV.write("MultistateModelsTests/reports/_id_knots.csv", knots_df)

println("\nData generation complete. Knots and data saved for R.")
'
```

```{bash}
#| label: id-julia-fitting
#| output: false

cd ../..
julia --project=. -e '
using MultistateModels
using MultistateModels: get_parameters_flat, get_loglik
using Random
using DataFrames
using CSV
using Printf
using Distributions

# Configuration (must match data generation)
n = 1000
max_time = 6.0
seed = 12345

true_params = (
    h12 = (shape = 1.3, rate = 0.25),
    h13 = (shape = 0.8, rate = 0.15),
    h23 = (shape = 1.5, rate = 0.35)
)

Random.seed!(seed)

# Helper functions
function weibull_quantile(p, shape, rate)
    return (-log(1.0 - p) / rate)^(1.0 / shape)
end

# Regenerate survival data
survival_data = DataFrame[]
for i in 1:n
    U12 = rand()
    U13 = rand()
    T12 = weibull_quantile(U12, true_params.h12.shape, true_params.h12.rate)
    T13 = weibull_quantile(U13, true_params.h13.shape, true_params.h13.rate)
    
    if min(T12, T13) >= max_time
        push!(survival_data, DataFrame(id = i, tstart = 0.0, tstop = max_time, statefrom = 1, stateto = 1, obstype = 1))
    elseif T12 < T13
        t_illness = T12
        U23 = rand()
        T23_residual = weibull_quantile(U23, true_params.h23.shape, true_params.h23.rate)
        T_death = t_illness + T23_residual
        if T_death >= max_time
            push!(survival_data, DataFrame(id = i, tstart = 0.0, tstop = t_illness, statefrom = 1, stateto = 2, obstype = 1))
            push!(survival_data, DataFrame(id = i, tstart = t_illness, tstop = max_time, statefrom = 2, stateto = 2, obstype = 1))
        else
            push!(survival_data, DataFrame(id = i, tstart = 0.0, tstop = t_illness, statefrom = 1, stateto = 2, obstype = 1))
            push!(survival_data, DataFrame(id = i, tstart = t_illness, tstop = T_death, statefrom = 2, stateto = 3, obstype = 1))
        end
    else
        push!(survival_data, DataFrame(id = i, tstart = 0.0, tstop = T13, statefrom = 1, stateto = 3, obstype = 1))
    end
end
surv_data = vcat(survival_data...)

# Extract event times for each transition (for rug plots later)
event_times_12 = surv_data[(surv_data.statefrom .== 1) .& (surv_data.stateto .== 2), :tstop]
event_times_13 = surv_data[(surv_data.statefrom .== 1) .& (surv_data.stateto .== 3), :tstop]

# Define and calibrate model
# Using monotone=0 for unconstrained (non-monotonic) spline - tests general flexibility
h12 = Hazard(@formula(0 ~ 1), "sp", 1, 2; degree = 3, knots = [max_time/2], boundaryknots = [0.0, max_time], natural_spline = true, monotone = 0)
h13 = Hazard(@formula(0 ~ 1), "sp", 1, 3; degree = 3, knots = [max_time/2], boundaryknots = [0.0, max_time], natural_spline = true, monotone = 0)
h23 = Hazard(@formula(0 ~ 1), "sp", 2, 3; degree = 3, knots = [max_time/2], boundaryknots = [0.0, max_time], natural_spline = true, monotone = 0)
model = multistatemodel(h12, h13, h23; data=surv_data)
calibrate_splines!(model; nknots=5, verbose=false)

# Helper to get total EDF - edf is (total=Float64, per_term=Vector{Float64})
total_edf(fitted) = fitted.edf.total

# Fit with each smoothing method using new API
println("=== Fitting with smoothing methods ===")

println("\nFitting with PIJCV method...")
fitted_pijcv = fit(model; penalty=:auto, select_lambda=:pijcv, vcov_type=:none, verbose=false)
println("  PIJCV λ = $(round.(fitted_pijcv.smoothing_parameters, digits=2)), EDF = $(round(total_edf(fitted_pijcv), digits=2))")

println("\nFitting with EFS method...")
fitted_efs = fit(model; penalty=:auto, select_lambda=:efs, vcov_type=:none, verbose=false)
println("  EFS λ = $(round.(fitted_efs.smoothing_parameters, digits=2)), EDF = $(round(total_edf(fitted_efs), digits=2))")

println("\nFitting with 10-fold CV method...")
fitted_cv10 = fit(model; penalty=:auto, select_lambda=:cv10, vcov_type=:none, verbose=false)
println("  CV10 λ = $(round.(fitted_cv10.smoothing_parameters, digits=2)), EDF = $(round(total_edf(fitted_cv10), digits=2))")

# Evaluation grid
eval_times = collect(range(0.01, max_time, length=200))

# Function to evaluate hazard curves for all transitions
function evaluate_id_curves(model, beta, eval_times)
    results = Dict{String, NamedTuple}()
    
    current_idx = 1
    for (idx, haz) in enumerate(model.hazards)
        trans_name = "h$(haz.statefrom)$(haz.stateto)"
        
        # Get parameter indices for this hazard
        npars = length(haz.parnames)
        start_idx = current_idx
        end_idx = current_idx + npars - 1
        beta_haz = beta[start_idx:end_idx]
        current_idx = end_idx + 1
        
        hazard_vals = [haz.hazard_fn(t, beta_haz, ()) for t in eval_times]
        cumhaz_vals = [haz.cumhaz_fn(0.0, t, beta_haz, ()) for t in eval_times]
        
        results[trans_name] = (hazard = hazard_vals, cumhaz = cumhaz_vals)
    end
    
    return results
end

# Extract flat parameters from fitted models
beta_pijcv = get_parameters_flat(fitted_pijcv)
beta_efs = get_parameters_flat(fitted_efs)
beta_cv10 = get_parameters_flat(fitted_cv10)

# Compute curves for each method
curves_pijcv = evaluate_id_curves(model, beta_pijcv, eval_times)
curves_efs = evaluate_id_curves(model, beta_efs, eval_times)
curves_cv10 = evaluate_id_curves(model, beta_cv10, eval_times)

# Get log-likelihoods directly from fitted models
loglik_pijcv = get_loglik(fitted_pijcv)
loglik_efs = get_loglik(fitted_efs)
loglik_cv10 = get_loglik(fitted_cv10)

# Save curves to CSV
curves_list = DataFrame[]
for (method_name, curves) in [("Julia PIJCV", curves_pijcv), 
                               ("Julia EFS", curves_efs),
                               ("Julia CV10", curves_cv10)]
    for trans in ["h12", "h13", "h23"]
        push!(curves_list, DataFrame(
            time = eval_times,
            hazard = curves[trans].hazard,
            cumhaz = curves[trans].cumhaz,
            transition = trans,
            method = method_name
        ))
    end
end
julia_curves = vcat(curves_list...)
CSV.write("MultistateModelsTests/reports/_id_julia_curves.csv", julia_curves)

# Save summary with per-transition EDF
julia_summary = DataFrame(
    method = ["PIJCV", "EFS", "CV10"],
    lambda = [fitted_pijcv.smoothing_parameters[1], fitted_efs.smoothing_parameters[1], fitted_cv10.smoothing_parameters[1]],
    edf_total = [total_edf(fitted_pijcv), total_edf(fitted_efs), total_edf(fitted_cv10)],
    loglik = [loglik_pijcv, loglik_efs, loglik_cv10]
)
CSV.write("MultistateModelsTests/reports/_id_julia_summary.csv", julia_summary)

# Save event times for rug plots
event_times_df = DataFrame(
    time = Float64[],
    transition = String[]
)
for (times, trans) in [(event_times_12, "h12"), (event_times_13, "h13")]
    append!(event_times_df, DataFrame(time = times, transition = fill(trans, length(times))))
end
# For h23, use the absolute death times (not duration in state 2)
death_times_23 = surv_data[(surv_data.statefrom .== 2) .& (surv_data.stateto .== 3), :tstop]
append!(event_times_df, DataFrame(time = death_times_23, transition = fill("h23", length(death_times_23))))
CSV.write("MultistateModelsTests/reports/_id_event_times.csv", event_times_df)

println("\nIllness-death curves computed and saved.")
'
```

## Load Illness-Death Results in R

```{r}
#| label: id-load-julia-results
#| output: false

# Load simulated data
id_surv_data <- read.csv("_id_surv_data.csv")

# Load Julia curves
id_julia_curves <- read.csv("_id_julia_curves.csv")
id_julia_summary <- read.csv("_id_julia_summary.csv")

# Load knots
id_knots_df <- read.csv("_id_knots.csv")
id_knots <- id_knots_df %>%
  group_by(transition) %>%
  summarise(
    interior_knots = list(interior_knot),
    boundary_lower = first(boundary_lower),
    boundary_upper = first(boundary_upper),
    .groups = "drop"
  )

# Load event times for rug plots
id_event_times <- read.csv("_id_event_times.csv")

# True parameters
id_true_params <- list(
  h12 = list(shape = 1.3, rate = 0.25),
  h13 = list(shape = 0.8, rate = 0.15),
  h23 = list(shape = 1.5, rate = 0.35)
)

id_max_time <- 6.0
id_n <- length(unique(id_surv_data$id))

cat("=== Illness-Death Julia Results ===\n")
print(id_julia_summary)

cat("\n=== Knot Configuration ===\n")
for (trans in c("h12", "h13", "h23")) {
  knots_trans <- id_knots %>% filter(transition == trans)
  cat(trans, ": interior = [", paste(round(unlist(knots_trans$interior_knots), 3), collapse=", "), "]\n")
}
```

## True Hazard Functions (Illness-Death)

```{r}
#| label: id-true-functions
#| output: false

# True hazard functions for each transition
id_true_hazard <- function(t, trans) {
  params <- id_true_params[[trans]]
  params$shape * params$rate * t^(params$shape - 1)
}

id_true_cumhaz <- function(t, trans) {
  params <- id_true_params[[trans]]
  params$rate * t^params$shape
}
```

## mgcv/pammtools Fits (Illness-Death)

For the illness-death model, we fit separate PAM models for each transition using
pammtools, ensuring the same knots as Julia for each transition.

```{r}
#| label: id-mgcv-fit
#| output: false

library(survival)

# Create transition-specific datasets for PAM fitting
# For multi-state models, we need to filter to relevant observations

# h12: Observations starting in state 1
id_data_h12 <- id_surv_data %>%
  filter(statefrom == 1) %>%
  mutate(
    time = tstop - tstart,
    status = as.integer(stateto == 2),
    id_row = row_number()
  )

# h13: Same observations, different event indicator
id_data_h13 <- id_surv_data %>%
  filter(statefrom == 1) %>%
  mutate(
    time = tstop - tstart,
    status = as.integer(stateto == 3),
    id_row = row_number()
  )

# h23: Observations starting in state 2
id_data_h23 <- id_surv_data %>%
  filter(statefrom == 2) %>%
  mutate(
    time = tstop - tstart,
    status = as.integer(stateto == 3),
    id_row = row_number()
  )

cat("h12 data: n =", nrow(id_data_h12), ", events =", sum(id_data_h12$status), "\n")
cat("h13 data: n =", nrow(id_data_h13), ", events =", sum(id_data_h13$status), "\n")
cat("h23 data: n =", nrow(id_data_h23), ", events =", sum(id_data_h23$status), "\n")

# Function to build mgcv knots from Julia interior knots
build_mgcv_knots <- function(interior_knots, boundary_lower, boundary_upper) {
  k_mgcv <- length(interior_knots) + 4
  m_order <- 2
  middle_knots <- c(boundary_lower, interior_knots, boundary_upper)
  delta <- mean(diff(middle_knots))
  padding_lower <- boundary_lower - (3:1) * delta
  padding_upper <- boundary_upper + (1:3) * delta
  c(padding_lower, middle_knots, padding_upper)
}

# Build knot vectors for each transition
id_mgcv_knots <- list()
for (trans in c("h12", "h13", "h23")) {
  knots_trans <- id_knots %>% filter(transition == trans)
  id_mgcv_knots[[trans]] <- build_mgcv_knots(
    unlist(knots_trans$interior_knots),
    knots_trans$boundary_lower,
    knots_trans$boundary_upper
  )
}

k_mgcv <- 9  # 5 interior + 4
m_order <- 2

# Fit PAM for h12
ped_h12 <- as_ped(
  formula = Surv(time, status) ~ 1,
  data = id_data_h12,
  id = "id_row"
)

fit_h12_gcv <- pamm(
  ped_status ~ s(tend, bs = "ps", k = k_mgcv, m = c(m_order, m_order)),
  data = ped_h12,
  method = "GCV.Cp",
  knots = list(tend = id_mgcv_knots[["h12"]])
)

cat("\nh12 mgcv fit: EDF =", round(sum(fit_h12_gcv$edf), 2), "\n")

# Fit PAM for h13
ped_h13 <- as_ped(
  formula = Surv(time, status) ~ 1,
  data = id_data_h13,
  id = "id_row"
)

fit_h13_gcv <- pamm(
  ped_status ~ s(tend, bs = "ps", k = k_mgcv, m = c(m_order, m_order)),
  data = ped_h13,
  method = "GCV.Cp",
  knots = list(tend = id_mgcv_knots[["h13"]])
)

cat("h13 mgcv fit: EDF =", round(sum(fit_h13_gcv$edf), 2), "\n")

# Fit PAM for h23
ped_h23 <- as_ped(
  formula = Surv(time, status) ~ 1,
  data = id_data_h23,
  id = "id_row"
)

fit_h23_gcv <- pamm(
  ped_status ~ s(tend, bs = "ps", k = k_mgcv, m = c(m_order, m_order)),
  data = ped_h23,
  method = "GCV.Cp",
  knots = list(tend = id_mgcv_knots[["h23"]])
)

cat("h23 mgcv fit: EDF =", round(sum(fit_h23_gcv$edf), 2), "\n")
```

## Effective Degrees of Freedom (Illness-Death)

```{r}
#| label: id-edf-comparison

# Collect EDF results for illness-death model
id_edf_summary <- rbind(
  id_julia_summary %>%
    mutate(Package = "MultistateModels.jl") %>%
    select(Package, Method = method, EDF = edf_total, Lambda = lambda),
  data.frame(
    Package = rep("mgcv", 3),
    Method = c("GCV h12", "GCV h13", "GCV h23"),
    EDF = c(sum(fit_h12_gcv$edf), sum(fit_h13_gcv$edf), sum(fit_h23_gcv$edf)),
    Lambda = c(fit_h12_gcv$sp, fit_h13_gcv$sp, fit_h23_gcv$sp)
  )
)

kable(id_edf_summary, digits = 4,
      caption = "Effective Degrees of Freedom - Illness-Death Model",
      col.names = c("Package", "Method", "EDF (total)", "λ")) %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  column_spec(3, bold = TRUE, background = "#f0f9e8")
```

## Model Selection Criteria (Illness-Death)

```{r}
#| label: id-aic-bic-comparison

id_n_obs <- id_n

# Julia AIC/BIC
id_julia_aic_bic <- id_julia_summary %>%
  mutate(
    Package = "MultistateModels.jl",
    AIC = -2 * loglik + 2 * edf_total,
    BIC = -2 * loglik + log(id_n_obs) * edf_total
  ) %>%
  select(Package, Method = method, EDF = edf_total, LogLik = loglik, AIC, BIC)

kable(id_julia_aic_bic, digits = 2,
      caption = "Model Selection Criteria - Illness-Death Model") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

## Hazard Function Comparison (Illness-Death)

```{r}
#| label: id-hazard-comparison
#| fig-height: 10

# Evaluation grid
id_eval_times <- seq(0.01, id_max_time, length.out = 200)

# Get mgcv hazard predictions
id_newdata <- data.frame(tend = id_eval_times)

hazard_mgcv_h12 <- add_hazard(id_newdata, fit_h12_gcv)
hazard_mgcv_h13 <- add_hazard(id_newdata, fit_h13_gcv)
hazard_mgcv_h23 <- add_hazard(id_newdata, fit_h23_gcv)

# Color palette
id_method_colors <- c("True (Weibull)" = "black",
                      "Julia PIJCV" = "#D55E00",
                      "Julia EFS" = "#56B4E9",
                      "Julia CV10" = "#F0E442",
                      "mgcv GCV" = "#E69F00")

id_method_linetypes <- c("True (Weibull)" = "solid",
                         "Julia PIJCV" = "dashed",
                         "Julia EFS" = "twodash",
                         "Julia CV10" = "dashed",
                         "mgcv GCV" = "dotdash")

# Create hazard plots for each transition
create_hazard_plot <- function(trans, trans_label) {
  # True hazard
  h_true <- id_true_hazard(id_eval_times, trans)
  
  # Julia hazards
  julia_methods <- c("Julia PIJCV", "Julia EFS", "Julia CV10")
  julia_hazards <- lapply(julia_methods, function(m) {
    id_julia_curves %>%
      filter(transition == trans, method == m) %>%
      pull(hazard)
  })
  
  # mgcv hazard
  mgcv_haz <- switch(trans,
    h12 = hazard_mgcv_h12$hazard,
    h13 = hazard_mgcv_h13$hazard,
    h23 = hazard_mgcv_h23$hazard
  )
  
  # Combine data
  hazard_df <- data.frame(
    time = rep(id_eval_times, 5),
    hazard = c(h_true, unlist(julia_hazards), mgcv_haz),
    method = factor(rep(c("True (Weibull)", julia_methods, "mgcv GCV"), each = length(id_eval_times)),
                    levels = c("True (Weibull)", julia_methods, "mgcv GCV"))
  )
  
  # Event times for this transition
  event_times_trans <- id_event_times %>% filter(transition == trans) %>% pull(time)
  
  ggplot(hazard_df, aes(x = time, y = hazard, color = method, linetype = method)) +
    geom_line(linewidth = 1) +
    geom_rug(data = data.frame(time = event_times_trans),
             aes(x = time), inherit.aes = FALSE,
             color = "gray30", alpha = 0.5, sides = "b") +
    scale_color_manual(values = id_method_colors) +
    scale_linetype_manual(values = id_method_linetypes) +
    labs(
      title = paste0("Hazard: ", trans_label),
      x = "Time",
      y = "h(t)",
      color = "Method",
      linetype = "Method"
    ) +
    theme(legend.position = "none")
}

p_h12 <- create_hazard_plot("h12", "Healthy → Ill (h12)")
p_h13 <- create_hazard_plot("h13", "Healthy → Dead (h13)")
p_h23 <- create_hazard_plot("h23", "Ill → Dead (h23)")

# Combine with shared legend
(p_h12 / p_h13 / p_h23) +
  plot_layout(guides = "collect") &
  theme(legend.position = "bottom") &
  guides(color = guide_legend(nrow = 2), linetype = guide_legend(nrow = 2))
```

## Cumulative Hazard Comparison (Illness-Death)

```{r}
#| label: id-cumhaz-comparison
#| fig-height: 10

# Create cumulative hazard plots for each transition
create_cumhaz_plot <- function(trans, trans_label) {
  # True cumulative hazard
  H_true <- id_true_cumhaz(id_eval_times, trans)
  
  # Julia cumulative hazards
  julia_methods <- c("Julia PIJCV", "Julia EFS", "Julia CV10")
  julia_cumhaz <- lapply(julia_methods, function(m) {
    id_julia_curves %>%
      filter(transition == trans, method == m) %>%
      pull(cumhaz)
  })
  
  # mgcv cumulative hazard (integrate hazard)
  mgcv_haz <- switch(trans,
    h12 = hazard_mgcv_h12$hazard,
    h13 = hazard_mgcv_h13$hazard,
    h23 = hazard_mgcv_h23$hazard
  )
  mgcv_cumhaz <- cumsum(mgcv_haz) * diff(id_eval_times)[1]
  
  # Combine data
  cumhaz_df <- data.frame(
    time = rep(id_eval_times, 5),
    cumhaz = c(H_true, unlist(julia_cumhaz), mgcv_cumhaz),
    method = factor(rep(c("True (Weibull)", julia_methods, "mgcv GCV"), each = length(id_eval_times)),
                    levels = c("True (Weibull)", julia_methods, "mgcv GCV"))
  )
  
  ggplot(cumhaz_df, aes(x = time, y = cumhaz, color = method, linetype = method)) +
    geom_line(linewidth = 1) +
    scale_color_manual(values = id_method_colors) +
    scale_linetype_manual(values = id_method_linetypes) +
    labs(
      title = paste0("Cumulative Hazard: ", trans_label),
      x = "Time",
      y = "H(t)",
      color = "Method",
      linetype = "Method"
    ) +
    theme(legend.position = "none")
}

p_H12 <- create_cumhaz_plot("h12", "Healthy → Ill (h12)")
p_H13 <- create_cumhaz_plot("h13", "Healthy → Dead (h13)")
p_H23 <- create_cumhaz_plot("h23", "Ill → Dead (h23)")

(p_H12 / p_H13 / p_H23) +
  plot_layout(guides = "collect") &
  theme(legend.position = "bottom") &
  guides(color = guide_legend(nrow = 2), linetype = guide_legend(nrow = 2))
```

## Accuracy Metrics (Illness-Death)

```{r}
#| label: id-accuracy-metrics

# RMSE function
rmse <- function(true, est) sqrt(mean((true - est)^2, na.rm = TRUE))

# Calculate metrics for each transition and method
id_metrics_list <- list()

for (trans in c("h12", "h13", "h23")) {
  h_true <- id_true_hazard(id_eval_times, trans)
  H_true <- id_true_cumhaz(id_eval_times, trans)
  
  # Julia methods (PIJCV, EFS, CV10)
  for (method in c("PIJCV", "EFS", "CV10")) {
    julia_method <- paste0("Julia ", method)
    h_julia <- id_julia_curves %>%
      filter(transition == trans, method == julia_method) %>%
      pull(hazard)
    H_julia <- id_julia_curves %>%
      filter(transition == trans, method == julia_method) %>%
      pull(cumhaz)
    
    id_metrics_list[[paste(trans, method)]] <- data.frame(
      Transition = trans,
      Package = "MultistateModels.jl",
      Method = method,
      Hazard_RMSE = rmse(h_true, h_julia),
      CumHaz_RMSE = rmse(H_true, H_julia)
    )
  }
  
  # mgcv
  mgcv_haz <- switch(trans,
    h12 = hazard_mgcv_h12$hazard,
    h13 = hazard_mgcv_h13$hazard,
    h23 = hazard_mgcv_h23$hazard
  )
  mgcv_cumhaz <- cumsum(mgcv_haz) * diff(id_eval_times)[1]
  
  id_metrics_list[[paste(trans, "mgcv")]] <- data.frame(
    Transition = trans,
    Package = "mgcv",
    Method = "GCV.Cp",
    Hazard_RMSE = rmse(h_true, mgcv_haz),
    CumHaz_RMSE = rmse(H_true, mgcv_cumhaz)
  )
}

id_metrics_df <- do.call(rbind, id_metrics_list)

kable(id_metrics_df, digits = 5,
      caption = "RMSE vs True Weibull Functions - Illness-Death Model",
      col.names = c("Transition", "Package", "Method", "Hazard RMSE", "Cum. Hazard RMSE")) %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  pack_rows("h12: Healthy → Ill", 1, 4) %>%
  pack_rows("h13: Healthy → Dead", 5, 8) %>%
  pack_rows("h23: Ill → Dead", 9, 12)
```

## Discussion (Illness-Death Model)

The illness-death model presents additional challenges compared to simple survival:

1. **Competing risks**: From state 1, subjects can transition to either state 2 (illness) 
   or state 3 (death), requiring proper handling of censoring for each transition.

2. **Transition-specific hazards**: Each transition has its own hazard function with 
   different shapes, testing the flexibility of spline smoothing.

3. **Data sparsity**: Some transitions may have fewer events, affecting the stability 
   of smoothing parameter selection.

**Key observations:**

- All Julia smoothing methods (PIJCV, PIJCV10, EFS, CV10) produce similar results
- EDF values are comparable between Julia and mgcv for each transition
- The decreasing hazard (h13) is more challenging to capture than increasing hazards
- The strongly increasing hazard (h23) is well-recovered by all methods

---

# Part 3: Bathtub Hazard (Non-Monotonic)

The Weibull simulations in Parts 1-2 favor flexsurv's log(time) parameterization because 
Weibull hazards become linear on the log scale. This section tests a **bathtub-shaped hazard** 
that is challenging for all methods equally.

## True Model Specification (Bathtub)

The bathtub hazard is a classic pattern in reliability engineering, combining:
- **Early failures** (infant mortality): Decreasing hazard
- **Random failures** (useful life): Constant hazard  
- **Wearout failures** (aging): Increasing hazard

We use a mixture of Weibull distributions to create the bathtub shape:
$$h(t) = w_1 \cdot h_1(t) + w_2 \cdot h_2(t)$$

where:
- $h_1(t)$ = Weibull with shape < 1 (decreasing, early failures)
- $h_2(t)$ = Weibull with shape > 1 (increasing, wearout)

Parameters:
- $h_1$: shape = 0.5, rate = 0.8, weight = 0.3
- $h_2$: shape = 2.5, rate = 0.15, weight = 0.7

```{bash}
#| label: bt-julia-data-generation
#| output: false

cd ../..
julia --project=. -e '
using MultistateModels
using Random
using DataFrames
using CSV
using Distributions

# Configuration
n = 1000
max_time = 5.0
seed = 54321
nknots = 8  # More knots for non-monotonic shape

Random.seed!(seed)

# Bathtub hazard parameters
# Component 1: decreasing (infant mortality)
shape1, rate1, w1 = 0.5, 0.8, 0.3
# Component 2: increasing (wearout)
shape2, rate2, w2 = 2.5, 0.15, 0.7

# Bathtub hazard function
function bathtub_hazard(t)
    h1 = shape1 * rate1 * (rate1 * t)^(shape1 - 1)
    h2 = shape2 * rate2 * (rate2 * t)^(shape2 - 1)
    return w1 * h1 + w2 * h2
end

# Bathtub cumulative hazard (numerical integration)
function bathtub_cumhaz(t; nsteps=1000)
    if t <= 0
        return 0.0
    end
    dt = t / nsteps
    H = 0.0
    for i in 1:nsteps
        ti = (i - 0.5) * dt
        H += bathtub_hazard(ti) * dt
    end
    return H
end

# Simulate event times via inversion (rejection sampling for efficiency)
function simulate_bathtub_event(max_t=20.0)
    # Use thinning algorithm with constant majorizing hazard
    M = maximum(bathtub_hazard.(range(0.01, max_t, length=500)))
    t = 0.0
    while true
        t += -log(rand()) / M  # Exponential with rate M
        if t > max_t
            return max_t + 1.0  # Censored beyond max_t
        end
        # Accept with probability h(t)/M
        if rand() < bathtub_hazard(t) / M
            return t
        end
    end
end

# Generate event times
event_times = [simulate_bathtub_event() for _ in 1:n]
obs_times = min.(event_times, max_time)
status = Int.(event_times .<= max_time)

println("=== Bathtub Hazard Data Summary ===")
println("Events: $(sum(status)) / $n ($(round(100*mean(status), digits=1))%)")
println("Median event time: $(round(median(obs_times[status .== 1]), digits=3))")

# Create survival data
surv_data = DataFrame(
    id = 1:n,
    tstart = zeros(n),
    tstop = obs_times,
    statefrom = ones(Int, n),
    stateto = fill(2, n),
    obstype = ones(Int, n)
)

# Save data
CSV.write("MultistateModelsTests/reports/_bt_surv_data.csv", DataFrame(time = obs_times, status = status))

# Build model with spline hazard
h12 = Hazard(@formula(0 ~ 1), "sp", 1, 2;
             degree = 3,
             knots = [max_time/2],
             boundaryknots = [0.0, max_time],
             natural_spline = true)
model = multistatemodel(h12; data=surv_data)

# Calibrate knots
knot_result = calibrate_splines!(model; nknots=nknots, verbose=true)
interior_knots = knot_result.h12.interior_knots
boundary_knots = knot_result.h12.boundary_knots

println("\nKnot Configuration:")
println("  Interior knots: $(round.(interior_knots, digits=3))")
println("  Boundary knots: $(boundary_knots)")

# Save knots
CSV.write("MultistateModelsTests/reports/_bt_knots.csv", DataFrame(
    interior_knots = interior_knots,
    boundary_lower = fill(boundary_knots[1], length(interior_knots)),
    boundary_upper = fill(boundary_knots[2], length(interior_knots))
))
'
```

```{bash}
#| label: bt-julia-fitting
#| output: false

cd ../..
julia --project=. -e '
using MultistateModels
using MultistateModels: get_parameters_flat, get_loglik
using Random
using DataFrames
using CSV
using Distributions

# Configuration (must match data generation)
n = 1000
max_time = 5.0
seed = 54321
nknots = 8

# Bathtub parameters
shape1, rate1, w1 = 0.5, 0.8, 0.3
shape2, rate2, w2 = 2.5, 0.15, 0.7

function bathtub_hazard(t)
    h1 = shape1 * rate1 * (rate1 * t)^(shape1 - 1)
    h2 = shape2 * rate2 * (rate2 * t)^(shape2 - 1)
    return w1 * h1 + w2 * h2
end

function bathtub_cumhaz(t; nsteps=1000)
    t <= 0 && return 0.0
    dt = t / nsteps
    H = 0.0
    for i in 1:nsteps
        H += bathtub_hazard((i - 0.5) * dt) * dt
    end
    return H
end

function simulate_bathtub_event(max_t=20.0)
    M = maximum(bathtub_hazard.(range(0.01, max_t, length=500)))
    t = 0.0
    while true
        t += -log(rand()) / M
        t > max_t && return max_t + 1.0
        rand() < bathtub_hazard(t) / M && return t
    end
end

Random.seed!(seed)
event_times = [simulate_bathtub_event() for _ in 1:n]
obs_times = min.(event_times, max_time)
status = Int.(event_times .<= max_time)

surv_data = DataFrame(
    id = 1:n,
    tstart = zeros(n),
    tstop = obs_times,
    statefrom = ones(Int, n),
    stateto = fill(2, n),
    obstype = ones(Int, n)
)

# Build and calibrate model
h12 = Hazard(@formula(0 ~ 1), "sp", 1, 2;
             degree = 3, knots = [max_time/2],
             boundaryknots = [0.0, max_time], natural_spline = true)
model = multistatemodel(h12; data=surv_data)
calibrate_splines!(model; nknots=nknots, verbose=false)

# Helper functions
total_edf(fitted) = fitted.edf === nothing ? NaN : fitted.edf.total

timings = Dict{String, Float64}()

# Fit with PIJCV
println("=== Fitting Bathtub Model ===")
println("\nFitting with PIJCV...")
fit(model; penalty=:auto, select_lambda=:pijcv, vcov_type=:none, verbose=false)
t_pijcv = @elapsed fitted_pijcv = fit(model; penalty=:auto, select_lambda=:pijcv, vcov_type=:none, verbose=false)
timings["PIJCV"] = t_pijcv
println("  PIJCV λ = $(round(fitted_pijcv.smoothing_parameters[1], digits=4)), EDF = $(round(total_edf(fitted_pijcv), digits=2))")

# Fit with EFS
println("\nFitting with EFS...")
fit(model; penalty=:auto, select_lambda=:efs, vcov_type=:none, verbose=false)
t_efs = @elapsed fitted_efs = fit(model; penalty=:auto, select_lambda=:efs, vcov_type=:none, verbose=false)
timings["EFS"] = t_efs
println("  EFS λ = $(round(fitted_efs.smoothing_parameters[1], digits=4)), EDF = $(round(total_edf(fitted_efs), digits=2))")

# Fit with CV10
println("\nFitting with CV10...")
fit(model; penalty=:auto, select_lambda=:cv10, vcov_type=:none, verbose=false)
t_cv10 = @elapsed fitted_cv10 = fit(model; penalty=:auto, select_lambda=:cv10, vcov_type=:none, verbose=false)
timings["CV10"] = t_cv10
println("  CV10 λ = $(round(fitted_cv10.smoothing_parameters[1], digits=4)), EDF = $(round(total_edf(fitted_cv10), digits=2))")

# =============================================================================
# WEIGHTED SMOOTHING METHODS (Bathtub)
# =============================================================================
# Weighted penalties are particularly relevant for non-monotonic hazards like
# the bathtub, where the shape changes significantly over time.

# PIJCV with at-risk weighting (fixed α=1.0)
println("\nFitting with PIJCV + at-risk weighting (α=1.0)...")
weighted_penalty = SplinePenalty(adaptive_weight=:atrisk, alpha=1.0)
fit(model; penalty=weighted_penalty, select_lambda=:pijcv, vcov_type=:none, verbose=false)
t_pijcv_w = @elapsed fitted_pijcv_w = fit(model; penalty=weighted_penalty, select_lambda=:pijcv, vcov_type=:none, verbose=false)
timings["PIJCV_weighted"] = t_pijcv_w
println("  PIJCV (weighted α=1) λ = $(round(fitted_pijcv_w.smoothing_parameters[1], digits=4)), EDF = $(round(total_edf(fitted_pijcv_w), digits=2))")

# PIJCV with learned α
println("\nFitting with PIJCV + at-risk weighting (learn α)...")
learn_alpha_penalty = SplinePenalty(adaptive_weight=:atrisk, learn_alpha=true)
fit(model; penalty=learn_alpha_penalty, select_lambda=:pijcv, vcov_type=:none, verbose=false)
t_pijcv_la = @elapsed fitted_pijcv_la = fit(model; penalty=learn_alpha_penalty, select_lambda=:pijcv, vcov_type=:none, verbose=false)
timings["PIJCV_learn_alpha"] = t_pijcv_la
println("  PIJCV (learn α) λ = $(round(fitted_pijcv_la.smoothing_parameters[1], digits=4)), EDF = $(round(total_edf(fitted_pijcv_la), digits=2))")

# Evaluation grid
eval_times = collect(range(0.01, max_time, length=200))

function evaluate_curves(model, beta, eval_times)
    haz = model.hazards[1]
    hazard_vals = [haz.hazard_fn(t, beta, ()) for t in eval_times]
    cumhaz_vals = [haz.cumhaz_fn(0.0, t, beta, ()) for t in eval_times]
    survival_vals = exp.(-cumhaz_vals)
    return (hazard = hazard_vals, cumhaz = cumhaz_vals, survival = survival_vals)
end

beta_pijcv = get_parameters_flat(fitted_pijcv)
beta_efs = get_parameters_flat(fitted_efs)
beta_cv10 = get_parameters_flat(fitted_cv10)
beta_pijcv_w = get_parameters_flat(fitted_pijcv_w)
beta_pijcv_la = get_parameters_flat(fitted_pijcv_la)

curves_pijcv = evaluate_curves(model, beta_pijcv, eval_times)
curves_efs = evaluate_curves(model, beta_efs, eval_times)
curves_cv10 = evaluate_curves(model, beta_cv10, eval_times)
curves_pijcv_w = evaluate_curves(model, beta_pijcv_w, eval_times)
curves_pijcv_la = evaluate_curves(model, beta_pijcv_la, eval_times)

# Save results (including weighted methods)
julia_results = DataFrame(
    time = repeat(eval_times, 5),
    hazard = vcat(curves_pijcv.hazard, curves_efs.hazard, curves_cv10.hazard,
                  curves_pijcv_w.hazard, curves_pijcv_la.hazard),
    cumhaz = vcat(curves_pijcv.cumhaz, curves_efs.cumhaz, curves_cv10.cumhaz,
                  curves_pijcv_w.cumhaz, curves_pijcv_la.cumhaz),
    survival = vcat(curves_pijcv.survival, curves_efs.survival, curves_cv10.survival,
                    curves_pijcv_w.survival, curves_pijcv_la.survival),
    method = repeat(["Julia PIJCV", "Julia EFS", "Julia CV10",
                     "Julia PIJCV (weighted)", "Julia PIJCV (learn α)"], inner=length(eval_times))
)
CSV.write("MultistateModelsTests/reports/_bt_julia_curves.csv", julia_results)

julia_summary = DataFrame(
    method = ["PIJCV", "EFS", "CV10", "PIJCV_weighted", "PIJCV_learn_alpha"],
    lambda = [fitted_pijcv.smoothing_parameters[1], fitted_efs.smoothing_parameters[1], 
              fitted_cv10.smoothing_parameters[1], fitted_pijcv_w.smoothing_parameters[1],
              fitted_pijcv_la.smoothing_parameters[1]],
    edf = [total_edf(fitted_pijcv), total_edf(fitted_efs), total_edf(fitted_cv10),
           total_edf(fitted_pijcv_w), total_edf(fitted_pijcv_la)],
    loglik = [get_loglik(fitted_pijcv), get_loglik(fitted_efs), get_loglik(fitted_cv10),
              get_loglik(fitted_pijcv_w), get_loglik(fitted_pijcv_la)],
    runtime_sec = [timings["PIJCV"], timings["EFS"], timings["CV10"],
                   timings["PIJCV_weighted"], timings["PIJCV_learn_alpha"]],
    weighted = [false, false, false, true, true]
)
CSV.write("MultistateModelsTests/reports/_bt_julia_summary.csv", julia_summary)

println("\nBathtub fitting complete.")
'
```

```{r}
#| label: bt-load-results
#| output: false

# Load bathtub data
bt_surv_data <- read.csv("_bt_surv_data.csv")
bt_julia_curves <- read.csv("_bt_julia_curves.csv")
bt_julia_summary <- read.csv("_bt_julia_summary.csv")
bt_knots <- read.csv("_bt_knots.csv")

bt_interior_knots <- bt_knots$interior_knots
bt_boundary_lower <- bt_knots$boundary_lower[1]
bt_boundary_upper <- bt_knots$boundary_upper[1]
bt_max_time <- bt_boundary_upper

# Bathtub parameters
bt_shape1 <- 0.5; bt_rate1 <- 0.8; bt_w1 <- 0.3
bt_shape2 <- 2.5; bt_rate2 <- 0.15; bt_w2 <- 0.7
```

```{r}
#| label: bt-true-functions

# True bathtub hazard
bt_true_hazard <- function(t) {
  h1 <- bt_shape1 * bt_rate1 * (bt_rate1 * t)^(bt_shape1 - 1)
  h2 <- bt_shape2 * bt_rate2 * (bt_rate2 * t)^(bt_shape2 - 1)
  return(bt_w1 * h1 + bt_w2 * h2)
}

# True cumulative hazard (numerical integration)
bt_true_cumhaz <- function(t) {
  if (t <= 0) return(0)
  integrate(bt_true_hazard, 0, t, subdivisions = 1000)$value
}
bt_true_cumhaz <- Vectorize(bt_true_cumhaz)

# True survival
bt_true_survival <- function(t) exp(-bt_true_cumhaz(t))
```

```{r}
#| label: bt-mgcv-fit
#| output: false

# PAM transformation
bt_surv_data_ped <- bt_surv_data %>% mutate(id = row_number())
bt_ped <- as_ped(
  formula = Surv(time, status) ~ 1,
  data = bt_surv_data_ped,
  id = "id"
)

# Knot setup for mgcv
bt_k_mgcv <- length(bt_interior_knots) + 4
bt_m_order <- 2
bt_middle_knots <- c(bt_boundary_lower, bt_interior_knots, bt_boundary_upper)
bt_delta <- mean(diff(bt_middle_knots))
bt_padding_lower <- bt_boundary_lower - (3:1) * bt_delta
bt_padding_upper <- bt_boundary_upper + (1:3) * bt_delta
bt_mgcv_knots <- c(bt_padding_lower, bt_middle_knots, bt_padding_upper)

# Fit PAM
bt_fit_gcv <- pamm(
  ped_status ~ s(tend, bs = "ps", k = bt_k_mgcv, m = c(bt_m_order, bt_m_order)),
  data = bt_ped,
  method = "GCV.Cp",
  knots = list(tend = bt_mgcv_knots)
)

cat("mgcv EDF (bathtub):", round(sum(bt_fit_gcv$edf), 2), "\n")
```

```{r}
#| label: bt-flexsurv-fit
#| output: false

# flexsurv with same interior knots (log-transformed)
bt_log_interior_knots <- log(bt_interior_knots)

bt_fit_flex <- flexsurvspline(
  Surv(time, status) ~ 1,
  data = bt_surv_data,
  knots = bt_log_interior_knots,
  scale = "hazard"
)

cat("flexsurv AIC (bathtub):", round(AIC(bt_fit_flex), 2), "\n")
```

## Hazard Comparison (Bathtub)

```{r}
#| label: bt-hazard-comparison
#| fig-height: 7

bt_eval_times <- seq(0.05, bt_max_time, length.out = 200)

# True hazard
bt_h_true <- bt_true_hazard(bt_eval_times)

# Julia methods (including weighted)
bt_julia_h_pijcv <- bt_julia_curves %>% filter(method == "Julia PIJCV") %>% pull(hazard)
bt_julia_h_efs <- bt_julia_curves %>% filter(method == "Julia EFS") %>% pull(hazard)
bt_julia_h_cv10 <- bt_julia_curves %>% filter(method == "Julia CV10") %>% pull(hazard)
bt_julia_h_weighted <- bt_julia_curves %>% filter(method == "Julia PIJCV (weighted)") %>% pull(hazard)
bt_julia_h_learn_alpha <- bt_julia_curves %>% filter(method == "Julia PIJCV (learn α)") %>% pull(hazard)

# mgcv
bt_newdata_mgcv <- data.frame(tend = bt_eval_times)
bt_hazard_mgcv <- add_hazard(bt_newdata_mgcv, bt_fit_gcv)
bt_h_mgcv <- bt_hazard_mgcv$hazard

# flexsurv
bt_h_flex <- summary(bt_fit_flex, t = bt_eval_times, type = "hazard")[[1]]$est

# Combine for plotting (including weighted methods)
bt_hazard_df <- data.frame(
  time = rep(bt_eval_times, 8),
  hazard = c(bt_h_true, bt_julia_h_pijcv, bt_julia_h_efs, bt_julia_h_cv10,
             bt_julia_h_weighted, bt_julia_h_learn_alpha,
             bt_h_mgcv, bt_h_flex),
  method = factor(rep(c("True (Bathtub)", "Julia PIJCV", "Julia EFS", "Julia CV10",
                        "Julia PIJCV (weighted)", "Julia PIJCV (learn α)",
                        "mgcv GCV", "flexsurv"), 
                      each = length(bt_eval_times)),
                  levels = c("True (Bathtub)", "Julia PIJCV", "Julia EFS", "Julia CV10",
                             "Julia PIJCV (weighted)", "Julia PIJCV (learn α)",
                             "mgcv GCV", "flexsurv"))
)

ggplot(bt_hazard_df, aes(x = time, y = hazard, color = method, linetype = method)) +
  geom_line(linewidth = 1) +
  geom_rug(data = bt_surv_data %>% filter(status == 1), 
           aes(x = time), inherit.aes = FALSE, 
           color = "gray30", alpha = 0.5, sides = "b") +
  scale_color_manual(values = c("True (Bathtub)" = "black", 
                                 "Julia PIJCV" = "#D55E00",
                                 "Julia EFS" = "#56B4E9",
                                 "Julia CV10" = "#F0E442",
                                 "Julia PIJCV (weighted)" = "#0072B2",
                                 "Julia PIJCV (learn α)" = "#CC79A7",
                                 "mgcv GCV" = "#E69F00",
                                 "flexsurv" = "#009E73")) +
  scale_linetype_manual(values = c("True (Bathtub)" = "solid",
                                    "Julia PIJCV" = "dashed",
                                    "Julia EFS" = "twodash",
                                    "Julia CV10" = "dashed",
                                    "Julia PIJCV (weighted)" = "dotted",
                                    "Julia PIJCV (learn α)" = "dotted",
                                    "mgcv GCV" = "dotdash",
                                    "flexsurv" = "solid")) +
  labs(
    title = "Bathtub Hazard Function Estimates",
    subtitle = "Non-monotonic hazard: early failures + wearout; rug = event times",
    x = "Time",
    y = "Hazard h(t)",
    color = "Method",
    linetype = "Method"
  ) +
  theme(legend.position = "bottom") +
  guides(color = guide_legend(nrow = 3), linetype = guide_legend(nrow = 3))
```

## Accuracy Metrics (Bathtub)

```{r}
#| label: bt-accuracy-metrics

bt_H_true <- bt_true_cumhaz(bt_eval_times)
bt_S_true <- exp(-bt_H_true)

# RMSE calculations (including weighted methods)
bt_metrics <- data.frame(
  Package = c(rep("MultistateModels.jl", 5), "mgcv", "flexsurv"),
  Method = c("PIJCV", "EFS", "CV10", "PIJCV (weighted)", "PIJCV (learn α)", "GCV.Cp", "spline"),
  Hazard_RMSE = c(
    rmse(bt_h_true, bt_julia_h_pijcv),
    rmse(bt_h_true, bt_julia_h_efs),
    rmse(bt_h_true, bt_julia_h_cv10),
    rmse(bt_h_true, bt_julia_h_weighted),
    rmse(bt_h_true, bt_julia_h_learn_alpha),
    rmse(bt_h_true, bt_h_mgcv),
    rmse(bt_h_true, bt_h_flex)
  )
)

kable(bt_metrics, digits = 5,
      caption = "RMSE vs True Bathtub Hazard Function",
      col.names = c("Package", "Method", "Hazard RMSE")) %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  pack_rows("MultistateModels.jl", 1, 5) %>%
  pack_rows("R Packages", 6, 7)
```

## Discussion (Bathtub Hazard)

The bathtub hazard provides a fairer comparison because:

1. **Non-monotonic shape**: The bathtub curve has both decreasing and increasing regions,
   which is not easily represented on any particular scale.

2. **No parameterization advantage**: Unlike Weibull, there's no scale where the bathtub
   hazard becomes linear—all methods must use multiple basis functions.

3. **Real-world relevance**: Bathtub hazards are common in reliability engineering 
   (early failures + useful life + wearout) and some medical contexts.

**Key observations:**

- **flexsurv no longer dominates**: Without the Weibull's log-linearity, flexsurv's 
  log(time) parameterization loses its advantage.
  
- **All penalized methods perform similarly**: The RMSE values are comparable across
  Julia's methods and mgcv, showing that penalized splines work well regardless of
  whether they model h(t) on linear time or log(cumulative hazard) on log(time).

- **More knots needed**: We used 8 interior knots (vs 5 for Weibull) to capture the
  non-monotonic shape—the penalty then prevents overfitting.

- **Weighted penalties show similar performance**: For this sample size (n=1000) and 
  hazard shape, weighted penalties (`adaptive_weight=:atrisk`) produce comparable 
  results to uniform penalties. Weighted penalties may show more benefit in scenarios 
  with sparse data in the tails or rapidly declining at-risk populations.

- **Alpha estimation**: The `learn_alpha=true` option provides automatic tuning of the 
  penalty weighting strength. In settings where α=1 is not optimal, this can improve 
  estimation quality at modest computational cost.

---

# Session Info

```{r}
#| label: session-info

sessionInfo()
```

```{r}
#| label: cleanup
#| output: false

# Clean up temporary files
unlink("_surv_data.csv")
unlink("_julia_curves.csv")
unlink("_julia_summary.csv")
unlink("_knots.csv")

# Illness-death files
unlink("_id_surv_data.csv")
unlink("_id_julia_curves.csv")
unlink("_id_julia_summary.csv")
unlink("_id_knots.csv")
unlink("_id_event_times.csv")

# Bathtub files
unlink("_bt_surv_data.csv")
unlink("_bt_julia_curves.csv")
unlink("_bt_julia_summary.csv")
unlink("_bt_knots.csv")
```
